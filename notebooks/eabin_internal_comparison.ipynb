{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959b3f28-d19a-4ab8-ac25-93cdc8123f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Python version 3.13 is not officially supported. Removing ISNs features for compatibility.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "cwd = os.getcwd()\n",
    "sep = os.sep\n",
    "\n",
    "from adin.ml import baselineComparison, train_test_split, create_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad5b542-b335-45a5-8532-958571ff86f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING DETERMINISTIC MODE FOR REPRODUCIBILITY, SEED: 0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from adin.utils import set_seed \n",
    "\n",
    "def set_deterministic(seed):\n",
    "    global current_seed\n",
    "    global is_deterministic\n",
    "\n",
    "    #FOR REPRODUCIBILITY:\n",
    "    print(f\"SETTING DETERMINISTIC MODE FOR REPRODUCIBILITY, SEED: {seed}\")\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":16:8\"\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    set_seed(seed)\n",
    "    current_seed = seed \n",
    "    is_deterministic = True\n",
    "\n",
    "set_deterministic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2ec801-6586-41c3-b536-46556f553dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<adin.ml_config.ML_config at 0x207cd3856a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from adin import ml_config\n",
    "ml_params = ml_config.ML_config(\"lbfgs\", \"l2\", 100, \"linear\", 1.0, 5, \"euclidean\", 100, -1, -1, 2, 0.7, 5)\n",
    "ml_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e4cbe7a-aefc-4f26-8bad-7034238c7e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Utente\\\\Desktop\\\\E-ABIN\\\\'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pardir = cwd.split(sep)[:-1]\n",
    "pardir_str = \"\"\n",
    "for elem in pardir:\n",
    "    pardir_str += elem + sep \n",
    "pardir = pardir_str\n",
    "pardir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df507455-dd32-403b-9023-04c7d50c32e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bladder_cancer', 'celiac', 'colorectal_cancer', 'parkinson']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_path = pardir + \"use_case\" + sep + \"data\" \n",
    "datasets_name = os.listdir(datasets_path)\n",
    "datasets_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c67fd2c-196c-4f52-9adf-44b8fecbf6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path):\n",
    "    import pandas as pd \n",
    "\n",
    "    files = os.listdir(dataset_path)\n",
    "    files = [file for file in files if \".csv\" in file]\n",
    "    if len(files) > 0:\n",
    "        file = files[0]\n",
    "        datapath = dataset_path + sep + file\n",
    "        df = pd.read_csv(datapath, index_col = 0)\n",
    "        return df\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d133cbd1-fa9a-4dd1-850d-3f999ba0d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot(df, column_name = \"AUC score\"):\n",
    "\n",
    "    #vertical bar plot of the models' performance for different datasets\n",
    "    # x dataset, group by model\n",
    "    # y model performance metric (e.g., AUC score)\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
    "    # Convert the 'Model' column to string type if it's not already\n",
    "    df['Model Name'] = df['Model Name'].astype(str)\n",
    "    # Set the aesthetic style of the plots\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Create a vertical bar plot\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Dataset', y=column_name, data=df, hue='Model Name', dodge=True)\n",
    "    plt.title(f'Comparison of Models by {column_name}', fontsize=26, fontweight='bold')\n",
    "    plt.xlabel('Dataset', fontsize=20, fontweight='bold')\n",
    "    plt.ylabel(column_name, fontsize=20, fontweight='bold')\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Model Name', bbox_to_anchor=(1.05, 1), loc='upper left', title_fontsize = 18, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "028e93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_similarity(expr):\n",
    "\n",
    "    # Move data to tensor, float32, and (optionally) to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    values = torch.as_tensor(expr, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Normalize the rows (L2 norm)\n",
    "    values = torch.nn.functional.normalize(values, p=2, dim=1)\n",
    "    \n",
    "    # Compute cosine similarity matrix: sim[i, j] = cosine_similarity(values[i], values[j])\n",
    "    sim_matrix = torch.matmul(values, values.T)\n",
    "\n",
    "    return sim_matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e902906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(sim, th = 0.93):\n",
    "\n",
    "    edges = torch.nonzero(sim > th, as_tuple=False)\n",
    "    edges = edges[edges[:, 0] != edges[:, 1]]  # Remove self-loops\n",
    "    edges = edges.cpu().numpy()\n",
    "    edges_i = []\n",
    "    for edge in edges:\n",
    "        source, target = edge\n",
    "        edges_i.append(f\"{source}_{target}\")\n",
    "        \n",
    "    return edges_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adin import utils, gaan_config, dl\n",
    "from adin.gaan import GAAN_Explainable\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from adin import gcn\n",
    "from adin.gae import GAE_Explainable\n",
    "import numpy as np\n",
    "from torch.cuda import empty_cache\n",
    "from gc import collect\n",
    "\n",
    "sims = {}\n",
    "node_mappings = {}\n",
    "\n",
    "def node_anomaly_detection(expr, targets, name, gaan_params = None, th = 0.93):\n",
    "\n",
    "    global sims \n",
    "\n",
    "    expr = expr.values\n",
    "    expr = torch.as_tensor(expr, dtype=torch.float32)\n",
    "    x = expr.cpu().numpy()\n",
    "    y = targets.values\n",
    "\n",
    "    if name not in sims:\n",
    "        print(\"Computing node similarity\")\n",
    "        sim = get_similarity(expr)\n",
    "        sims[name] = sim.to(\"cpu\")  # Store the similarity matrix in the global dictionary\n",
    "    else:\n",
    "        print(\"Using precomputed node similarity\")\n",
    "        sim = sims[name]\n",
    "\n",
    "    del sim, expr \n",
    "\n",
    "    edges_i = get_edges(sims[name], th=0.93)\n",
    "    source_nodes, target_nodes, node_mapping = utils.parse_edges(edges_i)\n",
    "    edge_index = utils.create_edge_index(source_nodes, target_nodes)\n",
    "    del source_nodes, target_nodes, edges_i\n",
    "    \n",
    "    mydataloader = dl.create_torch_geo_data(x, y, edge_index)\n",
    "    del x, edge_index \n",
    "\n",
    "    dataloader_train, dataloader_test = dl.train_test_split_and_mask(mydataloader, train_size = 0.7)\n",
    "    num_nodes = dataloader_train.x.shape[0] + dataloader_test.x.shape[0]\n",
    "    dataloader_train.edge_index, _ = add_self_loops(dataloader_train.edge_index, num_nodes=num_nodes)\n",
    "    dataloader_test.edge_index, _ = add_self_loops(dataloader_test.edge_index, num_nodes=num_nodes)\n",
    "    in_dim = dataloader_train.x.shape[1]\n",
    "\n",
    "    uqs, counts = np.unique(y, return_counts = True)\n",
    "    dict_counts = {}\n",
    "    for uq, count in zip(uqs, counts):\n",
    "        dict_counts[uq.item()] = count.item()\n",
    "    contamination = (dict_counts[1]/(dict_counts[0] + dict_counts[1]))*0.5\n",
    "\n",
    "    gpu = 0 if torch.cuda.is_available() else -1\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    gaan_params = gaan_config.GAAN_config(noise_dim=64, hid_dim=128, num_layers=2, dropout=0.3, contamination=contamination, lr = 0.00005, epoch = 200, gpu = gpu, batch_size=1, verbose = 1, isn = False, th = 0.93)\n",
    "            \n",
    "    attrs = vars(gaan_params)\n",
    "    print(', '.join(\"%s: %s\" % item for item in attrs.items()))\n",
    "\n",
    "    print(\"Create GAAN model\")\n",
    "    model_gaan = dl.create_model(in_dim, gaan_params, isn = False)\n",
    "    model_gaan = dl.train_gaan(model_gaan, dataloader_train)\n",
    "    collect()\n",
    "    empty_cache()\n",
    "\n",
    "    model_gae = GAE_Explainable(in_dim, device = device, hid_dim=gaan_params.hid_dim, num_layers=gaan_params.num_layers, dropout=gaan_params.dropout, contamination=gaan_params.contamination, lr=gaan_params.lr, epoch=gaan_params.epoch, verbose=gaan_params.verbose) #batch_size=gaan_params.batch_size\n",
    "    model_gae.fit(dataloader_train)\n",
    "    collect()\n",
    "    empty_cache()\n",
    "   \n",
    "    lr = gaan_params.lr\n",
    "    hidden_dims = [128]\n",
    "    inchannels = mydataloader.x.shape[1]\n",
    "    model_gcn = gcn.GCN(inchannels, hidden_dims=hidden_dims).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "    optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.0005)  # Define optimizer.\n",
    "    for epoch in range(2000):\n",
    "        if epoch%100 == 0:\n",
    "            print(\"Training GCN, epoch {}\".format(epoch))\n",
    "        loss = gcn.train(model_gcn, optimizer, criterion, dataloader_train)\n",
    "    collect()\n",
    "    empty_cache()\n",
    "    models = {\n",
    "        \"GAAN\": model_gaan,\n",
    "        \"GAE\": model_gae,\n",
    "        \"GCN\": model_gcn\n",
    "    } \n",
    "\n",
    "    df_result = dl.create_results_df(models, dataloader_test)\n",
    "\n",
    "    return df_result, models, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43347298-c464-40d3-86ef-c6112643fa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 2\n",
      "Running node anomaly detection,  (24, 27551)\n",
      "Using precomputed node similarity\n",
      "noise_dim: 64, hid_dim: 128, num_layers: 2, dropout: 0.3, act: <class 'torch.nn.modules.activation.ReLU'>, backbone: None, contamination: 0.375, lr: 5e-05, epoch: 200, gpu: 0, batch_size: 1, verbose: 1, isn: False, th: 0.93\n",
      "Create GAAN model\n",
      "Node Anomaly Detection task: expecting only one convergence/divergence graph\n",
      "GPU: 0\n",
      "Epoch 0000: Loss I 0.8575 | Loss O 2.1773 | \n",
      "Epoch 0001: Loss I 0.7731 | Loss O 2.1722 | \n",
      "Epoch 0002: Loss I 0.7932 | Loss O 2.3381 | \n",
      "Epoch 0003: Loss I 0.7744 | Loss O 1.9024 | \n",
      "Epoch 0004: Loss I 0.6597 | Loss O 4.7480 | \n",
      "Epoch 0005: Loss I 0.6789 | Loss O 2.0075 | \n",
      "Epoch 0006: Loss I 0.6714 | Loss O 1.7731 | \n",
      "Epoch 0007: Loss I 0.6345 | Loss O 1.8572 | \n",
      "Epoch 0008: Loss I 0.5870 | Loss O 1.8713 | \n",
      "Epoch 0009: Loss I 0.5885 | Loss O 2.3111 | \n",
      "Epoch 0010: Loss I 0.5557 | Loss O 1.7493 | \n",
      "Epoch 0011: Loss I 0.5377 | Loss O 1.6278 | \n",
      "Epoch 0012: Loss I 0.5438 | Loss O 1.6933 | \n",
      "Epoch 0013: Loss I 0.4416 | Loss O 1.6746 | \n",
      "Epoch 0014: Loss I 0.4195 | Loss O 1.8621 | \n",
      "Epoch 0015: Loss I 0.4077 | Loss O 1.7152 | \n",
      "Epoch 0016: Loss I 0.4039 | Loss O 1.7631 | \n",
      "Epoch 0017: Loss I 0.3502 | Loss O 1.8797 | \n",
      "Epoch 0018: Loss I 0.3231 | Loss O 1.7144 | \n",
      "Epoch 0019: Loss I 0.2784 | Loss O 1.8301 | \n",
      "Epoch 0020: Loss I 0.2794 | Loss O 1.6688 | \n",
      "Epoch 0021: Loss I 0.2698 | Loss O 1.6112 | \n",
      "Epoch 0022: Loss I 0.2275 | Loss O 1.6600 | \n",
      "Epoch 0023: Loss I 0.2089 | Loss O 1.7637 | \n",
      "Epoch 0024: Loss I 0.2057 | Loss O 1.5851 | \n",
      "Epoch 0025: Loss I 0.1808 | Loss O 1.9231 | \n",
      "Epoch 0026: Loss I 0.1734 | Loss O 1.8822 | \n",
      "Epoch 0027: Loss I 0.1431 | Loss O 1.8870 | \n",
      "Epoch 0028: Loss I 0.1283 | Loss O 2.1296 | \n",
      "Epoch 0029: Loss I 0.1244 | Loss O 1.9990 | \n",
      "Epoch 0030: Loss I 0.1144 | Loss O 1.9441 | \n",
      "Epoch 0031: Loss I 0.1032 | Loss O 2.2121 | \n",
      "Epoch 0032: Loss I 0.0972 | Loss O 1.9695 | \n",
      "Epoch 0033: Loss I 0.0829 | Loss O 2.0122 | \n",
      "Epoch 0034: Loss I 0.0719 | Loss O 2.1194 | \n",
      "Epoch 0035: Loss I 0.0584 | Loss O 2.2259 | \n",
      "Epoch 0036: Loss I 0.0631 | Loss O 2.1914 | \n",
      "Epoch 0037: Loss I 0.0571 | Loss O 2.3153 | \n",
      "Epoch 0038: Loss I 0.0495 | Loss O 2.2703 | \n",
      "Epoch 0039: Loss I 0.0467 | Loss O 2.2248 | \n",
      "Epoch 0040: Loss I 0.0446 | Loss O 2.3023 | \n",
      "Epoch 0041: Loss I 0.0461 | Loss O 2.3366 | \n",
      "Epoch 0042: Loss I 0.0356 | Loss O 2.4290 | \n",
      "Epoch 0043: Loss I 0.0352 | Loss O 2.4973 | \n",
      "Epoch 0044: Loss I 0.0343 | Loss O 2.5091 | \n",
      "Epoch 0045: Loss I 0.0327 | Loss O 2.4158 | \n",
      "Epoch 0046: Loss I 0.0338 | Loss O 2.5355 | \n",
      "Epoch 0047: Loss I 0.0287 | Loss O 2.4821 | \n",
      "Epoch 0048: Loss I 0.0265 | Loss O 2.5493 | \n",
      "Epoch 0049: Loss I 0.0258 | Loss O 2.5826 | \n",
      "Epoch 0050: Loss I 0.0263 | Loss O 2.5506 | \n",
      "Epoch 0051: Loss I 0.0233 | Loss O 2.6255 | \n",
      "Epoch 0052: Loss I 0.0216 | Loss O 2.6975 | \n",
      "Epoch 0053: Loss I 0.0194 | Loss O 2.7211 | \n",
      "Epoch 0054: Loss I 0.0183 | Loss O 2.6395 | \n",
      "Epoch 0055: Loss I 0.0179 | Loss O 2.8068 | \n",
      "Epoch 0056: Loss I 0.0166 | Loss O 2.7399 | \n",
      "Epoch 0057: Loss I 0.0172 | Loss O 2.6855 | \n",
      "Epoch 0058: Loss I 0.0168 | Loss O 2.7521 | \n",
      "Epoch 0059: Loss I 0.0142 | Loss O 2.7419 | \n",
      "Epoch 0060: Loss I 0.0142 | Loss O 2.8420 | \n",
      "Epoch 0061: Loss I 0.0128 | Loss O 2.8676 | \n",
      "Epoch 0062: Loss I 0.0128 | Loss O 2.8682 | \n",
      "Epoch 0063: Loss I 0.0121 | Loss O 2.8864 | \n",
      "Epoch 0064: Loss I 0.0118 | Loss O 2.8784 | \n",
      "Epoch 0065: Loss I 0.0114 | Loss O 2.8862 | \n",
      "Epoch 0066: Loss I 0.0109 | Loss O 2.9639 | \n",
      "Epoch 0067: Loss I 0.0104 | Loss O 2.9235 | \n",
      "Epoch 0068: Loss I 0.0095 | Loss O 2.9758 | \n",
      "Epoch 0069: Loss I 0.0101 | Loss O 2.9456 | \n",
      "Epoch 0070: Loss I 0.0099 | Loss O 3.0219 | \n",
      "Epoch 0071: Loss I 0.0089 | Loss O 2.9494 | \n",
      "Epoch 0072: Loss I 0.0086 | Loss O 3.0284 | \n",
      "Epoch 0073: Loss I 0.0080 | Loss O 3.1118 | \n",
      "Epoch 0074: Loss I 0.0083 | Loss O 3.1068 | \n",
      "Epoch 0075: Loss I 0.0076 | Loss O 3.0933 | \n",
      "Epoch 0076: Loss I 0.0077 | Loss O 3.0904 | \n",
      "Epoch 0077: Loss I 0.0071 | Loss O 3.1015 | \n",
      "Epoch 0078: Loss I 0.0066 | Loss O 3.1190 | \n",
      "Epoch 0079: Loss I 0.0069 | Loss O 3.1326 | \n",
      "Epoch 0080: Loss I 0.0066 | Loss O 3.1093 | \n",
      "Epoch 0081: Loss I 0.0063 | Loss O 3.1092 | \n",
      "Epoch 0082: Loss I 0.0069 | Loss O 3.1140 | \n",
      "Epoch 0083: Loss I 0.0065 | Loss O 3.1561 | \n",
      "Epoch 0084: Loss I 0.0060 | Loss O 3.2707 | \n",
      "Epoch 0085: Loss I 0.0059 | Loss O 3.3015 | \n",
      "Epoch 0086: Loss I 0.0055 | Loss O 3.2252 | \n",
      "Epoch 0087: Loss I 0.0055 | Loss O 3.1922 | \n",
      "Epoch 0088: Loss I 0.0052 | Loss O 3.3088 | \n",
      "Epoch 0089: Loss I 0.0051 | Loss O 3.3292 | \n",
      "Epoch 0090: Loss I 0.0046 | Loss O 3.3272 | \n",
      "Epoch 0091: Loss I 0.0046 | Loss O 3.3233 | \n",
      "Epoch 0092: Loss I 0.0047 | Loss O 3.3781 | \n",
      "Epoch 0093: Loss I 0.0047 | Loss O 3.3150 | \n",
      "Epoch 0094: Loss I 0.0044 | Loss O 3.2725 | \n",
      "Epoch 0095: Loss I 0.0043 | Loss O 3.2336 | \n",
      "Epoch 0096: Loss I 0.0040 | Loss O 3.3605 | \n",
      "Epoch 0097: Loss I 0.0042 | Loss O 3.5087 | \n",
      "Epoch 0098: Loss I 0.0040 | Loss O 3.3185 | \n",
      "Epoch 0099: Loss I 0.0037 | Loss O 3.4627 | \n",
      "Epoch 0100: Loss I 0.0036 | Loss O 3.3970 | \n",
      "Epoch 0101: Loss I 0.0036 | Loss O 3.4912 | \n",
      "Epoch 0102: Loss I 0.0037 | Loss O 3.3996 | \n",
      "Epoch 0103: Loss I 0.0036 | Loss O 3.4528 | \n",
      "Epoch 0104: Loss I 0.0034 | Loss O 3.4153 | \n",
      "Epoch 0105: Loss I 0.0034 | Loss O 3.4311 | \n",
      "Epoch 0106: Loss I 0.0032 | Loss O 3.4639 | \n",
      "Epoch 0107: Loss I 0.0029 | Loss O 3.5475 | \n",
      "Epoch 0108: Loss I 0.0029 | Loss O 3.5242 | \n",
      "Epoch 0109: Loss I 0.0029 | Loss O 3.5492 | \n",
      "Epoch 0110: Loss I 0.0031 | Loss O 3.4865 | \n",
      "Epoch 0111: Loss I 0.0031 | Loss O 3.4457 | \n",
      "Epoch 0112: Loss I 0.0028 | Loss O 3.5296 | \n",
      "Epoch 0113: Loss I 0.0027 | Loss O 3.6576 | \n",
      "Epoch 0114: Loss I 0.0028 | Loss O 3.5456 | \n",
      "Epoch 0115: Loss I 0.0029 | Loss O 3.6072 | \n",
      "Epoch 0116: Loss I 0.0026 | Loss O 3.6060 | \n",
      "Epoch 0117: Loss I 0.0026 | Loss O 3.6360 | \n",
      "Epoch 0118: Loss I 0.0024 | Loss O 3.5937 | \n",
      "Epoch 0119: Loss I 0.0024 | Loss O 3.5747 | \n",
      "Epoch 0120: Loss I 0.0026 | Loss O 3.6108 | \n",
      "Epoch 0121: Loss I 0.0024 | Loss O 3.6943 | \n",
      "Epoch 0122: Loss I 0.0023 | Loss O 3.7141 | \n",
      "Epoch 0123: Loss I 0.0022 | Loss O 3.6525 | \n",
      "Epoch 0124: Loss I 0.0022 | Loss O 3.6357 | \n",
      "Epoch 0125: Loss I 0.0021 | Loss O 3.6493 | \n",
      "Epoch 0126: Loss I 0.0021 | Loss O 3.6869 | \n",
      "Epoch 0127: Loss I 0.0021 | Loss O 3.6657 | \n",
      "Epoch 0128: Loss I 0.0020 | Loss O 3.6642 | \n",
      "Epoch 0129: Loss I 0.0019 | Loss O 3.6331 | \n",
      "Epoch 0130: Loss I 0.0019 | Loss O 3.6776 | \n",
      "Epoch 0131: Loss I 0.0020 | Loss O 3.6976 | \n",
      "Epoch 0132: Loss I 0.0019 | Loss O 3.6502 | \n",
      "Epoch 0133: Loss I 0.0018 | Loss O 3.6465 | \n",
      "Epoch 0134: Loss I 0.0018 | Loss O 3.7164 | \n",
      "Epoch 0135: Loss I 0.0016 | Loss O 3.7624 | \n",
      "Epoch 0136: Loss I 0.0017 | Loss O 3.7841 | \n",
      "Epoch 0137: Loss I 0.0016 | Loss O 3.8190 | \n",
      "Epoch 0138: Loss I 0.0016 | Loss O 3.8408 | \n",
      "Epoch 0139: Loss I 0.0018 | Loss O 3.7098 | \n",
      "Epoch 0140: Loss I 0.0016 | Loss O 3.8198 | \n",
      "Epoch 0141: Loss I 0.0017 | Loss O 3.7665 | \n",
      "Epoch 0142: Loss I 0.0015 | Loss O 3.7912 | \n",
      "Epoch 0143: Loss I 0.0016 | Loss O 3.8552 | \n",
      "Epoch 0144: Loss I 0.0015 | Loss O 3.9019 | \n",
      "Epoch 0145: Loss I 0.0015 | Loss O 3.9063 | \n",
      "Epoch 0146: Loss I 0.0014 | Loss O 3.8733 | \n",
      "Epoch 0147: Loss I 0.0014 | Loss O 3.9291 | \n",
      "Epoch 0148: Loss I 0.0014 | Loss O 3.9090 | \n",
      "Epoch 0149: Loss I 0.0014 | Loss O 3.9286 | \n",
      "Epoch 0150: Loss I 0.0013 | Loss O 3.8913 | \n",
      "Epoch 0151: Loss I 0.0013 | Loss O 3.8899 | \n",
      "Epoch 0152: Loss I 0.0012 | Loss O 3.9820 | \n",
      "Epoch 0153: Loss I 0.0013 | Loss O 3.8959 | \n",
      "Epoch 0154: Loss I 0.0012 | Loss O 3.9289 | \n",
      "Epoch 0155: Loss I 0.0012 | Loss O 3.9693 | \n",
      "Epoch 0156: Loss I 0.0012 | Loss O 3.9216 | \n",
      "Epoch 0157: Loss I 0.0012 | Loss O 3.9118 | \n",
      "Epoch 0158: Loss I 0.0012 | Loss O 3.9574 | \n",
      "Epoch 0159: Loss I 0.0011 | Loss O 4.1509 | \n",
      "Epoch 0160: Loss I 0.0010 | Loss O 3.9874 | \n",
      "Epoch 0161: Loss I 0.0011 | Loss O 4.0670 | \n",
      "Epoch 0162: Loss I 0.0010 | Loss O 3.9904 | \n",
      "Epoch 0163: Loss I 0.0011 | Loss O 4.0857 | \n",
      "Epoch 0164: Loss I 0.0011 | Loss O 3.9739 | \n",
      "Epoch 0165: Loss I 0.0011 | Loss O 4.0024 | \n",
      "Epoch 0166: Loss I 0.0010 | Loss O 4.0638 | \n",
      "Epoch 0167: Loss I 0.0010 | Loss O 3.9940 | \n",
      "Epoch 0168: Loss I 0.0010 | Loss O 4.0230 | \n",
      "Epoch 0169: Loss I 0.0009 | Loss O 4.0565 | \n",
      "Epoch 0170: Loss I 0.0010 | Loss O 3.9663 | \n",
      "Epoch 0171: Loss I 0.0009 | Loss O 4.0540 | \n",
      "Epoch 0172: Loss I 0.0009 | Loss O 4.0903 | \n",
      "Epoch 0173: Loss I 0.0010 | Loss O 4.0925 | \n",
      "Epoch 0174: Loss I 0.0009 | Loss O 4.0742 | \n",
      "Epoch 0175: Loss I 0.0009 | Loss O 4.0123 | \n",
      "Epoch 0176: Loss I 0.0009 | Loss O 4.1524 | \n",
      "Epoch 0177: Loss I 0.0008 | Loss O 4.0674 | \n",
      "Epoch 0178: Loss I 0.0009 | Loss O 4.1159 | \n",
      "Epoch 0179: Loss I 0.0008 | Loss O 4.1670 | \n",
      "Epoch 0180: Loss I 0.0008 | Loss O 4.0653 | \n",
      "Epoch 0181: Loss I 0.0009 | Loss O 4.0292 | \n",
      "Epoch 0182: Loss I 0.0008 | Loss O 4.1762 | \n",
      "Epoch 0183: Loss I 0.0008 | Loss O 4.1193 | \n",
      "Epoch 0184: Loss I 0.0008 | Loss O 4.1304 | \n",
      "Epoch 0185: Loss I 0.0008 | Loss O 4.0673 | \n",
      "Epoch 0186: Loss I 0.0008 | Loss O 4.1993 | \n",
      "Epoch 0187: Loss I 0.0007 | Loss O 6.8855 | \n",
      "Epoch 0188: Loss I 0.0008 | Loss O 4.1606 | \n",
      "Epoch 0189: Loss I 0.0007 | Loss O 4.2367 | \n",
      "Epoch 0190: Loss I 0.0007 | Loss O 4.1661 | \n",
      "Epoch 0191: Loss I 0.0007 | Loss O 4.2528 | \n",
      "Epoch 0192: Loss I 0.0007 | Loss O 4.1172 | \n",
      "Epoch 0193: Loss I 0.0007 | Loss O 4.3099 | \n",
      "Epoch 0194: Loss I 0.0007 | Loss O 4.2001 | \n",
      "Epoch 0195: Loss I 0.0007 | Loss O 4.2272 | \n",
      "Epoch 0196: Loss I 0.0007 | Loss O 4.2072 | \n",
      "Epoch 0197: Loss I 0.0006 | Loss O 4.1892 | \n",
      "Epoch 0198: Loss I 0.0007 | Loss O 4.2375 | \n",
      "Epoch 0199: Loss I 0.0006 | Loss O 4.3090 | \n",
      "Epoch 0000: Loss 0.1556 | \n",
      "Epoch 0001: Loss 0.1532 | \n",
      "Epoch 0002: Loss 0.1523 | \n",
      "Epoch 0003: Loss 0.1517 | \n",
      "Epoch 0004: Loss 0.1508 | \n",
      "Epoch 0005: Loss 0.1493 | \n",
      "Epoch 0006: Loss 0.1475 | \n",
      "Epoch 0007: Loss 0.1453 | \n",
      "Epoch 0008: Loss 0.1427 | \n",
      "Epoch 0009: Loss 0.1398 | \n",
      "Epoch 0010: Loss 0.1365 | \n",
      "Epoch 0011: Loss 0.1328 | \n",
      "Epoch 0012: Loss 0.1287 | \n",
      "Epoch 0013: Loss 0.1243 | \n",
      "Epoch 0014: Loss 0.1194 | \n",
      "Epoch 0015: Loss 0.1143 | \n",
      "Epoch 0016: Loss 0.1089 | \n",
      "Epoch 0017: Loss 0.1034 | \n",
      "Epoch 0018: Loss 0.0976 | \n",
      "Epoch 0019: Loss 0.0919 | \n",
      "Epoch 0020: Loss 0.0862 | \n",
      "Epoch 0021: Loss 0.0805 | \n",
      "Epoch 0022: Loss 0.0751 | \n",
      "Epoch 0023: Loss 0.0698 | \n",
      "Epoch 0024: Loss 0.0649 | \n",
      "Epoch 0025: Loss 0.0602 | \n",
      "Epoch 0026: Loss 0.0558 | \n",
      "Epoch 0027: Loss 0.0517 | \n",
      "Epoch 0028: Loss 0.0480 | \n",
      "Epoch 0029: Loss 0.0445 | \n",
      "Epoch 0030: Loss 0.0414 | \n",
      "Epoch 0031: Loss 0.0386 | \n",
      "Epoch 0032: Loss 0.0361 | \n",
      "Epoch 0033: Loss 0.0338 | \n",
      "Epoch 0034: Loss 0.0318 | \n",
      "Epoch 0035: Loss 0.0300 | \n",
      "Epoch 0036: Loss 0.0284 | \n",
      "Epoch 0037: Loss 0.0269 | \n",
      "Epoch 0038: Loss 0.0256 | \n",
      "Epoch 0039: Loss 0.0245 | \n",
      "Epoch 0040: Loss 0.0235 | \n",
      "Epoch 0041: Loss 0.0226 | \n",
      "Epoch 0042: Loss 0.0218 | \n",
      "Epoch 0043: Loss 0.0211 | \n",
      "Epoch 0044: Loss 0.0205 | \n",
      "Epoch 0045: Loss 0.0200 | \n",
      "Epoch 0046: Loss 0.0195 | \n",
      "Epoch 0047: Loss 0.0190 | \n",
      "Epoch 0048: Loss 0.0186 | \n",
      "Epoch 0049: Loss 0.0183 | \n",
      "Epoch 0050: Loss 0.0180 | \n",
      "Epoch 0051: Loss 0.0177 | \n",
      "Epoch 0052: Loss 0.0174 | \n",
      "Epoch 0053: Loss 0.0172 | \n",
      "Epoch 0054: Loss 0.0170 | \n",
      "Epoch 0055: Loss 0.0168 | \n",
      "Epoch 0056: Loss 0.0166 | \n",
      "Epoch 0057: Loss 0.0164 | \n",
      "Epoch 0058: Loss 0.0163 | \n",
      "Epoch 0059: Loss 0.0161 | \n",
      "Epoch 0060: Loss 0.0160 | \n",
      "Epoch 0061: Loss 0.0159 | \n",
      "Epoch 0062: Loss 0.0158 | \n",
      "Epoch 0063: Loss 0.0157 | \n",
      "Epoch 0064: Loss 0.0156 | \n",
      "Epoch 0065: Loss 0.0156 | \n",
      "Epoch 0066: Loss 0.0155 | \n",
      "Epoch 0067: Loss 0.0154 | \n",
      "Epoch 0068: Loss 0.0154 | \n",
      "Epoch 0069: Loss 0.0153 | \n",
      "Epoch 0070: Loss 0.0153 | \n",
      "Epoch 0071: Loss 0.0152 | \n",
      "Epoch 0072: Loss 0.0152 | \n",
      "Epoch 0073: Loss 0.0152 | \n",
      "Epoch 0074: Loss 0.0151 | \n",
      "Epoch 0075: Loss 0.0151 | \n",
      "Epoch 0076: Loss 0.0151 | \n",
      "Epoch 0077: Loss 0.0150 | \n",
      "Epoch 0078: Loss 0.0150 | \n",
      "Epoch 0079: Loss 0.0150 | \n",
      "Epoch 0080: Loss 0.0150 | \n",
      "Epoch 0081: Loss 0.0150 | \n",
      "Epoch 0082: Loss 0.0149 | \n",
      "Epoch 0083: Loss 0.0149 | \n",
      "Epoch 0084: Loss 0.0149 | \n",
      "Epoch 0085: Loss 0.0149 | \n",
      "Epoch 0086: Loss 0.0149 | \n",
      "Epoch 0087: Loss 0.0149 | \n",
      "Epoch 0088: Loss 0.0148 | \n",
      "Epoch 0089: Loss 0.0148 | \n",
      "Epoch 0090: Loss 0.0148 | \n",
      "Epoch 0091: Loss 0.0148 | \n",
      "Epoch 0092: Loss 0.0148 | \n",
      "Epoch 0093: Loss 0.0148 | \n",
      "Epoch 0094: Loss 0.0148 | \n",
      "Epoch 0095: Loss 0.0148 | \n",
      "Epoch 0096: Loss 0.0148 | \n",
      "Epoch 0097: Loss 0.0147 | \n",
      "Epoch 0098: Loss 0.0147 | \n",
      "Epoch 0099: Loss 0.0147 | \n",
      "Epoch 0100: Loss 0.0147 | \n",
      "Epoch 0101: Loss 0.0147 | \n",
      "Epoch 0102: Loss 0.0147 | \n",
      "Epoch 0103: Loss 0.0147 | \n",
      "Epoch 0104: Loss 0.0147 | \n",
      "Epoch 0105: Loss 0.0147 | \n",
      "Epoch 0106: Loss 0.0147 | \n",
      "Epoch 0107: Loss 0.0147 | \n",
      "Epoch 0108: Loss 0.0146 | \n",
      "Epoch 0109: Loss 0.0146 | \n",
      "Epoch 0110: Loss 0.0146 | \n",
      "Epoch 0111: Loss 0.0146 | \n",
      "Epoch 0112: Loss 0.0146 | \n",
      "Epoch 0113: Loss 0.0146 | \n",
      "Epoch 0114: Loss 0.0146 | \n",
      "Epoch 0115: Loss 0.0146 | \n",
      "Epoch 0116: Loss 0.0146 | \n",
      "Epoch 0117: Loss 0.0146 | \n",
      "Epoch 0118: Loss 0.0146 | \n",
      "Epoch 0119: Loss 0.0146 | \n",
      "Epoch 0120: Loss 0.0146 | \n",
      "Epoch 0121: Loss 0.0145 | \n",
      "Epoch 0122: Loss 0.0145 | \n",
      "Epoch 0123: Loss 0.0145 | \n",
      "Epoch 0124: Loss 0.0145 | \n",
      "Epoch 0125: Loss 0.0145 | \n",
      "Epoch 0126: Loss 0.0145 | \n",
      "Epoch 0127: Loss 0.0145 | \n",
      "Epoch 0128: Loss 0.0145 | \n",
      "Epoch 0129: Loss 0.0145 | \n",
      "Epoch 0130: Loss 0.0145 | \n",
      "Epoch 0131: Loss 0.0145 | \n",
      "Epoch 0132: Loss 0.0145 | \n",
      "Epoch 0133: Loss 0.0145 | \n",
      "Epoch 0134: Loss 0.0145 | \n",
      "Epoch 0135: Loss 0.0144 | \n",
      "Epoch 0136: Loss 0.0144 | \n",
      "Epoch 0137: Loss 0.0144 | \n",
      "Epoch 0138: Loss 0.0144 | \n",
      "Epoch 0139: Loss 0.0144 | \n",
      "Epoch 0140: Loss 0.0144 | \n",
      "Epoch 0141: Loss 0.0144 | \n",
      "Epoch 0142: Loss 0.0144 | \n",
      "Epoch 0143: Loss 0.0144 | \n",
      "Epoch 0144: Loss 0.0144 | \n",
      "Epoch 0145: Loss 0.0144 | \n",
      "Epoch 0146: Loss 0.0144 | \n",
      "Epoch 0147: Loss 0.0144 | \n",
      "Epoch 0148: Loss 0.0144 | \n",
      "Epoch 0149: Loss 0.0143 | \n",
      "Epoch 0150: Loss 0.0143 | \n",
      "Epoch 0151: Loss 0.0143 | \n",
      "Epoch 0152: Loss 0.0143 | \n",
      "Epoch 0153: Loss 0.0143 | \n",
      "Epoch 0154: Loss 0.0143 | \n",
      "Epoch 0155: Loss 0.0143 | \n",
      "Epoch 0156: Loss 0.0143 | \n",
      "Epoch 0157: Loss 0.0143 | \n",
      "Epoch 0158: Loss 0.0143 | \n",
      "Epoch 0159: Loss 0.0143 | \n",
      "Epoch 0160: Loss 0.0143 | \n",
      "Epoch 0161: Loss 0.0143 | \n",
      "Epoch 0162: Loss 0.0142 | \n",
      "Epoch 0163: Loss 0.0142 | \n",
      "Epoch 0164: Loss 0.0142 | \n",
      "Epoch 0165: Loss 0.0142 | \n",
      "Epoch 0166: Loss 0.0142 | \n",
      "Epoch 0167: Loss 0.0142 | \n",
      "Epoch 0168: Loss 0.0142 | \n",
      "Epoch 0169: Loss 0.0142 | \n",
      "Epoch 0170: Loss 0.0142 | \n",
      "Epoch 0171: Loss 0.0142 | \n",
      "Epoch 0172: Loss 0.0142 | \n",
      "Epoch 0173: Loss 0.0142 | \n",
      "Epoch 0174: Loss 0.0141 | \n",
      "Epoch 0175: Loss 0.0141 | \n",
      "Epoch 0176: Loss 0.0141 | \n",
      "Epoch 0177: Loss 0.0141 | \n",
      "Epoch 0178: Loss 0.0141 | \n",
      "Epoch 0179: Loss 0.0141 | \n",
      "Epoch 0180: Loss 0.0141 | \n",
      "Epoch 0181: Loss 0.0141 | \n",
      "Epoch 0182: Loss 0.0141 | \n",
      "Epoch 0183: Loss 0.0141 | \n",
      "Epoch 0184: Loss 0.0141 | \n",
      "Epoch 0185: Loss 0.0140 | \n",
      "Epoch 0186: Loss 0.0140 | \n",
      "Epoch 0187: Loss 0.0140 | \n",
      "Epoch 0188: Loss 0.0140 | \n",
      "Epoch 0189: Loss 0.0140 | \n",
      "Epoch 0190: Loss 0.0140 | \n",
      "Epoch 0191: Loss 0.0140 | \n",
      "Epoch 0192: Loss 0.0140 | \n",
      "Epoch 0193: Loss 0.0140 | \n",
      "Epoch 0194: Loss 0.0139 | \n",
      "Epoch 0195: Loss 0.0139 | \n",
      "Epoch 0196: Loss 0.0139 | \n",
      "Epoch 0197: Loss 0.0139 | \n",
      "Epoch 0198: Loss 0.0139 | \n",
      "Epoch 0199: Loss 0.0139 | \n",
      "Training GCN, epoch 0\n",
      "Training GCN, epoch 100\n",
      "Training GCN, epoch 200\n",
      "Training GCN, epoch 300\n",
      "Training GCN, epoch 400\n",
      "Training GCN, epoch 500\n",
      "Training GCN, epoch 600\n",
      "Training GCN, epoch 700\n",
      "Training GCN, epoch 800\n",
      "Training GCN, epoch 900\n",
      "Training GCN, epoch 1000\n",
      "Training GCN, epoch 1100\n",
      "Training GCN, epoch 1200\n",
      "Training GCN, epoch 1300\n",
      "Training GCN, epoch 1400\n",
      "Training GCN, epoch 1500\n",
      "Training GCN, epoch 1600\n",
      "Training GCN, epoch 1700\n",
      "Training GCN, epoch 1800\n",
      "Training GCN, epoch 1900\n",
      "Test: Loss I 0.0016 | Loss O 0.7674 | \n",
      "Test: Loss 0.0344 | \n",
      "Running node anomaly detection,  (132, 18981)\n",
      "Using precomputed node similarity\n",
      "noise_dim: 64, hid_dim: 128, num_layers: 2, dropout: 0.3, act: <class 'torch.nn.modules.activation.ReLU'>, backbone: None, contamination: 0.4166666666666667, lr: 5e-05, epoch: 200, gpu: 0, batch_size: 1, verbose: 1, isn: False, th: 0.93\n",
      "Create GAAN model\n",
      "Node Anomaly Detection task: expecting only one convergence/divergence graph\n",
      "GPU: 0\n",
      "Epoch 0000: Loss I 0.7892 | Loss O 0.6517 | \n",
      "Epoch 0001: Loss I 0.4767 | Loss O 0.6464 | \n",
      "Epoch 0002: Loss I 0.2739 | Loss O 0.8145 | \n",
      "Epoch 0003: Loss I 0.1533 | Loss O 1.0505 | \n",
      "Epoch 0004: Loss I 0.0895 | Loss O 1.2888 | \n",
      "Epoch 0005: Loss I 0.0562 | Loss O 1.5033 | \n",
      "Epoch 0006: Loss I 0.0379 | Loss O 1.6897 | \n",
      "Epoch 0007: Loss I 0.0270 | Loss O 1.8529 | \n",
      "Epoch 0008: Loss I 0.0201 | Loss O 1.9969 | \n",
      "Epoch 0009: Loss I 0.0155 | Loss O 2.1240 | \n",
      "Epoch 0010: Loss I 0.0122 | Loss O 2.2382 | \n",
      "Epoch 0011: Loss I 0.0099 | Loss O 2.3419 | \n",
      "Epoch 0012: Loss I 0.0082 | Loss O 2.4374 | \n",
      "Epoch 0013: Loss I 0.0068 | Loss O 2.5267 | \n",
      "Epoch 0014: Loss I 0.0058 | Loss O 2.6070 | \n",
      "Epoch 0015: Loss I 0.0049 | Loss O 2.6839 | \n",
      "Epoch 0016: Loss I 0.0043 | Loss O 2.7556 | \n",
      "Epoch 0017: Loss I 0.0037 | Loss O 2.8244 | \n",
      "Epoch 0018: Loss I 0.0033 | Loss O 2.8882 | \n",
      "Epoch 0019: Loss I 0.0029 | Loss O 2.9504 | \n",
      "Epoch 0020: Loss I 0.0026 | Loss O 3.0091 | \n",
      "Epoch 0021: Loss I 0.0023 | Loss O 3.0662 | \n",
      "Epoch 0022: Loss I 0.0021 | Loss O 3.1190 | \n",
      "Epoch 0023: Loss I 0.0018 | Loss O 3.1720 | \n",
      "Epoch 0024: Loss I 0.0017 | Loss O 3.2224 | \n",
      "Epoch 0025: Loss I 0.0015 | Loss O 3.2715 | \n",
      "Epoch 0026: Loss I 0.0014 | Loss O 3.3186 | \n",
      "Epoch 0027: Loss I 0.0013 | Loss O 3.3658 | \n",
      "Epoch 0028: Loss I 0.0011 | Loss O 3.4101 | \n",
      "Epoch 0029: Loss I 0.0010 | Loss O 3.4533 | \n",
      "Epoch 0030: Loss I 0.0010 | Loss O 3.4963 | \n",
      "Epoch 0031: Loss I 0.0009 | Loss O 3.5377 | \n",
      "Epoch 0032: Loss I 0.0008 | Loss O 3.5781 | \n",
      "Epoch 0033: Loss I 0.0008 | Loss O 3.6182 | \n",
      "Epoch 0034: Loss I 0.0007 | Loss O 3.6566 | \n",
      "Epoch 0035: Loss I 0.0006 | Loss O 3.6953 | \n",
      "Epoch 0036: Loss I 0.0006 | Loss O 3.7343 | \n",
      "Epoch 0037: Loss I 0.0006 | Loss O 3.7698 | \n",
      "Epoch 0038: Loss I 0.0005 | Loss O 3.8065 | \n",
      "Epoch 0039: Loss I 0.0005 | Loss O 3.8425 | \n",
      "Epoch 0040: Loss I 0.0004 | Loss O 3.8780 | \n",
      "Epoch 0041: Loss I 0.0004 | Loss O 3.9124 | \n",
      "Epoch 0042: Loss I 0.0004 | Loss O 3.9482 | \n",
      "Epoch 0043: Loss I 0.0004 | Loss O 3.9804 | \n",
      "Epoch 0044: Loss I 0.0003 | Loss O 4.0138 | \n",
      "Epoch 0045: Loss I 0.0003 | Loss O 4.0476 | \n",
      "Epoch 0046: Loss I 0.0003 | Loss O 4.0805 | \n",
      "Epoch 0047: Loss I 0.0003 | Loss O 4.1133 | \n",
      "Epoch 0048: Loss I 0.0003 | Loss O 4.1453 | \n",
      "Epoch 0049: Loss I 0.0002 | Loss O 4.1772 | \n",
      "Epoch 0050: Loss I 0.0002 | Loss O 4.2088 | \n",
      "Epoch 0051: Loss I 0.0002 | Loss O 4.2395 | \n",
      "Epoch 0052: Loss I 0.0002 | Loss O 4.2709 | \n",
      "Epoch 0053: Loss I 0.0002 | Loss O 4.3016 | \n",
      "Epoch 0054: Loss I 0.0002 | Loss O 4.3321 | \n",
      "Epoch 0055: Loss I 0.0002 | Loss O 4.3630 | \n",
      "Epoch 0056: Loss I 0.0002 | Loss O 4.3928 | \n",
      "Epoch 0057: Loss I 0.0002 | Loss O 4.4218 | \n",
      "Epoch 0058: Loss I 0.0001 | Loss O 4.4520 | \n",
      "Epoch 0059: Loss I 0.0001 | Loss O 4.4818 | \n",
      "Epoch 0060: Loss I 0.0001 | Loss O 4.5118 | \n",
      "Epoch 0061: Loss I 0.0001 | Loss O 4.5403 | \n",
      "Epoch 0062: Loss I 0.0001 | Loss O 4.5696 | \n",
      "Epoch 0063: Loss I 0.0001 | Loss O 4.5979 | \n",
      "Epoch 0064: Loss I 0.0001 | Loss O 4.6266 | \n",
      "Epoch 0065: Loss I 0.0001 | Loss O 4.6557 | \n",
      "Epoch 0066: Loss I 0.0001 | Loss O 4.6838 | \n",
      "Epoch 0067: Loss I 0.0001 | Loss O 4.7121 | \n",
      "Epoch 0068: Loss I 0.0001 | Loss O 4.7404 | \n",
      "Epoch 0069: Loss I 0.0001 | Loss O 4.7692 | \n",
      "Epoch 0070: Loss I 0.0001 | Loss O 4.7964 | \n",
      "Epoch 0071: Loss I 0.0001 | Loss O 4.8240 | \n",
      "Epoch 0072: Loss I 0.0001 | Loss O 4.8522 | \n",
      "Epoch 0073: Loss I 0.0001 | Loss O 4.8801 | \n",
      "Epoch 0074: Loss I 0.0001 | Loss O 4.9080 | \n",
      "Epoch 0075: Loss I 0.0001 | Loss O 4.9349 | \n",
      "Epoch 0076: Loss I 0.0001 | Loss O 4.9623 | \n",
      "Epoch 0077: Loss I 0.0000 | Loss O 4.9898 | \n",
      "Epoch 0078: Loss I 0.0000 | Loss O 5.0174 | \n",
      "Epoch 0079: Loss I 0.0000 | Loss O 5.0445 | \n",
      "Epoch 0080: Loss I 0.0000 | Loss O 5.0710 | \n",
      "Epoch 0081: Loss I 0.0000 | Loss O 5.0983 | \n",
      "Epoch 0082: Loss I 0.0000 | Loss O 5.1248 | \n",
      "Epoch 0083: Loss I 0.0000 | Loss O 5.1523 | \n",
      "Epoch 0084: Loss I 0.0000 | Loss O 5.1793 | \n",
      "Epoch 0085: Loss I 0.0000 | Loss O 5.2058 | \n",
      "Epoch 0086: Loss I 0.0000 | Loss O 5.2323 | \n",
      "Epoch 0087: Loss I 0.0000 | Loss O 5.2589 | \n",
      "Epoch 0088: Loss I 0.0000 | Loss O 5.2855 | \n",
      "Epoch 0089: Loss I 0.0000 | Loss O 5.3121 | \n",
      "Epoch 0090: Loss I 0.0000 | Loss O 5.3386 | \n",
      "Epoch 0091: Loss I 0.0000 | Loss O 5.3652 | \n",
      "Epoch 0092: Loss I 0.0000 | Loss O 5.3916 | \n",
      "Epoch 0093: Loss I 0.0000 | Loss O 5.4183 | \n",
      "Epoch 0094: Loss I 0.0000 | Loss O 5.4442 | \n",
      "Epoch 0095: Loss I 0.0000 | Loss O 5.4708 | \n",
      "Epoch 0096: Loss I 0.0000 | Loss O 5.4961 | \n",
      "Epoch 0097: Loss I 0.0000 | Loss O 5.5238 | \n",
      "Epoch 0098: Loss I 0.0000 | Loss O 5.5486 | \n",
      "Epoch 0099: Loss I 0.0000 | Loss O 5.5751 | \n",
      "Epoch 0100: Loss I 0.0000 | Loss O 5.6012 | \n",
      "Epoch 0101: Loss I 0.0000 | Loss O 5.6274 | \n",
      "Epoch 0102: Loss I 0.0000 | Loss O 5.6526 | \n",
      "Epoch 0103: Loss I 0.0000 | Loss O 5.6789 | \n",
      "Epoch 0104: Loss I 0.0000 | Loss O 5.7047 | \n",
      "Epoch 0105: Loss I 0.0000 | Loss O 5.7312 | \n",
      "Epoch 0106: Loss I 0.0000 | Loss O 5.7571 | \n",
      "Epoch 0107: Loss I 0.0000 | Loss O 5.7824 | \n",
      "Epoch 0108: Loss I 0.0000 | Loss O 5.8092 | \n",
      "Epoch 0109: Loss I 0.0000 | Loss O 5.8349 | \n",
      "Epoch 0110: Loss I 0.0000 | Loss O 5.8601 | \n",
      "Epoch 0111: Loss I 0.0000 | Loss O 5.8867 | \n",
      "Epoch 0112: Loss I 0.0000 | Loss O 5.9122 | \n",
      "Epoch 0113: Loss I 0.0000 | Loss O 5.9374 | \n",
      "Epoch 0114: Loss I 0.0000 | Loss O 5.9640 | \n",
      "Epoch 0115: Loss I 0.0000 | Loss O 5.9890 | \n",
      "Epoch 0116: Loss I 0.0000 | Loss O 6.0154 | \n",
      "Epoch 0117: Loss I 0.0000 | Loss O 6.0405 | \n",
      "Epoch 0118: Loss I 0.0000 | Loss O 6.0659 | \n",
      "Epoch 0119: Loss I 0.0000 | Loss O 6.0911 | \n",
      "Epoch 0120: Loss I 0.0000 | Loss O 6.1169 | \n",
      "Epoch 0121: Loss I 0.0000 | Loss O 6.1418 | \n",
      "Epoch 0122: Loss I 0.0000 | Loss O 6.1680 | \n",
      "Epoch 0123: Loss I 0.0000 | Loss O 6.1934 | \n",
      "Epoch 0124: Loss I 0.0000 | Loss O 6.2197 | \n",
      "Epoch 0125: Loss I 0.0000 | Loss O 6.2449 | \n",
      "Epoch 0126: Loss I 0.0000 | Loss O 6.2697 | \n",
      "Epoch 0127: Loss I 0.0000 | Loss O 6.2957 | \n",
      "Epoch 0128: Loss I 0.0000 | Loss O 6.3213 | \n",
      "Epoch 0129: Loss I 0.0000 | Loss O 6.3465 | \n",
      "Epoch 0130: Loss I 0.0000 | Loss O 6.3716 | \n",
      "Epoch 0131: Loss I 0.0000 | Loss O 6.3971 | \n",
      "Epoch 0132: Loss I 0.0000 | Loss O 6.4228 | \n",
      "Epoch 0133: Loss I 0.0000 | Loss O 6.4471 | \n",
      "Epoch 0134: Loss I 0.0000 | Loss O 6.4726 | \n",
      "Epoch 0135: Loss I 0.0000 | Loss O 6.4982 | \n",
      "Epoch 0136: Loss I 0.0000 | Loss O 6.5242 | \n",
      "Epoch 0137: Loss I 0.0000 | Loss O 6.5482 | \n",
      "Epoch 0138: Loss I 0.0000 | Loss O 6.5741 | \n",
      "Epoch 0139: Loss I 0.0000 | Loss O 6.5995 | \n",
      "Epoch 0140: Loss I 0.0000 | Loss O 6.6242 | \n",
      "Epoch 0141: Loss I 0.0000 | Loss O 6.6498 | \n",
      "Epoch 0142: Loss I 0.0000 | Loss O 6.6745 | \n",
      "Epoch 0143: Loss I 0.0000 | Loss O 6.6998 | \n",
      "Epoch 0144: Loss I 0.0000 | Loss O 6.7259 | \n",
      "Epoch 0145: Loss I 0.0000 | Loss O 6.7503 | \n",
      "Epoch 0146: Loss I 0.0000 | Loss O 6.7754 | \n",
      "Epoch 0147: Loss I 0.0000 | Loss O 6.8007 | \n",
      "Epoch 0148: Loss I 0.0000 | Loss O 6.8255 | \n",
      "Epoch 0149: Loss I 0.0000 | Loss O 6.8515 | \n",
      "Epoch 0150: Loss I 0.0000 | Loss O 6.8765 | \n",
      "Epoch 0151: Loss I 0.0000 | Loss O 6.9013 | \n",
      "Epoch 0152: Loss I 0.0000 | Loss O 6.9264 | \n",
      "Epoch 0153: Loss I 0.0000 | Loss O 6.9515 | \n",
      "Epoch 0154: Loss I 0.0000 | Loss O 6.9771 | \n",
      "Epoch 0155: Loss I 0.0000 | Loss O 7.0018 | \n",
      "Epoch 0156: Loss I 0.0000 | Loss O 7.0273 | \n",
      "Epoch 0157: Loss I 0.0000 | Loss O 7.0526 | \n",
      "Epoch 0158: Loss I 0.0000 | Loss O 7.0773 | \n",
      "Epoch 0159: Loss I 0.0000 | Loss O 7.1034 | \n",
      "Epoch 0160: Loss I 0.0000 | Loss O 7.1289 | \n",
      "Epoch 0161: Loss I 0.0000 | Loss O 7.1561 | \n",
      "Epoch 0162: Loss I 0.0000 | Loss O 7.1824 | \n",
      "Epoch 0163: Loss I 0.0000 | Loss O 7.2085 | \n",
      "Epoch 0164: Loss I 0.0000 | Loss O 7.2377 | \n",
      "Epoch 0165: Loss I 0.0000 | Loss O 7.2652 | \n",
      "Epoch 0166: Loss I 0.0000 | Loss O 7.2969 | \n",
      "Epoch 0167: Loss I 0.0000 | Loss O 7.3282 | \n",
      "Epoch 0168: Loss I 0.0000 | Loss O 7.3601 | \n",
      "Epoch 0169: Loss I 0.0000 | Loss O 7.3954 | \n",
      "Epoch 0170: Loss I 0.0000 | Loss O 7.4364 | \n",
      "Epoch 0171: Loss I 0.0000 | Loss O 7.4758 | \n",
      "Epoch 0172: Loss I 0.0000 | Loss O 7.5185 | \n",
      "Epoch 0173: Loss I 0.0000 | Loss O 7.5682 | \n",
      "Epoch 0174: Loss I 0.0000 | Loss O 7.6147 | \n",
      "Epoch 0175: Loss I 0.0000 | Loss O 7.6623 | \n",
      "Epoch 0176: Loss I 0.0000 | Loss O 7.7227 | \n",
      "Epoch 0177: Loss I 0.0000 | Loss O 7.7747 | \n",
      "Epoch 0178: Loss I 0.0000 | Loss O 7.8273 | \n",
      "Epoch 0179: Loss I 0.0000 | Loss O 7.8765 | \n",
      "Epoch 0180: Loss I 0.0000 | Loss O 7.9302 | \n",
      "Epoch 0181: Loss I 0.0000 | Loss O 7.9832 | \n",
      "Epoch 0182: Loss I 0.0000 | Loss O 8.0372 | \n",
      "Epoch 0183: Loss I 0.0000 | Loss O 8.0818 | \n",
      "Epoch 0184: Loss I 0.0000 | Loss O 8.1321 | \n",
      "Epoch 0185: Loss I 0.0000 | Loss O 8.1793 | \n",
      "Epoch 0186: Loss I 0.0000 | Loss O 8.2214 | \n",
      "Epoch 0187: Loss I 0.0000 | Loss O 8.2622 | \n",
      "Epoch 0188: Loss I 0.0000 | Loss O 8.3021 | \n",
      "Epoch 0189: Loss I 0.0000 | Loss O 8.3321 | \n",
      "Epoch 0190: Loss I 0.0000 | Loss O 8.3583 | \n",
      "Epoch 0191: Loss I 0.0000 | Loss O 8.3788 | \n",
      "Epoch 0192: Loss I 0.0000 | Loss O 8.3963 | \n",
      "Epoch 0193: Loss I 0.0000 | Loss O 8.4098 | \n",
      "Epoch 0194: Loss I 0.0000 | Loss O 8.4246 | \n",
      "Epoch 0195: Loss I 0.0000 | Loss O 8.4380 | \n",
      "Epoch 0196: Loss I 0.0000 | Loss O 8.4669 | \n",
      "Epoch 0197: Loss I 0.0000 | Loss O 8.5158 | \n",
      "Epoch 0198: Loss I 0.0000 | Loss O 8.6186 | \n",
      "Epoch 0199: Loss I 0.0000 | Loss O 8.8091 | \n",
      "Epoch 0000: Loss 23.6394 | \n",
      "Epoch 0001: Loss 23.0899 | \n",
      "Epoch 0002: Loss 22.8687 | \n",
      "Epoch 0003: Loss 22.7220 | \n",
      "Epoch 0004: Loss 22.4840 | \n",
      "Epoch 0005: Loss 22.1224 | \n",
      "Epoch 0006: Loss 21.6749 | \n",
      "Epoch 0007: Loss 21.1766 | \n",
      "Epoch 0008: Loss 20.6380 | \n",
      "Epoch 0009: Loss 20.0565 | \n",
      "Epoch 0010: Loss 19.4322 | \n",
      "Epoch 0011: Loss 18.7682 | \n",
      "Epoch 0012: Loss 18.0685 | \n",
      "Epoch 0013: Loss 17.3363 | \n",
      "Epoch 0014: Loss 16.5762 | \n",
      "Epoch 0015: Loss 15.7952 | \n",
      "Epoch 0016: Loss 15.0014 | \n",
      "Epoch 0017: Loss 14.2032 | \n",
      "Epoch 0018: Loss 13.4072 | \n",
      "Epoch 0019: Loss 12.6184 | \n",
      "Epoch 0020: Loss 11.8408 | \n",
      "Epoch 0021: Loss 11.0785 | \n",
      "Epoch 0022: Loss 10.3358 | \n",
      "Epoch 0023: Loss 9.6162 | \n",
      "Epoch 0024: Loss 8.9226 | \n",
      "Epoch 0025: Loss 8.2567 | \n",
      "Epoch 0026: Loss 7.6204 | \n",
      "Epoch 0027: Loss 7.0152 | \n",
      "Epoch 0028: Loss 6.4427 | \n",
      "Epoch 0029: Loss 5.9037 | \n",
      "Epoch 0030: Loss 5.3981 | \n",
      "Epoch 0031: Loss 4.9254 | \n",
      "Epoch 0032: Loss 4.4849 | \n",
      "Epoch 0033: Loss 4.0761 | \n",
      "Epoch 0034: Loss 3.6987 | \n",
      "Epoch 0035: Loss 3.3519 | \n",
      "Epoch 0036: Loss 3.0346 | \n",
      "Epoch 0037: Loss 2.7454 | \n",
      "Epoch 0038: Loss 2.4824 | \n",
      "Epoch 0039: Loss 2.2438 | \n",
      "Epoch 0040: Loss 2.0277 | \n",
      "Epoch 0041: Loss 1.8326 | \n",
      "Epoch 0042: Loss 1.6567 | \n",
      "Epoch 0043: Loss 1.4982 | \n",
      "Epoch 0044: Loss 1.3554 | \n",
      "Epoch 0045: Loss 1.2268 | \n",
      "Epoch 0046: Loss 1.1110 | \n",
      "Epoch 0047: Loss 1.0068 | \n",
      "Epoch 0048: Loss 0.9130 | \n",
      "Epoch 0049: Loss 0.8282 | \n",
      "Epoch 0050: Loss 0.7514 | \n",
      "Epoch 0051: Loss 0.6819 | \n",
      "Epoch 0052: Loss 0.6192 | \n",
      "Epoch 0053: Loss 0.5624 | \n",
      "Epoch 0054: Loss 0.5111 | \n",
      "Epoch 0055: Loss 0.4646 | \n",
      "Epoch 0056: Loss 0.4225 | \n",
      "Epoch 0057: Loss 0.3844 | \n",
      "Epoch 0058: Loss 0.3501 | \n",
      "Epoch 0059: Loss 0.3190 | \n",
      "Epoch 0060: Loss 0.2910 | \n",
      "Epoch 0061: Loss 0.2658 | \n",
      "Epoch 0062: Loss 0.2430 | \n",
      "Epoch 0063: Loss 0.2225 | \n",
      "Epoch 0064: Loss 0.2040 | \n",
      "Epoch 0065: Loss 0.1873 | \n",
      "Epoch 0066: Loss 0.1723 | \n",
      "Epoch 0067: Loss 0.1588 | \n",
      "Epoch 0068: Loss 0.1466 | \n",
      "Epoch 0069: Loss 0.1356 | \n",
      "Epoch 0070: Loss 0.1256 | \n",
      "Epoch 0071: Loss 0.1165 | \n",
      "Epoch 0072: Loss 0.1084 | \n",
      "Epoch 0073: Loss 0.1010 | \n",
      "Epoch 0074: Loss 0.0943 | \n",
      "Epoch 0075: Loss 0.0882 | \n",
      "Epoch 0076: Loss 0.0827 | \n",
      "Epoch 0077: Loss 0.0778 | \n",
      "Epoch 0078: Loss 0.0732 | \n",
      "Epoch 0079: Loss 0.0692 | \n",
      "Epoch 0080: Loss 0.0654 | \n",
      "Epoch 0081: Loss 0.0621 | \n",
      "Epoch 0082: Loss 0.0590 | \n",
      "Epoch 0083: Loss 0.0563 | \n",
      "Epoch 0084: Loss 0.0537 | \n",
      "Epoch 0085: Loss 0.0514 | \n",
      "Epoch 0086: Loss 0.0494 | \n",
      "Epoch 0087: Loss 0.0475 | \n",
      "Epoch 0088: Loss 0.0458 | \n",
      "Epoch 0089: Loss 0.0442 | \n",
      "Epoch 0090: Loss 0.0428 | \n",
      "Epoch 0091: Loss 0.0415 | \n",
      "Epoch 0092: Loss 0.0403 | \n",
      "Epoch 0093: Loss 0.0393 | \n",
      "Epoch 0094: Loss 0.0383 | \n",
      "Epoch 0095: Loss 0.0374 | \n",
      "Epoch 0096: Loss 0.0366 | \n",
      "Epoch 0097: Loss 0.0359 | \n",
      "Epoch 0098: Loss 0.0352 | \n",
      "Epoch 0099: Loss 0.0346 | \n",
      "Epoch 0100: Loss 0.0341 | \n",
      "Epoch 0101: Loss 0.0336 | \n",
      "Epoch 0102: Loss 0.0332 | \n",
      "Epoch 0103: Loss 0.0328 | \n",
      "Epoch 0104: Loss 0.0324 | \n",
      "Epoch 0105: Loss 0.0321 | \n",
      "Epoch 0106: Loss 0.0318 | \n",
      "Epoch 0107: Loss 0.0315 | \n",
      "Epoch 0108: Loss 0.0313 | \n",
      "Epoch 0109: Loss 0.0310 | \n",
      "Epoch 0110: Loss 0.0308 | \n",
      "Epoch 0111: Loss 0.0307 | \n",
      "Epoch 0112: Loss 0.0305 | \n",
      "Epoch 0113: Loss 0.0303 | \n",
      "Epoch 0114: Loss 0.0302 | \n",
      "Epoch 0115: Loss 0.0301 | \n",
      "Epoch 0116: Loss 0.0300 | \n",
      "Epoch 0117: Loss 0.0299 | \n",
      "Epoch 0118: Loss 0.0298 | \n",
      "Epoch 0119: Loss 0.0297 | \n",
      "Epoch 0120: Loss 0.0296 | \n",
      "Epoch 0121: Loss 0.0296 | \n",
      "Epoch 0122: Loss 0.0295 | \n",
      "Epoch 0123: Loss 0.0294 | \n",
      "Epoch 0124: Loss 0.0294 | \n",
      "Epoch 0125: Loss 0.0294 | \n",
      "Epoch 0126: Loss 0.0293 | \n",
      "Epoch 0127: Loss 0.0293 | \n",
      "Epoch 0128: Loss 0.0292 | \n",
      "Epoch 0129: Loss 0.0292 | \n",
      "Epoch 0130: Loss 0.0292 | \n",
      "Epoch 0131: Loss 0.0292 | \n",
      "Epoch 0132: Loss 0.0291 | \n",
      "Epoch 0133: Loss 0.0291 | \n",
      "Epoch 0134: Loss 0.0291 | \n",
      "Epoch 0135: Loss 0.0291 | \n",
      "Epoch 0136: Loss 0.0291 | \n",
      "Epoch 0137: Loss 0.0290 | \n",
      "Epoch 0138: Loss 0.0290 | \n",
      "Epoch 0139: Loss 0.0290 | \n",
      "Epoch 0140: Loss 0.0290 | \n",
      "Epoch 0141: Loss 0.0290 | \n",
      "Epoch 0142: Loss 0.0290 | \n",
      "Epoch 0143: Loss 0.0290 | \n",
      "Epoch 0144: Loss 0.0290 | \n",
      "Epoch 0145: Loss 0.0290 | \n",
      "Epoch 0146: Loss 0.0290 | \n",
      "Epoch 0147: Loss 0.0290 | \n",
      "Epoch 0148: Loss 0.0290 | \n",
      "Epoch 0149: Loss 0.0290 | \n",
      "Epoch 0150: Loss 0.0290 | \n",
      "Epoch 0151: Loss 0.0289 | \n",
      "Epoch 0152: Loss 0.0289 | \n",
      "Epoch 0153: Loss 0.0289 | \n",
      "Epoch 0154: Loss 0.0289 | \n",
      "Epoch 0155: Loss 0.0289 | \n",
      "Epoch 0156: Loss 0.0289 | \n",
      "Epoch 0157: Loss 0.0289 | \n",
      "Epoch 0158: Loss 0.0289 | \n",
      "Epoch 0159: Loss 0.0289 | \n",
      "Epoch 0160: Loss 0.0289 | \n",
      "Epoch 0161: Loss 0.0289 | \n",
      "Epoch 0162: Loss 0.0289 | \n",
      "Epoch 0163: Loss 0.0289 | \n",
      "Epoch 0164: Loss 0.0289 | \n",
      "Epoch 0165: Loss 0.0289 | \n",
      "Epoch 0166: Loss 0.0289 | \n",
      "Epoch 0167: Loss 0.0289 | \n",
      "Epoch 0168: Loss 0.0289 | \n",
      "Epoch 0169: Loss 0.0289 | \n",
      "Epoch 0170: Loss 0.0289 | \n",
      "Epoch 0171: Loss 0.0289 | \n",
      "Epoch 0172: Loss 0.0289 | \n",
      "Epoch 0173: Loss 0.0289 | \n",
      "Epoch 0174: Loss 0.0289 | \n",
      "Epoch 0175: Loss 0.0289 | \n",
      "Epoch 0176: Loss 0.0289 | \n",
      "Epoch 0177: Loss 0.0289 | \n",
      "Epoch 0178: Loss 0.0289 | \n",
      "Epoch 0179: Loss 0.0289 | \n",
      "Epoch 0180: Loss 0.0289 | \n",
      "Epoch 0181: Loss 0.0289 | \n",
      "Epoch 0182: Loss 0.0289 | \n",
      "Epoch 0183: Loss 0.0289 | \n",
      "Epoch 0184: Loss 0.0289 | \n",
      "Epoch 0185: Loss 0.0289 | \n",
      "Epoch 0186: Loss 0.0289 | \n",
      "Epoch 0187: Loss 0.0289 | \n",
      "Epoch 0188: Loss 0.0289 | \n",
      "Epoch 0189: Loss 0.0289 | \n",
      "Epoch 0190: Loss 0.0289 | \n",
      "Epoch 0191: Loss 0.0289 | \n",
      "Epoch 0192: Loss 0.0289 | \n",
      "Epoch 0193: Loss 0.0289 | \n",
      "Epoch 0194: Loss 0.0289 | \n",
      "Epoch 0195: Loss 0.0289 | \n",
      "Epoch 0196: Loss 0.0289 | \n",
      "Epoch 0197: Loss 0.0289 | \n",
      "Epoch 0198: Loss 0.0289 | \n",
      "Epoch 0199: Loss 0.0289 | \n",
      "Training GCN, epoch 0\n",
      "Training GCN, epoch 100\n",
      "Training GCN, epoch 200\n",
      "Training GCN, epoch 300\n",
      "Training GCN, epoch 400\n",
      "Training GCN, epoch 500\n",
      "Training GCN, epoch 600\n",
      "Training GCN, epoch 700\n",
      "Training GCN, epoch 800\n",
      "Training GCN, epoch 900\n",
      "Training GCN, epoch 1000\n",
      "Training GCN, epoch 1100\n",
      "Training GCN, epoch 1200\n",
      "Training GCN, epoch 1300\n",
      "Training GCN, epoch 1400\n",
      "Training GCN, epoch 1500\n",
      "Training GCN, epoch 1600\n",
      "Training GCN, epoch 1700\n",
      "Training GCN, epoch 1800\n",
      "Training GCN, epoch 1900\n",
      "Test: Loss I 0.0000 | Loss O 0.2400 | \n",
      "Test: Loss 0.0628 | \n",
      "Running node anomaly detection,  (105, 21225)\n",
      "Using precomputed node similarity\n",
      "noise_dim: 64, hid_dim: 128, num_layers: 2, dropout: 0.3, act: <class 'torch.nn.modules.activation.ReLU'>, backbone: None, contamination: 0.23809523809523808, lr: 5e-05, epoch: 200, gpu: 0, batch_size: 1, verbose: 1, isn: False, th: 0.93\n",
      "Create GAAN model\n",
      "Node Anomaly Detection task: expecting only one convergence/divergence graph\n",
      "GPU: 0\n",
      "Epoch 0000: Loss I 0.8257 | Loss O 0.7091 | \n",
      "Epoch 0001: Loss I 0.5461 | Loss O 0.6594 | \n",
      "Epoch 0002: Loss I 0.3538 | Loss O 0.7543 | \n",
      "Epoch 0003: Loss I 0.2213 | Loss O 0.9206 | \n",
      "Epoch 0004: Loss I 0.1383 | Loss O 1.1145 | \n",
      "Epoch 0005: Loss I 0.0895 | Loss O 1.3057 | \n",
      "Epoch 0006: Loss I 0.0613 | Loss O 1.4784 | \n",
      "Epoch 0007: Loss I 0.0436 | Loss O 1.6380 | \n",
      "Epoch 0008: Loss I 0.0325 | Loss O 1.7764 | \n",
      "Epoch 0009: Loss I 0.0251 | Loss O 1.9016 | \n",
      "Epoch 0010: Loss I 0.0198 | Loss O 2.0173 | \n",
      "Epoch 0011: Loss I 0.0161 | Loss O 2.1179 | \n",
      "Epoch 0012: Loss I 0.0132 | Loss O 2.2112 | \n",
      "Epoch 0013: Loss I 0.0111 | Loss O 2.2989 | \n",
      "Epoch 0014: Loss I 0.0094 | Loss O 2.3781 | \n",
      "Epoch 0015: Loss I 0.0080 | Loss O 2.4555 | \n",
      "Epoch 0016: Loss I 0.0069 | Loss O 2.5278 | \n",
      "Epoch 0017: Loss I 0.0061 | Loss O 2.5912 | \n",
      "Epoch 0018: Loss I 0.0054 | Loss O 2.6544 | \n",
      "Epoch 0019: Loss I 0.0047 | Loss O 2.7140 | \n",
      "Epoch 0020: Loss I 0.0042 | Loss O 2.7715 | \n",
      "Epoch 0021: Loss I 0.0038 | Loss O 2.8265 | \n",
      "Epoch 0022: Loss I 0.0034 | Loss O 2.8790 | \n",
      "Epoch 0023: Loss I 0.0031 | Loss O 2.9296 | \n",
      "Epoch 0024: Loss I 0.0028 | Loss O 2.9773 | \n",
      "Epoch 0025: Loss I 0.0025 | Loss O 3.0238 | \n",
      "Epoch 0026: Loss I 0.0023 | Loss O 3.0693 | \n",
      "Epoch 0027: Loss I 0.0021 | Loss O 3.1123 | \n",
      "Epoch 0028: Loss I 0.0019 | Loss O 3.1557 | \n",
      "Epoch 0029: Loss I 0.0018 | Loss O 3.1965 | \n",
      "Epoch 0030: Loss I 0.0016 | Loss O 3.2372 | \n",
      "Epoch 0031: Loss I 0.0015 | Loss O 3.2757 | \n",
      "Epoch 0032: Loss I 0.0014 | Loss O 3.3147 | \n",
      "Epoch 0033: Loss I 0.0013 | Loss O 3.3513 | \n",
      "Epoch 0034: Loss I 0.0012 | Loss O 3.3872 | \n",
      "Epoch 0035: Loss I 0.0011 | Loss O 3.4241 | \n",
      "Epoch 0036: Loss I 0.0011 | Loss O 3.4578 | \n",
      "Epoch 0037: Loss I 0.0010 | Loss O 3.4927 | \n",
      "Epoch 0038: Loss I 0.0009 | Loss O 3.5264 | \n",
      "Epoch 0039: Loss I 0.0009 | Loss O 3.5593 | \n",
      "Epoch 0040: Loss I 0.0008 | Loss O 3.5920 | \n",
      "Epoch 0041: Loss I 0.0008 | Loss O 3.6227 | \n",
      "Epoch 0042: Loss I 0.0007 | Loss O 3.6540 | \n",
      "Epoch 0043: Loss I 0.0007 | Loss O 3.6839 | \n",
      "Epoch 0044: Loss I 0.0006 | Loss O 3.7140 | \n",
      "Epoch 0045: Loss I 0.0006 | Loss O 3.7450 | \n",
      "Epoch 0046: Loss I 0.0006 | Loss O 3.7731 | \n",
      "Epoch 0047: Loss I 0.0005 | Loss O 3.8037 | \n",
      "Epoch 0048: Loss I 0.0005 | Loss O 3.8319 | \n",
      "Epoch 0049: Loss I 0.0005 | Loss O 3.8617 | \n",
      "Epoch 0050: Loss I 0.0004 | Loss O 3.8893 | \n",
      "Epoch 0051: Loss I 0.0004 | Loss O 3.9175 | \n",
      "Epoch 0052: Loss I 0.0004 | Loss O 3.9444 | \n",
      "Epoch 0053: Loss I 0.0004 | Loss O 3.9721 | \n",
      "Epoch 0054: Loss I 0.0004 | Loss O 3.9987 | \n",
      "Epoch 0055: Loss I 0.0003 | Loss O 4.0249 | \n",
      "Epoch 0056: Loss I 0.0003 | Loss O 4.0511 | \n",
      "Epoch 0057: Loss I 0.0003 | Loss O 4.0776 | \n",
      "Epoch 0058: Loss I 0.0003 | Loss O 4.1033 | \n",
      "Epoch 0059: Loss I 0.0003 | Loss O 4.1294 | \n",
      "Epoch 0060: Loss I 0.0003 | Loss O 4.1552 | \n",
      "Epoch 0061: Loss I 0.0002 | Loss O 4.1810 | \n",
      "Epoch 0062: Loss I 0.0002 | Loss O 4.2050 | \n",
      "Epoch 0063: Loss I 0.0002 | Loss O 4.2300 | \n",
      "Epoch 0064: Loss I 0.0002 | Loss O 4.2545 | \n",
      "Epoch 0065: Loss I 0.0002 | Loss O 4.2792 | \n",
      "Epoch 0066: Loss I 0.0002 | Loss O 4.3037 | \n",
      "Epoch 0067: Loss I 0.0002 | Loss O 4.3281 | \n",
      "Epoch 0068: Loss I 0.0002 | Loss O 4.3518 | \n",
      "Epoch 0069: Loss I 0.0002 | Loss O 4.3768 | \n",
      "Epoch 0070: Loss I 0.0002 | Loss O 4.3996 | \n",
      "Epoch 0071: Loss I 0.0002 | Loss O 4.4245 | \n",
      "Epoch 0072: Loss I 0.0001 | Loss O 4.4467 | \n",
      "Epoch 0073: Loss I 0.0001 | Loss O 4.4714 | \n",
      "Epoch 0074: Loss I 0.0001 | Loss O 4.4946 | \n",
      "Epoch 0075: Loss I 0.0001 | Loss O 4.5170 | \n",
      "Epoch 0076: Loss I 0.0001 | Loss O 4.5406 | \n",
      "Epoch 0077: Loss I 0.0001 | Loss O 4.5636 | \n",
      "Epoch 0078: Loss I 0.0001 | Loss O 4.5868 | \n",
      "Epoch 0079: Loss I 0.0001 | Loss O 4.6091 | \n",
      "Epoch 0080: Loss I 0.0001 | Loss O 4.6316 | \n",
      "Epoch 0081: Loss I 0.0001 | Loss O 4.6558 | \n",
      "Epoch 0082: Loss I 0.0001 | Loss O 4.6778 | \n",
      "Epoch 0083: Loss I 0.0001 | Loss O 4.7001 | \n",
      "Epoch 0084: Loss I 0.0001 | Loss O 4.7222 | \n",
      "Epoch 0085: Loss I 0.0001 | Loss O 4.7452 | \n",
      "Epoch 0086: Loss I 0.0001 | Loss O 4.7668 | \n",
      "Epoch 0087: Loss I 0.0001 | Loss O 4.7893 | \n",
      "Epoch 0088: Loss I 0.0001 | Loss O 4.8116 | \n",
      "Epoch 0089: Loss I 0.0001 | Loss O 4.8338 | \n",
      "Epoch 0090: Loss I 0.0001 | Loss O 4.8559 | \n",
      "Epoch 0091: Loss I 0.0001 | Loss O 4.8783 | \n",
      "Epoch 0092: Loss I 0.0001 | Loss O 4.9001 | \n",
      "Epoch 0093: Loss I 0.0001 | Loss O 4.9209 | \n",
      "Epoch 0094: Loss I 0.0001 | Loss O 4.9435 | \n",
      "Epoch 0095: Loss I 0.0001 | Loss O 4.9651 | \n",
      "Epoch 0096: Loss I 0.0000 | Loss O 4.9862 | \n",
      "Epoch 0097: Loss I 0.0000 | Loss O 5.0079 | \n",
      "Epoch 0098: Loss I 0.0000 | Loss O 5.0300 | \n",
      "Epoch 0099: Loss I 0.0000 | Loss O 5.0518 | \n",
      "Epoch 0100: Loss I 0.0000 | Loss O 5.0727 | \n",
      "Epoch 0101: Loss I 0.0000 | Loss O 5.0939 | \n",
      "Epoch 0102: Loss I 0.0000 | Loss O 5.1160 | \n",
      "Epoch 0103: Loss I 0.0000 | Loss O 5.1376 | \n",
      "Epoch 0104: Loss I 0.0000 | Loss O 5.1591 | \n",
      "Epoch 0105: Loss I 0.0000 | Loss O 5.1798 | \n",
      "Epoch 0106: Loss I 0.0000 | Loss O 5.2013 | \n",
      "Epoch 0107: Loss I 0.0000 | Loss O 5.2227 | \n",
      "Epoch 0108: Loss I 0.0000 | Loss O 5.2444 | \n",
      "Epoch 0109: Loss I 0.0000 | Loss O 5.2641 | \n",
      "Epoch 0110: Loss I 0.0000 | Loss O 5.2852 | \n",
      "Epoch 0111: Loss I 0.0000 | Loss O 5.3071 | \n",
      "Epoch 0112: Loss I 0.0000 | Loss O 5.3289 | \n",
      "Epoch 0113: Loss I 0.0000 | Loss O 5.3493 | \n",
      "Epoch 0114: Loss I 0.0000 | Loss O 5.3698 | \n",
      "Epoch 0115: Loss I 0.0000 | Loss O 5.3907 | \n",
      "Epoch 0116: Loss I 0.0000 | Loss O 5.4119 | \n",
      "Epoch 0117: Loss I 0.0000 | Loss O 5.4325 | \n",
      "Epoch 0118: Loss I 0.0000 | Loss O 5.4533 | \n",
      "Epoch 0119: Loss I 0.0000 | Loss O 5.4744 | \n",
      "Epoch 0120: Loss I 0.0000 | Loss O 5.4953 | \n",
      "Epoch 0121: Loss I 0.0000 | Loss O 5.5163 | \n",
      "Epoch 0122: Loss I 0.0000 | Loss O 5.5363 | \n",
      "Epoch 0123: Loss I 0.0000 | Loss O 5.5581 | \n",
      "Epoch 0124: Loss I 0.0000 | Loss O 5.5782 | \n",
      "Epoch 0125: Loss I 0.0000 | Loss O 5.5991 | \n",
      "Epoch 0126: Loss I 0.0000 | Loss O 5.6189 | \n",
      "Epoch 0127: Loss I 0.0000 | Loss O 5.6393 | \n",
      "Epoch 0128: Loss I 0.0000 | Loss O 5.6613 | \n",
      "Epoch 0129: Loss I 0.0000 | Loss O 5.6808 | \n",
      "Epoch 0130: Loss I 0.0000 | Loss O 5.7018 | \n",
      "Epoch 0131: Loss I 0.0000 | Loss O 5.7218 | \n",
      "Epoch 0132: Loss I 0.0000 | Loss O 5.7432 | \n",
      "Epoch 0133: Loss I 0.0000 | Loss O 5.7639 | \n",
      "Epoch 0134: Loss I 0.0000 | Loss O 5.7833 | \n",
      "Epoch 0135: Loss I 0.0000 | Loss O 5.8045 | \n",
      "Epoch 0136: Loss I 0.0000 | Loss O 5.8251 | \n",
      "Epoch 0137: Loss I 0.0000 | Loss O 5.8461 | \n",
      "Epoch 0138: Loss I 0.0000 | Loss O 5.8648 | \n",
      "Epoch 0139: Loss I 0.0000 | Loss O 5.8857 | \n",
      "Epoch 0140: Loss I 0.0000 | Loss O 5.9067 | \n",
      "Epoch 0141: Loss I 0.0000 | Loss O 5.9280 | \n",
      "Epoch 0142: Loss I 0.0000 | Loss O 5.9465 | \n",
      "Epoch 0143: Loss I 0.0000 | Loss O 5.9674 | \n",
      "Epoch 0144: Loss I 0.0000 | Loss O 5.9894 | \n",
      "Epoch 0145: Loss I 0.0000 | Loss O 6.0091 | \n",
      "Epoch 0146: Loss I 0.0000 | Loss O 6.0290 | \n",
      "Epoch 0147: Loss I 0.0000 | Loss O 6.0485 | \n",
      "Epoch 0148: Loss I 0.0000 | Loss O 6.0693 | \n",
      "Epoch 0149: Loss I 0.0000 | Loss O 6.0898 | \n",
      "Epoch 0150: Loss I 0.0000 | Loss O 6.1108 | \n",
      "Epoch 0151: Loss I 0.0000 | Loss O 6.1299 | \n",
      "Epoch 0152: Loss I 0.0000 | Loss O 6.1503 | \n",
      "Epoch 0153: Loss I 0.0000 | Loss O 6.1697 | \n",
      "Epoch 0154: Loss I 0.0000 | Loss O 6.1907 | \n",
      "Epoch 0155: Loss I 0.0000 | Loss O 6.2105 | \n",
      "Epoch 0156: Loss I 0.0000 | Loss O 6.2315 | \n",
      "Epoch 0157: Loss I 0.0000 | Loss O 6.2517 | \n",
      "Epoch 0158: Loss I 0.0000 | Loss O 6.2715 | \n",
      "Epoch 0159: Loss I 0.0000 | Loss O 6.2918 | \n",
      "Epoch 0160: Loss I 0.0000 | Loss O 6.3126 | \n",
      "Epoch 0161: Loss I 0.0000 | Loss O 6.3335 | \n",
      "Epoch 0162: Loss I 0.0000 | Loss O 6.3516 | \n",
      "Epoch 0163: Loss I 0.0000 | Loss O 6.3722 | \n",
      "Epoch 0164: Loss I 0.0000 | Loss O 6.3925 | \n",
      "Epoch 0165: Loss I 0.0000 | Loss O 6.4129 | \n",
      "Epoch 0166: Loss I 0.0000 | Loss O 6.4335 | \n",
      "Epoch 0167: Loss I 0.0000 | Loss O 6.4527 | \n",
      "Epoch 0168: Loss I 0.0000 | Loss O 6.4725 | \n",
      "Epoch 0169: Loss I 0.0000 | Loss O 6.4928 | \n",
      "Epoch 0170: Loss I 0.0000 | Loss O 6.5120 | \n",
      "Epoch 0171: Loss I 0.0000 | Loss O 6.5331 | \n",
      "Epoch 0172: Loss I 0.0000 | Loss O 6.5521 | \n",
      "Epoch 0173: Loss I 0.0000 | Loss O 6.5723 | \n",
      "Epoch 0174: Loss I 0.0000 | Loss O 6.5927 | \n",
      "Epoch 0175: Loss I 0.0000 | Loss O 6.6132 | \n",
      "Epoch 0176: Loss I 0.0000 | Loss O 6.6328 | \n",
      "Epoch 0177: Loss I 0.0000 | Loss O 6.6524 | \n",
      "Epoch 0178: Loss I 0.0000 | Loss O 6.6721 | \n",
      "Epoch 0179: Loss I 0.0000 | Loss O 6.6929 | \n",
      "Epoch 0180: Loss I 0.0000 | Loss O 6.7128 | \n",
      "Epoch 0181: Loss I 0.0000 | Loss O 6.7322 | \n",
      "Epoch 0182: Loss I 0.0000 | Loss O 6.7527 | \n",
      "Epoch 0183: Loss I 0.0000 | Loss O 6.7724 | \n",
      "Epoch 0184: Loss I 0.0000 | Loss O 6.7931 | \n",
      "Epoch 0185: Loss I 0.0000 | Loss O 6.8130 | \n",
      "Epoch 0186: Loss I 0.0000 | Loss O 6.8331 | \n",
      "Epoch 0187: Loss I 0.0000 | Loss O 6.8537 | \n",
      "Epoch 0188: Loss I 0.0000 | Loss O 6.8738 | \n",
      "Epoch 0189: Loss I 0.0000 | Loss O 6.8948 | \n",
      "Epoch 0190: Loss I 0.0000 | Loss O 6.9130 | \n",
      "Epoch 0191: Loss I 0.0000 | Loss O 6.9347 | \n",
      "Epoch 0192: Loss I 0.0000 | Loss O 6.9565 | \n",
      "Epoch 0193: Loss I 0.0000 | Loss O 6.9784 | \n",
      "Epoch 0194: Loss I 0.0000 | Loss O 6.9970 | \n",
      "Epoch 0195: Loss I 0.0000 | Loss O 7.0189 | \n",
      "Epoch 0196: Loss I 0.0000 | Loss O 7.0389 | \n",
      "Epoch 0197: Loss I 0.0000 | Loss O 7.0638 | \n",
      "Epoch 0198: Loss I 0.0000 | Loss O 7.0828 | \n",
      "Epoch 0199: Loss I 0.0000 | Loss O 7.1055 | \n",
      "Epoch 0000: Loss 1055383.5000 | \n",
      "Epoch 0001: Loss 1047880.6250 | \n",
      "Epoch 0002: Loss 1042072.1250 | \n",
      "Epoch 0003: Loss 1037537.5000 | \n",
      "Epoch 0004: Loss 1033920.6250 | \n",
      "Epoch 0005: Loss 1030957.9375 | \n",
      "Epoch 0006: Loss 1028442.0625 | \n",
      "Epoch 0007: Loss 1026192.6250 | \n",
      "Epoch 0008: Loss 1024053.0625 | \n",
      "Epoch 0009: Loss 1021893.5625 | \n",
      "Epoch 0010: Loss 1019612.6875 | \n",
      "Epoch 0011: Loss 1017133.0625 | \n",
      "Epoch 0012: Loss 1014395.8125 | \n",
      "Epoch 0013: Loss 1011354.3125 | \n",
      "Epoch 0014: Loss 1007970.8125 | \n",
      "Epoch 0015: Loss 1004214.0000 | \n",
      "Epoch 0016: Loss 1000058.3125 | \n",
      "Epoch 0017: Loss 995483.2500 | \n",
      "Epoch 0018: Loss 990472.5625 | \n",
      "Epoch 0019: Loss 985013.5000 | \n",
      "Epoch 0020: Loss 979097.5000 | \n",
      "Epoch 0021: Loss 972719.3125 | \n",
      "Epoch 0022: Loss 965877.6875 | \n",
      "Epoch 0023: Loss 958576.1250 | \n",
      "Epoch 0024: Loss 950821.2500 | \n",
      "Epoch 0025: Loss 942624.4375 | \n",
      "Epoch 0026: Loss 934000.1250 | \n",
      "Epoch 0027: Loss 924964.8125 | \n",
      "Epoch 0028: Loss 915536.3750 | \n",
      "Epoch 0029: Loss 905733.5000 | \n",
      "Epoch 0030: Loss 895576.3125 | \n",
      "Epoch 0031: Loss 885088.1250 | \n",
      "Epoch 0032: Loss 874294.8750 | \n",
      "Epoch 0033: Loss 863226.3750 | \n",
      "Epoch 0034: Loss 851915.3750 | \n",
      "Epoch 0035: Loss 840396.4375 | \n",
      "Epoch 0036: Loss 828703.9375 | \n",
      "Epoch 0037: Loss 816870.0625 | \n",
      "Epoch 0038: Loss 804923.8125 | \n",
      "Epoch 0039: Loss 792890.2500 | \n",
      "Epoch 0040: Loss 780790.8125 | \n",
      "Epoch 0041: Loss 768644.0625 | \n",
      "Epoch 0042: Loss 756465.8750 | \n",
      "Epoch 0043: Loss 744271.4375 | \n",
      "Epoch 0044: Loss 732078.0000 | \n",
      "Epoch 0045: Loss 719906.4375 | \n",
      "Epoch 0046: Loss 707779.6875 | \n",
      "Epoch 0047: Loss 695718.6875 | \n",
      "Epoch 0048: Loss 683741.1250 | \n",
      "Epoch 0049: Loss 671865.2500 | \n",
      "Epoch 0050: Loss 660111.8125 | \n",
      "Epoch 0051: Loss 648500.9375 | \n",
      "Epoch 0052: Loss 637047.9375 | \n",
      "Epoch 0053: Loss 625762.9375 | \n",
      "Epoch 0054: Loss 614651.3750 | \n",
      "Epoch 0055: Loss 603713.8750 | \n",
      "Epoch 0056: Loss 592948.6875 | \n",
      "Epoch 0057: Loss 582355.2500 | \n",
      "Epoch 0058: Loss 571933.0625 | \n",
      "Epoch 0059: Loss 561682.1875 | \n",
      "Epoch 0060: Loss 551603.9375 | \n",
      "Epoch 0061: Loss 541699.7500 | \n",
      "Epoch 0062: Loss 531969.6250 | \n",
      "Epoch 0063: Loss 522412.4375 | \n",
      "Epoch 0064: Loss 513026.2500 | \n",
      "Epoch 0065: Loss 503809.4062 | \n",
      "Epoch 0066: Loss 494761.6250 | \n",
      "Epoch 0067: Loss 485882.9688 | \n",
      "Epoch 0068: Loss 477174.3125 | \n",
      "Epoch 0069: Loss 468638.9062 | \n",
      "Epoch 0070: Loss 460279.6250 | \n",
      "Epoch 0071: Loss 452098.5312 | \n",
      "Epoch 0072: Loss 444095.7812 | \n",
      "Epoch 0073: Loss 436273.2500 | \n",
      "Epoch 0074: Loss 428633.5312 | \n",
      "Epoch 0075: Loss 421176.7500 | \n",
      "Epoch 0076: Loss 413899.6562 | \n",
      "Epoch 0077: Loss 406798.5938 | \n",
      "Epoch 0078: Loss 399870.5625 | \n",
      "Epoch 0079: Loss 393113.8125 | \n",
      "Epoch 0080: Loss 386525.9688 | \n",
      "Epoch 0081: Loss 380104.9688 | \n",
      "Epoch 0082: Loss 373847.1562 | \n",
      "Epoch 0083: Loss 367746.4062 | \n",
      "Epoch 0084: Loss 361796.4062 | \n",
      "Epoch 0085: Loss 355992.5000 | \n",
      "Epoch 0086: Loss 350331.1875 | \n",
      "Epoch 0087: Loss 344808.1562 | \n",
      "Epoch 0088: Loss 339418.9688 | \n",
      "Epoch 0089: Loss 334158.5312 | \n",
      "Epoch 0090: Loss 329021.2500 | \n",
      "Epoch 0091: Loss 324001.7500 | \n",
      "Epoch 0092: Loss 319094.6875 | \n",
      "Epoch 0093: Loss 314294.5625 | \n",
      "Epoch 0094: Loss 309596.7188 | \n",
      "Epoch 0095: Loss 304997.0625 | \n",
      "Epoch 0096: Loss 300491.9688 | \n",
      "Epoch 0097: Loss 296077.6562 | \n",
      "Epoch 0098: Loss 291752.9688 | \n",
      "Epoch 0099: Loss 287517.2188 | \n",
      "Epoch 0100: Loss 283369.5312 | \n",
      "Epoch 0101: Loss 279309.0625 | \n",
      "Epoch 0102: Loss 275334.7500 | \n",
      "Epoch 0103: Loss 271446.0938 | \n",
      "Epoch 0104: Loss 267642.3438 | \n",
      "Epoch 0105: Loss 263923.1250 | \n",
      "Epoch 0106: Loss 260288.1875 | \n",
      "Epoch 0107: Loss 256736.7656 | \n",
      "Epoch 0108: Loss 253267.3750 | \n",
      "Epoch 0109: Loss 249880.1094 | \n",
      "Epoch 0110: Loss 246574.3281 | \n",
      "Epoch 0111: Loss 243349.3750 | \n",
      "Epoch 0112: Loss 240204.5469 | \n",
      "Epoch 0113: Loss 237138.4375 | \n",
      "Epoch 0114: Loss 234150.2500 | \n",
      "Epoch 0115: Loss 231239.3125 | \n",
      "Epoch 0116: Loss 228405.5625 | \n",
      "Epoch 0117: Loss 225648.7969 | \n",
      "Epoch 0118: Loss 222968.4375 | \n",
      "Epoch 0119: Loss 220363.8125 | \n",
      "Epoch 0120: Loss 217834.0938 | \n",
      "Epoch 0121: Loss 215378.5469 | \n",
      "Epoch 0122: Loss 212996.2812 | \n",
      "Epoch 0123: Loss 210686.7812 | \n",
      "Epoch 0124: Loss 208448.7031 | \n",
      "Epoch 0125: Loss 206280.9844 | \n",
      "Epoch 0126: Loss 204183.4219 | \n",
      "Epoch 0127: Loss 202154.7031 | \n",
      "Epoch 0128: Loss 200193.7031 | \n",
      "Epoch 0129: Loss 198300.1094 | \n",
      "Epoch 0130: Loss 196472.7031 | \n",
      "Epoch 0131: Loss 194710.0938 | \n",
      "Epoch 0132: Loss 193010.5781 | \n",
      "Epoch 0133: Loss 191373.7656 | \n",
      "Epoch 0134: Loss 189798.8281 | \n",
      "Epoch 0135: Loss 188283.6875 | \n",
      "Epoch 0136: Loss 186827.7500 | \n",
      "Epoch 0137: Loss 185429.3594 | \n",
      "Epoch 0138: Loss 184087.3281 | \n",
      "Epoch 0139: Loss 182800.4375 | \n",
      "Epoch 0140: Loss 181567.1719 | \n",
      "Epoch 0141: Loss 180386.4375 | \n",
      "Epoch 0142: Loss 179256.3750 | \n",
      "Epoch 0143: Loss 178176.0000 | \n",
      "Epoch 0144: Loss 177144.0000 | \n",
      "Epoch 0145: Loss 176158.7188 | \n",
      "Epoch 0146: Loss 175218.6094 | \n",
      "Epoch 0147: Loss 174322.5156 | \n",
      "Epoch 0148: Loss 173469.0469 | \n",
      "Epoch 0149: Loss 172656.9062 | \n",
      "Epoch 0150: Loss 171884.9688 | \n",
      "Epoch 0151: Loss 171151.4219 | \n",
      "Epoch 0152: Loss 170454.7500 | \n",
      "Epoch 0153: Loss 169794.3281 | \n",
      "Epoch 0154: Loss 169168.1406 | \n",
      "Epoch 0155: Loss 168574.9531 | \n",
      "Epoch 0156: Loss 168013.7188 | \n",
      "Epoch 0157: Loss 167483.3594 | \n",
      "Epoch 0158: Loss 166982.0781 | \n",
      "Epoch 0159: Loss 166508.9531 | \n",
      "Epoch 0160: Loss 166062.9375 | \n",
      "Epoch 0161: Loss 165642.4375 | \n",
      "Epoch 0162: Loss 165246.6875 | \n",
      "Epoch 0163: Loss 164873.9375 | \n",
      "Epoch 0164: Loss 164522.9375 | \n",
      "Epoch 0165: Loss 164193.1562 | \n",
      "Epoch 0166: Loss 163883.7344 | \n",
      "Epoch 0167: Loss 163593.0000 | \n",
      "Epoch 0168: Loss 163319.8906 | \n",
      "Epoch 0169: Loss 163063.5781 | \n",
      "Epoch 0170: Loss 162823.4375 | \n",
      "Epoch 0171: Loss 162598.4844 | \n",
      "Epoch 0172: Loss 162386.9844 | \n",
      "Epoch 0173: Loss 162188.3281 | \n",
      "Epoch 0174: Loss 162002.5312 | \n",
      "Epoch 0175: Loss 161828.8281 | \n",
      "Epoch 0176: Loss 161665.7188 | \n",
      "Epoch 0177: Loss 161512.6562 | \n",
      "Epoch 0178: Loss 161369.3125 | \n",
      "Epoch 0179: Loss 161234.9531 | \n",
      "Epoch 0180: Loss 161108.7969 | \n",
      "Epoch 0181: Loss 160990.0781 | \n",
      "Epoch 0182: Loss 160878.7812 | \n",
      "Epoch 0183: Loss 160774.3594 | \n",
      "Epoch 0184: Loss 160676.1250 | \n",
      "Epoch 0185: Loss 160583.8750 | \n",
      "Epoch 0186: Loss 160497.1406 | \n",
      "Epoch 0187: Loss 160415.1562 | \n",
      "Epoch 0188: Loss 160337.7188 | \n",
      "Epoch 0189: Loss 160264.9375 | \n",
      "Epoch 0190: Loss 160196.4062 | \n",
      "Epoch 0191: Loss 160131.6094 | \n",
      "Epoch 0192: Loss 160070.3594 | \n",
      "Epoch 0193: Loss 160012.4688 | \n",
      "Epoch 0194: Loss 159957.5312 | \n",
      "Epoch 0195: Loss 159905.6094 | \n",
      "Epoch 0196: Loss 159856.5938 | \n",
      "Epoch 0197: Loss 159809.9531 | \n",
      "Epoch 0198: Loss 159765.5938 | \n",
      "Epoch 0199: Loss 159723.5000 | \n",
      "Training GCN, epoch 0\n",
      "Training GCN, epoch 100\n",
      "Training GCN, epoch 200\n",
      "Training GCN, epoch 300\n",
      "Training GCN, epoch 400\n",
      "Training GCN, epoch 500\n",
      "Training GCN, epoch 600\n",
      "Training GCN, epoch 700\n",
      "Training GCN, epoch 800\n",
      "Training GCN, epoch 900\n",
      "Training GCN, epoch 1000\n",
      "Training GCN, epoch 1100\n",
      "Training GCN, epoch 1200\n",
      "Training GCN, epoch 1300\n",
      "Training GCN, epoch 1400\n",
      "Training GCN, epoch 1500\n",
      "Training GCN, epoch 1600\n",
      "Training GCN, epoch 1700\n",
      "Training GCN, epoch 1800\n",
      "Training GCN, epoch 1900\n",
      "Test: Loss I 0.0000 | Loss O 0.2229 | \n",
      "Test: Loss 619300.5615 | \n",
      "Iteration 2 of 2\n",
      "Running node anomaly detection,  (24, 27551)\n",
      "Using precomputed node similarity\n",
      "noise_dim: 64, hid_dim: 128, num_layers: 2, dropout: 0.3, act: <class 'torch.nn.modules.activation.ReLU'>, backbone: None, contamination: 0.375, lr: 5e-05, epoch: 200, gpu: 0, batch_size: 1, verbose: 1, isn: False, th: 0.93\n",
      "Create GAAN model\n",
      "Node Anomaly Detection task: expecting only one convergence/divergence graph\n",
      "GPU: 0\n",
      "Epoch 0000: Loss I 0.6490 | Loss O 5.8106 | \n",
      "Epoch 0001: Loss I 0.7365 | Loss O 2.4394 | \n",
      "Epoch 0002: Loss I 0.7338 | Loss O 8.0328 | \n",
      "Epoch 0003: Loss I 0.7802 | Loss O 2.6813 | \n",
      "Epoch 0004: Loss I 0.6552 | Loss O 2.5676 | \n",
      "Epoch 0005: Loss I 0.6395 | Loss O 2.6961 | \n",
      "Epoch 0006: Loss I 0.5467 | Loss O 2.3230 | \n",
      "Epoch 0007: Loss I 0.5579 | Loss O 2.4275 | \n",
      "Epoch 0008: Loss I 0.5621 | Loss O 4.9254 | \n",
      "Epoch 0009: Loss I 0.5302 | Loss O 2.3120 | \n",
      "Epoch 0010: Loss I 0.4972 | Loss O 2.1932 | \n",
      "Epoch 0011: Loss I 0.5354 | Loss O 2.4222 | \n",
      "Epoch 0012: Loss I 0.5084 | Loss O 4.8176 | \n",
      "Epoch 0013: Loss I 0.4089 | Loss O 2.3793 | \n",
      "Epoch 0014: Loss I 0.3812 | Loss O 2.5102 | \n",
      "Epoch 0015: Loss I 0.5035 | Loss O 2.2179 | \n",
      "Epoch 0016: Loss I 0.4299 | Loss O 2.2048 | \n",
      "Epoch 0017: Loss I 0.3980 | Loss O 2.1805 | \n",
      "Epoch 0018: Loss I 0.3224 | Loss O 2.1666 | \n",
      "Epoch 0019: Loss I 0.3459 | Loss O 2.1126 | \n",
      "Epoch 0020: Loss I 0.3505 | Loss O 1.9875 | \n",
      "Epoch 0021: Loss I 0.2989 | Loss O 2.1625 | \n",
      "Epoch 0022: Loss I 0.3458 | Loss O 2.1015 | \n",
      "Epoch 0023: Loss I 0.2687 | Loss O 2.1013 | \n",
      "Epoch 0024: Loss I 0.2744 | Loss O 2.0545 | \n",
      "Epoch 0025: Loss I 0.2180 | Loss O 2.1132 | \n",
      "Epoch 0026: Loss I 0.1917 | Loss O 2.3254 | \n",
      "Epoch 0027: Loss I 0.1701 | Loss O 2.2885 | \n",
      "Epoch 0028: Loss I 0.1781 | Loss O 2.2426 | \n",
      "Epoch 0029: Loss I 0.1606 | Loss O 2.2387 | \n",
      "Epoch 0030: Loss I 0.1514 | Loss O 2.3294 | \n",
      "Epoch 0031: Loss I 0.1287 | Loss O 2.3854 | \n",
      "Epoch 0032: Loss I 0.1267 | Loss O 2.4207 | \n",
      "Epoch 0033: Loss I 0.0988 | Loss O 2.5161 | \n",
      "Epoch 0034: Loss I 0.0968 | Loss O 2.3288 | \n",
      "Epoch 0035: Loss I 0.0970 | Loss O 2.4186 | \n",
      "Epoch 0036: Loss I 0.0789 | Loss O 2.3776 | \n",
      "Epoch 0037: Loss I 0.0836 | Loss O 2.6406 | \n",
      "Epoch 0038: Loss I 0.0665 | Loss O 2.5089 | \n",
      "Epoch 0039: Loss I 0.0668 | Loss O 2.6559 | \n",
      "Epoch 0040: Loss I 0.0623 | Loss O 2.6838 | \n",
      "Epoch 0041: Loss I 0.0532 | Loss O 2.7467 | \n",
      "Epoch 0042: Loss I 0.0502 | Loss O 2.6658 | \n",
      "Epoch 0043: Loss I 0.0519 | Loss O 2.5716 | \n",
      "Epoch 0044: Loss I 0.0485 | Loss O 2.7959 | \n",
      "Epoch 0045: Loss I 0.0413 | Loss O 2.6904 | \n",
      "Epoch 0046: Loss I 0.0366 | Loss O 2.9611 | \n",
      "Epoch 0047: Loss I 0.0359 | Loss O 2.7806 | \n",
      "Epoch 0048: Loss I 0.0408 | Loss O 2.7859 | \n",
      "Epoch 0049: Loss I 0.0278 | Loss O 2.9586 | \n",
      "Epoch 0050: Loss I 0.0354 | Loss O 2.8721 | \n",
      "Epoch 0051: Loss I 0.0303 | Loss O 2.8257 | \n",
      "Epoch 0052: Loss I 0.0327 | Loss O 2.9868 | \n",
      "Epoch 0053: Loss I 0.0265 | Loss O 2.8362 | \n",
      "Epoch 0054: Loss I 0.0263 | Loss O 3.0220 | \n",
      "Epoch 0055: Loss I 0.0230 | Loss O 2.9823 | \n",
      "Epoch 0056: Loss I 0.0243 | Loss O 2.9209 | \n",
      "Epoch 0057: Loss I 0.0201 | Loss O 2.9845 | \n",
      "Epoch 0058: Loss I 0.0225 | Loss O 2.9408 | \n",
      "Epoch 0059: Loss I 0.0205 | Loss O 3.0136 | \n",
      "Epoch 0060: Loss I 0.0177 | Loss O 3.1146 | \n",
      "Epoch 0061: Loss I 0.0168 | Loss O 3.0332 | \n",
      "Epoch 0062: Loss I 0.0193 | Loss O 3.0534 | \n",
      "Epoch 0063: Loss I 0.0161 | Loss O 3.0113 | \n",
      "Epoch 0064: Loss I 0.0152 | Loss O 3.1751 | \n",
      "Epoch 0065: Loss I 0.0130 | Loss O 3.1155 | \n",
      "Epoch 0066: Loss I 0.0158 | Loss O 3.0823 | \n",
      "Epoch 0067: Loss I 0.0135 | Loss O 3.1920 | \n",
      "Epoch 0068: Loss I 0.0130 | Loss O 3.2432 | \n",
      "Epoch 0069: Loss I 0.0135 | Loss O 3.1248 | \n",
      "Epoch 0070: Loss I 0.0132 | Loss O 3.2359 | \n",
      "Epoch 0071: Loss I 0.0128 | Loss O 3.2429 | \n",
      "Epoch 0072: Loss I 0.0105 | Loss O 3.2192 | \n",
      "Epoch 0073: Loss I 0.0109 | Loss O 3.2981 | \n",
      "Epoch 0074: Loss I 0.0099 | Loss O 3.3211 | \n",
      "Epoch 0075: Loss I 0.0093 | Loss O 3.2935 | \n",
      "Epoch 0076: Loss I 0.0118 | Loss O 3.3713 | \n",
      "Epoch 0077: Loss I 0.0088 | Loss O 3.2839 | \n",
      "Epoch 0078: Loss I 0.0094 | Loss O 3.4487 | \n",
      "Epoch 0079: Loss I 0.0088 | Loss O 3.4878 | \n",
      "Epoch 0080: Loss I 0.0092 | Loss O 3.3312 | \n",
      "Epoch 0081: Loss I 0.0089 | Loss O 3.5153 | \n",
      "Epoch 0082: Loss I 0.0085 | Loss O 3.4683 | \n",
      "Epoch 0083: Loss I 0.0083 | Loss O 3.3057 | \n",
      "Epoch 0084: Loss I 0.0074 | Loss O 3.4083 | \n",
      "Epoch 0085: Loss I 0.0073 | Loss O 3.4708 | \n",
      "Epoch 0086: Loss I 0.0069 | Loss O 3.5427 | \n",
      "Epoch 0087: Loss I 0.0073 | Loss O 3.4229 | \n",
      "Epoch 0088: Loss I 0.0067 | Loss O 3.4538 | \n",
      "Epoch 0089: Loss I 0.0064 | Loss O 3.4935 | \n",
      "Epoch 0090: Loss I 0.0058 | Loss O 3.6794 | \n",
      "Epoch 0091: Loss I 0.0055 | Loss O 3.5943 | \n",
      "Epoch 0092: Loss I 0.0054 | Loss O 3.5410 | \n",
      "Epoch 0093: Loss I 0.0061 | Loss O 3.5166 | \n",
      "Epoch 0094: Loss I 0.0054 | Loss O 3.5877 | \n",
      "Epoch 0095: Loss I 0.0055 | Loss O 3.5737 | \n",
      "Epoch 0096: Loss I 0.0055 | Loss O 3.4901 | \n",
      "Epoch 0097: Loss I 0.0052 | Loss O 3.4470 | \n",
      "Epoch 0098: Loss I 0.0051 | Loss O 3.6270 | \n",
      "Epoch 0099: Loss I 0.0046 | Loss O 3.6343 | \n",
      "Epoch 0100: Loss I 0.0047 | Loss O 3.7168 | \n",
      "Epoch 0101: Loss I 0.0045 | Loss O 3.7270 | \n",
      "Epoch 0102: Loss I 0.0040 | Loss O 3.5816 | \n",
      "Epoch 0103: Loss I 0.0043 | Loss O 3.5566 | \n",
      "Epoch 0104: Loss I 0.0043 | Loss O 3.7111 | \n",
      "Epoch 0105: Loss I 0.0044 | Loss O 3.6724 | \n",
      "Epoch 0106: Loss I 0.0048 | Loss O 3.7416 | \n",
      "Epoch 0107: Loss I 0.0042 | Loss O 3.6845 | \n",
      "Epoch 0108: Loss I 0.0035 | Loss O 3.6215 | \n",
      "Epoch 0109: Loss I 0.0041 | Loss O 3.6026 | \n",
      "Epoch 0110: Loss I 0.0037 | Loss O 3.7684 | \n",
      "Epoch 0111: Loss I 0.0034 | Loss O 3.7777 | \n",
      "Epoch 0112: Loss I 0.0037 | Loss O 3.7670 | \n",
      "Epoch 0113: Loss I 0.0034 | Loss O 3.8509 | \n",
      "Epoch 0114: Loss I 0.0034 | Loss O 3.7371 | \n",
      "Epoch 0115: Loss I 0.0032 | Loss O 3.7822 | \n",
      "Epoch 0116: Loss I 0.0033 | Loss O 3.7369 | \n",
      "Epoch 0117: Loss I 0.0031 | Loss O 3.8189 | \n",
      "Epoch 0118: Loss I 0.0030 | Loss O 3.8445 | \n",
      "Epoch 0119: Loss I 0.0027 | Loss O 3.8566 | \n",
      "Epoch 0120: Loss I 0.0029 | Loss O 3.8615 | \n",
      "Epoch 0121: Loss I 0.0029 | Loss O 3.9574 | \n",
      "Epoch 0122: Loss I 0.0029 | Loss O 3.7819 | \n",
      "Epoch 0123: Loss I 0.0029 | Loss O 3.8206 | \n",
      "Epoch 0124: Loss I 0.0024 | Loss O 3.9205 | \n",
      "Epoch 0125: Loss I 0.0028 | Loss O 3.9108 | \n",
      "Epoch 0126: Loss I 0.0024 | Loss O 3.8817 | \n",
      "Epoch 0127: Loss I 0.0023 | Loss O 4.0466 | \n",
      "Epoch 0128: Loss I 0.0025 | Loss O 3.8109 | \n",
      "Epoch 0129: Loss I 0.0026 | Loss O 3.8983 | \n",
      "Epoch 0130: Loss I 0.0023 | Loss O 3.9518 | \n",
      "Epoch 0131: Loss I 0.0022 | Loss O 3.9565 | \n",
      "Epoch 0132: Loss I 0.0023 | Loss O 4.0153 | \n",
      "Epoch 0133: Loss I 0.0022 | Loss O 4.0429 | \n",
      "Epoch 0134: Loss I 0.0023 | Loss O 3.9622 | \n",
      "Epoch 0135: Loss I 0.0023 | Loss O 4.0114 | \n",
      "Epoch 0136: Loss I 0.0022 | Loss O 4.1426 | \n",
      "Epoch 0137: Loss I 0.0019 | Loss O 3.9454 | \n",
      "Epoch 0138: Loss I 0.0022 | Loss O 3.9585 | \n",
      "Epoch 0139: Loss I 0.0021 | Loss O 3.9213 | \n",
      "Epoch 0140: Loss I 0.0020 | Loss O 4.1589 | \n",
      "Epoch 0141: Loss I 0.0019 | Loss O 3.8833 | \n",
      "Epoch 0142: Loss I 0.0019 | Loss O 4.0268 | \n",
      "Epoch 0143: Loss I 0.0019 | Loss O 4.0448 | \n",
      "Epoch 0144: Loss I 0.0019 | Loss O 3.9843 | \n",
      "Epoch 0145: Loss I 0.0015 | Loss O 4.1143 | \n",
      "Epoch 0146: Loss I 0.0017 | Loss O 4.0455 | \n",
      "Epoch 0147: Loss I 0.0018 | Loss O 3.9235 | \n",
      "Epoch 0148: Loss I 0.0019 | Loss O 6.8231 | \n",
      "Epoch 0149: Loss I 0.0018 | Loss O 4.0499 | \n",
      "Epoch 0150: Loss I 0.0016 | Loss O 4.0379 | \n",
      "Epoch 0151: Loss I 0.0016 | Loss O 4.1252 | \n",
      "Epoch 0152: Loss I 0.0016 | Loss O 4.1057 | \n",
      "Epoch 0153: Loss I 0.0015 | Loss O 4.2199 | \n",
      "Epoch 0154: Loss I 0.0016 | Loss O 4.1663 | \n",
      "Epoch 0155: Loss I 0.0015 | Loss O 4.2303 | \n",
      "Epoch 0156: Loss I 0.0014 | Loss O 4.1018 | \n",
      "Epoch 0157: Loss I 0.0012 | Loss O 4.1861 | \n",
      "Epoch 0158: Loss I 0.0014 | Loss O 4.1650 | \n",
      "Epoch 0159: Loss I 0.0014 | Loss O 4.2989 | \n",
      "Epoch 0160: Loss I 0.0012 | Loss O 4.1035 | \n",
      "Epoch 0161: Loss I 0.0014 | Loss O 4.2636 | \n",
      "Epoch 0162: Loss I 0.0014 | Loss O 4.2555 | \n",
      "Epoch 0163: Loss I 0.0013 | Loss O 4.2751 | \n",
      "Epoch 0164: Loss I 0.0011 | Loss O 4.2618 | \n",
      "Epoch 0165: Loss I 0.0012 | Loss O 4.2397 | \n",
      "Epoch 0166: Loss I 0.0012 | Loss O 4.1749 | \n",
      "Epoch 0167: Loss I 0.0012 | Loss O 4.2103 | \n",
      "Epoch 0168: Loss I 0.0011 | Loss O 4.0590 | \n",
      "Epoch 0169: Loss I 0.0013 | Loss O 4.2972 | \n",
      "Epoch 0170: Loss I 0.0011 | Loss O 4.1959 | \n",
      "Epoch 0171: Loss I 0.0013 | Loss O 4.2480 | \n",
      "Epoch 0172: Loss I 0.0011 | Loss O 4.3281 | \n",
      "Epoch 0173: Loss I 0.0011 | Loss O 4.2847 | \n",
      "Epoch 0174: Loss I 0.0012 | Loss O 4.2706 | \n",
      "Epoch 0175: Loss I 0.0011 | Loss O 4.0863 | \n",
      "Epoch 0176: Loss I 0.0010 | Loss O 4.2791 | \n",
      "Epoch 0177: Loss I 0.0009 | Loss O 4.2738 | \n",
      "Epoch 0178: Loss I 0.0012 | Loss O 4.4002 | \n",
      "Epoch 0179: Loss I 0.0010 | Loss O 4.3258 | \n",
      "Epoch 0180: Loss I 0.0010 | Loss O 4.2684 | \n",
      "Epoch 0181: Loss I 0.0009 | Loss O 4.3367 | \n",
      "Epoch 0182: Loss I 0.0010 | Loss O 4.2442 | \n",
      "Epoch 0183: Loss I 0.0009 | Loss O 4.3794 | \n",
      "Epoch 0184: Loss I 0.0010 | Loss O 4.3227 | \n",
      "Epoch 0185: Loss I 0.0008 | Loss O 4.4012 | \n",
      "Epoch 0186: Loss I 0.0009 | Loss O 4.4136 | \n",
      "Epoch 0187: Loss I 0.0009 | Loss O 4.4112 | \n",
      "Epoch 0188: Loss I 0.0010 | Loss O 4.4887 | \n",
      "Epoch 0189: Loss I 0.0009 | Loss O 4.4424 | \n",
      "Epoch 0190: Loss I 0.0010 | Loss O 4.4231 | \n",
      "Epoch 0191: Loss I 0.0009 | Loss O 4.5525 | \n",
      "Epoch 0192: Loss I 0.0008 | Loss O 4.3297 | \n",
      "Epoch 0193: Loss I 0.0008 | Loss O 4.3337 | \n",
      "Epoch 0194: Loss I 0.0008 | Loss O 7.0551 | \n",
      "Epoch 0195: Loss I 0.0008 | Loss O 4.4113 | \n",
      "Epoch 0196: Loss I 0.0008 | Loss O 4.4539 | \n",
      "Epoch 0197: Loss I 0.0008 | Loss O 4.5746 | \n",
      "Epoch 0198: Loss I 0.0008 | Loss O 4.4520 | \n",
      "Epoch 0199: Loss I 0.0008 | Loss O 4.6061 | \n",
      "Epoch 0000: Loss 0.1525 | \n",
      "Epoch 0001: Loss 0.1498 | \n",
      "Epoch 0002: Loss 0.1486 | \n",
      "Epoch 0003: Loss 0.1479 | \n",
      "Epoch 0004: Loss 0.1469 | \n",
      "Epoch 0005: Loss 0.1456 | \n",
      "Epoch 0006: Loss 0.1438 | \n",
      "Epoch 0007: Loss 0.1417 | \n",
      "Epoch 0008: Loss 0.1391 | \n",
      "Epoch 0009: Loss 0.1362 | \n",
      "Epoch 0010: Loss 0.1328 | \n",
      "Epoch 0011: Loss 0.1290 | \n",
      "Epoch 0012: Loss 0.1249 | \n",
      "Epoch 0013: Loss 0.1205 | \n",
      "Epoch 0014: Loss 0.1158 | \n",
      "Epoch 0015: Loss 0.1107 | \n",
      "Epoch 0016: Loss 0.1055 | \n",
      "Epoch 0017: Loss 0.1001 | \n",
      "Epoch 0018: Loss 0.0945 | \n",
      "Epoch 0019: Loss 0.0889 | \n",
      "Epoch 0020: Loss 0.0834 | \n",
      "Epoch 0021: Loss 0.0780 | \n",
      "Epoch 0022: Loss 0.0727 | \n",
      "Epoch 0023: Loss 0.0676 | \n",
      "Epoch 0024: Loss 0.0627 | \n",
      "Epoch 0025: Loss 0.0582 | \n",
      "Epoch 0026: Loss 0.0540 | \n",
      "Epoch 0027: Loss 0.0500 | \n",
      "Epoch 0028: Loss 0.0464 | \n",
      "Epoch 0029: Loss 0.0431 | \n",
      "Epoch 0030: Loss 0.0401 | \n",
      "Epoch 0031: Loss 0.0373 | \n",
      "Epoch 0032: Loss 0.0348 | \n",
      "Epoch 0033: Loss 0.0326 | \n",
      "Epoch 0034: Loss 0.0306 | \n",
      "Epoch 0035: Loss 0.0288 | \n",
      "Epoch 0036: Loss 0.0272 | \n",
      "Epoch 0037: Loss 0.0257 | \n",
      "Epoch 0038: Loss 0.0244 | \n",
      "Epoch 0039: Loss 0.0233 | \n",
      "Epoch 0040: Loss 0.0223 | \n",
      "Epoch 0041: Loss 0.0214 | \n",
      "Epoch 0042: Loss 0.0206 | \n",
      "Epoch 0043: Loss 0.0199 | \n",
      "Epoch 0044: Loss 0.0192 | \n",
      "Epoch 0045: Loss 0.0187 | \n",
      "Epoch 0046: Loss 0.0182 | \n",
      "Epoch 0047: Loss 0.0177 | \n",
      "Epoch 0048: Loss 0.0173 | \n",
      "Epoch 0049: Loss 0.0170 | \n",
      "Epoch 0050: Loss 0.0166 | \n",
      "Epoch 0051: Loss 0.0163 | \n",
      "Epoch 0052: Loss 0.0161 | \n",
      "Epoch 0053: Loss 0.0158 | \n",
      "Epoch 0054: Loss 0.0156 | \n",
      "Epoch 0055: Loss 0.0154 | \n",
      "Epoch 0056: Loss 0.0152 | \n",
      "Epoch 0057: Loss 0.0150 | \n",
      "Epoch 0058: Loss 0.0149 | \n",
      "Epoch 0059: Loss 0.0148 | \n",
      "Epoch 0060: Loss 0.0146 | \n",
      "Epoch 0061: Loss 0.0145 | \n",
      "Epoch 0062: Loss 0.0144 | \n",
      "Epoch 0063: Loss 0.0143 | \n",
      "Epoch 0064: Loss 0.0142 | \n",
      "Epoch 0065: Loss 0.0142 | \n",
      "Epoch 0066: Loss 0.0141 | \n",
      "Epoch 0067: Loss 0.0140 | \n",
      "Epoch 0068: Loss 0.0140 | \n",
      "Epoch 0069: Loss 0.0139 | \n",
      "Epoch 0070: Loss 0.0139 | \n",
      "Epoch 0071: Loss 0.0138 | \n",
      "Epoch 0072: Loss 0.0138 | \n",
      "Epoch 0073: Loss 0.0138 | \n",
      "Epoch 0074: Loss 0.0137 | \n",
      "Epoch 0075: Loss 0.0137 | \n",
      "Epoch 0076: Loss 0.0137 | \n",
      "Epoch 0077: Loss 0.0136 | \n",
      "Epoch 0078: Loss 0.0136 | \n",
      "Epoch 0079: Loss 0.0136 | \n",
      "Epoch 0080: Loss 0.0136 | \n",
      "Epoch 0081: Loss 0.0136 | \n",
      "Epoch 0082: Loss 0.0135 | \n",
      "Epoch 0083: Loss 0.0135 | \n",
      "Epoch 0084: Loss 0.0135 | \n",
      "Epoch 0085: Loss 0.0135 | \n",
      "Epoch 0086: Loss 0.0135 | \n",
      "Epoch 0087: Loss 0.0134 | \n",
      "Epoch 0088: Loss 0.0134 | \n",
      "Epoch 0089: Loss 0.0134 | \n",
      "Epoch 0090: Loss 0.0134 | \n",
      "Epoch 0091: Loss 0.0134 | \n",
      "Epoch 0092: Loss 0.0134 | \n",
      "Epoch 0093: Loss 0.0133 | \n",
      "Epoch 0094: Loss 0.0133 | \n",
      "Epoch 0095: Loss 0.0133 | \n",
      "Epoch 0096: Loss 0.0133 | \n",
      "Epoch 0097: Loss 0.0133 | \n",
      "Epoch 0098: Loss 0.0133 | \n",
      "Epoch 0099: Loss 0.0133 | \n",
      "Epoch 0100: Loss 0.0132 | \n",
      "Epoch 0101: Loss 0.0132 | \n",
      "Epoch 0102: Loss 0.0132 | \n",
      "Epoch 0103: Loss 0.0132 | \n",
      "Epoch 0104: Loss 0.0132 | \n",
      "Epoch 0105: Loss 0.0132 | \n",
      "Epoch 0106: Loss 0.0132 | \n",
      "Epoch 0107: Loss 0.0131 | \n",
      "Epoch 0108: Loss 0.0131 | \n",
      "Epoch 0109: Loss 0.0131 | \n",
      "Epoch 0110: Loss 0.0131 | \n",
      "Epoch 0111: Loss 0.0131 | \n",
      "Epoch 0112: Loss 0.0131 | \n",
      "Epoch 0113: Loss 0.0131 | \n",
      "Epoch 0114: Loss 0.0130 | \n",
      "Epoch 0115: Loss 0.0130 | \n",
      "Epoch 0116: Loss 0.0130 | \n",
      "Epoch 0117: Loss 0.0130 | \n",
      "Epoch 0118: Loss 0.0130 | \n",
      "Epoch 0119: Loss 0.0130 | \n",
      "Epoch 0120: Loss 0.0129 | \n",
      "Epoch 0121: Loss 0.0129 | \n",
      "Epoch 0122: Loss 0.0129 | \n",
      "Epoch 0123: Loss 0.0129 | \n",
      "Epoch 0124: Loss 0.0129 | \n",
      "Epoch 0125: Loss 0.0128 | \n",
      "Epoch 0126: Loss 0.0128 | \n",
      "Epoch 0127: Loss 0.0128 | \n",
      "Epoch 0128: Loss 0.0128 | \n",
      "Epoch 0129: Loss 0.0128 | \n",
      "Epoch 0130: Loss 0.0127 | \n",
      "Epoch 0131: Loss 0.0127 | \n",
      "Epoch 0132: Loss 0.0127 | \n",
      "Epoch 0133: Loss 0.0127 | \n",
      "Epoch 0134: Loss 0.0127 | \n",
      "Epoch 0135: Loss 0.0126 | \n",
      "Epoch 0136: Loss 0.0126 | \n",
      "Epoch 0137: Loss 0.0126 | \n",
      "Epoch 0138: Loss 0.0126 | \n",
      "Epoch 0139: Loss 0.0125 | \n",
      "Epoch 0140: Loss 0.0125 | \n",
      "Epoch 0141: Loss 0.0125 | \n",
      "Epoch 0142: Loss 0.0125 | \n",
      "Epoch 0143: Loss 0.0124 | \n",
      "Epoch 0144: Loss 0.0124 | \n",
      "Epoch 0145: Loss 0.0124 | \n",
      "Epoch 0146: Loss 0.0124 | \n",
      "Epoch 0147: Loss 0.0123 | \n",
      "Epoch 0148: Loss 0.0123 | \n",
      "Epoch 0149: Loss 0.0123 | \n",
      "Epoch 0150: Loss 0.0123 | \n",
      "Epoch 0151: Loss 0.0122 | \n",
      "Epoch 0152: Loss 0.0122 | \n",
      "Epoch 0153: Loss 0.0122 | \n",
      "Epoch 0154: Loss 0.0122 | \n",
      "Epoch 0155: Loss 0.0121 | \n",
      "Epoch 0156: Loss 0.0121 | \n",
      "Epoch 0157: Loss 0.0121 | \n",
      "Epoch 0158: Loss 0.0120 | \n",
      "Epoch 0159: Loss 0.0120 | \n",
      "Epoch 0160: Loss 0.0120 | \n",
      "Epoch 0161: Loss 0.0120 | \n",
      "Epoch 0162: Loss 0.0119 | \n",
      "Epoch 0163: Loss 0.0119 | \n",
      "Epoch 0164: Loss 0.0119 | \n",
      "Epoch 0165: Loss 0.0118 | \n",
      "Epoch 0166: Loss 0.0118 | \n",
      "Epoch 0167: Loss 0.0118 | \n",
      "Epoch 0168: Loss 0.0118 | \n",
      "Epoch 0169: Loss 0.0117 | \n",
      "Epoch 0170: Loss 0.0117 | \n",
      "Epoch 0171: Loss 0.0117 | \n",
      "Epoch 0172: Loss 0.0116 | \n",
      "Epoch 0173: Loss 0.0116 | \n",
      "Epoch 0174: Loss 0.0116 | \n",
      "Epoch 0175: Loss 0.0116 | \n",
      "Epoch 0176: Loss 0.0115 | \n",
      "Epoch 0177: Loss 0.0115 | \n",
      "Epoch 0178: Loss 0.0115 | \n",
      "Epoch 0179: Loss 0.0114 | \n",
      "Epoch 0180: Loss 0.0114 | \n",
      "Epoch 0181: Loss 0.0114 | \n",
      "Epoch 0182: Loss 0.0114 | \n",
      "Epoch 0183: Loss 0.0113 | \n",
      "Epoch 0184: Loss 0.0113 | \n",
      "Epoch 0185: Loss 0.0113 | \n",
      "Epoch 0186: Loss 0.0112 | \n",
      "Epoch 0187: Loss 0.0112 | \n",
      "Epoch 0188: Loss 0.0112 | \n",
      "Epoch 0189: Loss 0.0112 | \n",
      "Epoch 0190: Loss 0.0111 | \n",
      "Epoch 0191: Loss 0.0111 | \n",
      "Epoch 0192: Loss 0.0111 | \n",
      "Epoch 0193: Loss 0.0111 | \n",
      "Epoch 0194: Loss 0.0110 | \n",
      "Epoch 0195: Loss 0.0110 | \n",
      "Epoch 0196: Loss 0.0110 | \n",
      "Epoch 0197: Loss 0.0110 | \n",
      "Epoch 0198: Loss 0.0109 | \n",
      "Epoch 0199: Loss 0.0109 | \n",
      "Training GCN, epoch 0\n",
      "Training GCN, epoch 100\n",
      "Training GCN, epoch 200\n",
      "Training GCN, epoch 300\n",
      "Training GCN, epoch 400\n",
      "Training GCN, epoch 500\n",
      "Training GCN, epoch 600\n",
      "Training GCN, epoch 700\n",
      "Training GCN, epoch 800\n",
      "Training GCN, epoch 900\n",
      "Training GCN, epoch 1000\n",
      "Training GCN, epoch 1100\n",
      "Training GCN, epoch 1200\n",
      "Training GCN, epoch 1300\n",
      "Training GCN, epoch 1400\n",
      "Training GCN, epoch 1500\n",
      "Training GCN, epoch 1600\n",
      "Training GCN, epoch 1700\n",
      "Training GCN, epoch 1800\n",
      "Training GCN, epoch 1900\n",
      "Test: Loss I 0.0024 | Loss O 0.4950 | \n",
      "Test: Loss 0.0346 | \n",
      "Running node anomaly detection,  (132, 18981)\n",
      "Using precomputed node similarity\n",
      "noise_dim: 64, hid_dim: 128, num_layers: 2, dropout: 0.3, act: <class 'torch.nn.modules.activation.ReLU'>, backbone: None, contamination: 0.4166666666666667, lr: 5e-05, epoch: 200, gpu: 0, batch_size: 1, verbose: 1, isn: False, th: 0.93\n",
      "Create GAAN model\n",
      "Node Anomaly Detection task: expecting only one convergence/divergence graph\n",
      "GPU: 0\n",
      "Epoch 0000: Loss I 0.7836 | Loss O 0.6641 | \n",
      "Epoch 0001: Loss I 0.4726 | Loss O 0.6552 | \n",
      "Epoch 0002: Loss I 0.2694 | Loss O 0.8244 | \n",
      "Epoch 0003: Loss I 0.1510 | Loss O 1.0596 | \n",
      "Epoch 0004: Loss I 0.0885 | Loss O 1.2957 | \n",
      "Epoch 0005: Loss I 0.0558 | Loss O 1.5086 | \n",
      "Epoch 0006: Loss I 0.0377 | Loss O 1.6942 | \n",
      "Epoch 0007: Loss I 0.0268 | Loss O 1.8570 | \n",
      "Epoch 0008: Loss I 0.0200 | Loss O 1.9993 | \n",
      "Epoch 0009: Loss I 0.0154 | Loss O 2.1270 | \n",
      "Epoch 0010: Loss I 0.0122 | Loss O 2.2406 | \n",
      "Epoch 0011: Loss I 0.0099 | Loss O 2.3444 | \n",
      "Epoch 0012: Loss I 0.0081 | Loss O 2.4398 | \n",
      "Epoch 0013: Loss I 0.0068 | Loss O 2.5286 | \n",
      "Epoch 0014: Loss I 0.0058 | Loss O 2.6087 | \n",
      "Epoch 0015: Loss I 0.0049 | Loss O 2.6856 | \n",
      "Epoch 0016: Loss I 0.0043 | Loss O 2.7573 | \n",
      "Epoch 0017: Loss I 0.0037 | Loss O 2.8260 | \n",
      "Epoch 0018: Loss I 0.0033 | Loss O 2.8896 | \n",
      "Epoch 0019: Loss I 0.0029 | Loss O 2.9521 | \n",
      "Epoch 0020: Loss I 0.0026 | Loss O 3.0108 | \n",
      "Epoch 0021: Loss I 0.0023 | Loss O 3.0679 | \n",
      "Epoch 0022: Loss I 0.0021 | Loss O 3.1209 | \n",
      "Epoch 0023: Loss I 0.0018 | Loss O 3.1736 | \n",
      "Epoch 0024: Loss I 0.0017 | Loss O 3.2239 | \n",
      "Epoch 0025: Loss I 0.0015 | Loss O 3.2731 | \n",
      "Epoch 0026: Loss I 0.0014 | Loss O 3.3206 | \n",
      "Epoch 0027: Loss I 0.0012 | Loss O 3.3677 | \n",
      "Epoch 0028: Loss I 0.0011 | Loss O 3.4117 | \n",
      "Epoch 0029: Loss I 0.0010 | Loss O 3.4553 | \n",
      "Epoch 0030: Loss I 0.0010 | Loss O 3.4984 | \n",
      "Epoch 0031: Loss I 0.0009 | Loss O 3.5396 | \n",
      "Epoch 0032: Loss I 0.0008 | Loss O 3.5798 | \n",
      "Epoch 0033: Loss I 0.0008 | Loss O 3.6201 | \n",
      "Epoch 0034: Loss I 0.0007 | Loss O 3.6591 | \n",
      "Epoch 0035: Loss I 0.0006 | Loss O 3.6974 | \n",
      "Epoch 0036: Loss I 0.0006 | Loss O 3.7363 | \n",
      "Epoch 0037: Loss I 0.0006 | Loss O 3.7722 | \n",
      "Epoch 0038: Loss I 0.0005 | Loss O 3.8084 | \n",
      "Epoch 0039: Loss I 0.0005 | Loss O 3.8446 | \n",
      "Epoch 0040: Loss I 0.0004 | Loss O 3.8800 | \n",
      "Epoch 0041: Loss I 0.0004 | Loss O 3.9144 | \n",
      "Epoch 0042: Loss I 0.0004 | Loss O 3.9505 | \n",
      "Epoch 0043: Loss I 0.0004 | Loss O 3.9829 | \n",
      "Epoch 0044: Loss I 0.0003 | Loss O 4.0163 | \n",
      "Epoch 0045: Loss I 0.0003 | Loss O 4.0503 | \n",
      "Epoch 0046: Loss I 0.0003 | Loss O 4.0830 | \n",
      "Epoch 0047: Loss I 0.0003 | Loss O 4.1156 | \n",
      "Epoch 0048: Loss I 0.0003 | Loss O 4.1478 | \n",
      "Epoch 0049: Loss I 0.0002 | Loss O 4.1795 | \n",
      "Epoch 0050: Loss I 0.0002 | Loss O 4.2114 | \n",
      "Epoch 0051: Loss I 0.0002 | Loss O 4.2422 | \n",
      "Epoch 0052: Loss I 0.0002 | Loss O 4.2732 | \n",
      "Epoch 0053: Loss I 0.0002 | Loss O 4.3040 | \n",
      "Epoch 0054: Loss I 0.0002 | Loss O 4.3348 | \n",
      "Epoch 0055: Loss I 0.0002 | Loss O 4.3655 | \n",
      "Epoch 0056: Loss I 0.0002 | Loss O 4.3955 | \n",
      "Epoch 0057: Loss I 0.0001 | Loss O 4.4244 | \n",
      "Epoch 0058: Loss I 0.0001 | Loss O 4.4547 | \n",
      "Epoch 0059: Loss I 0.0001 | Loss O 4.4848 | \n",
      "Epoch 0060: Loss I 0.0001 | Loss O 4.5144 | \n",
      "Epoch 0061: Loss I 0.0001 | Loss O 4.5433 | \n",
      "Epoch 0062: Loss I 0.0001 | Loss O 4.5721 | \n",
      "Epoch 0063: Loss I 0.0001 | Loss O 4.6010 | \n",
      "Epoch 0064: Loss I 0.0001 | Loss O 4.6296 | \n",
      "Epoch 0065: Loss I 0.0001 | Loss O 4.6584 | \n",
      "Epoch 0066: Loss I 0.0001 | Loss O 4.6869 | \n",
      "Epoch 0067: Loss I 0.0001 | Loss O 4.7150 | \n",
      "Epoch 0068: Loss I 0.0001 | Loss O 4.7435 | \n",
      "Epoch 0069: Loss I 0.0001 | Loss O 4.7719 | \n",
      "Epoch 0070: Loss I 0.0001 | Loss O 4.7996 | \n",
      "Epoch 0071: Loss I 0.0001 | Loss O 4.8271 | \n",
      "Epoch 0072: Loss I 0.0001 | Loss O 4.8552 | \n",
      "Epoch 0073: Loss I 0.0001 | Loss O 4.8832 | \n",
      "Epoch 0074: Loss I 0.0001 | Loss O 4.9110 | \n",
      "Epoch 0075: Loss I 0.0001 | Loss O 4.9383 | \n",
      "Epoch 0076: Loss I 0.0001 | Loss O 4.9653 | \n",
      "Epoch 0077: Loss I 0.0000 | Loss O 4.9928 | \n",
      "Epoch 0078: Loss I 0.0000 | Loss O 5.0208 | \n",
      "Epoch 0079: Loss I 0.0000 | Loss O 5.0479 | \n",
      "Epoch 0080: Loss I 0.0000 | Loss O 5.0744 | \n",
      "Epoch 0081: Loss I 0.0000 | Loss O 5.1018 | \n",
      "Epoch 0082: Loss I 0.0000 | Loss O 5.1282 | \n",
      "Epoch 0083: Loss I 0.0000 | Loss O 5.1556 | \n",
      "Epoch 0084: Loss I 0.0000 | Loss O 5.1827 | \n",
      "Epoch 0085: Loss I 0.0000 | Loss O 5.2096 | \n",
      "Epoch 0086: Loss I 0.0000 | Loss O 5.2356 | \n",
      "Epoch 0087: Loss I 0.0000 | Loss O 5.2624 | \n",
      "Epoch 0088: Loss I 0.0000 | Loss O 5.2894 | \n",
      "Epoch 0089: Loss I 0.0000 | Loss O 5.3160 | \n",
      "Epoch 0090: Loss I 0.0000 | Loss O 5.3425 | \n",
      "Epoch 0091: Loss I 0.0000 | Loss O 5.3686 | \n",
      "Epoch 0092: Loss I 0.0000 | Loss O 5.3950 | \n",
      "Epoch 0093: Loss I 0.0000 | Loss O 5.4218 | \n",
      "Epoch 0094: Loss I 0.0000 | Loss O 5.4477 | \n",
      "Epoch 0095: Loss I 0.0000 | Loss O 5.4745 | \n",
      "Epoch 0096: Loss I 0.0000 | Loss O 5.5004 | \n",
      "Epoch 0097: Loss I 0.0000 | Loss O 5.5277 | \n",
      "Epoch 0098: Loss I 0.0000 | Loss O 5.5526 | \n",
      "Epoch 0099: Loss I 0.0000 | Loss O 5.5789 | \n",
      "Epoch 0100: Loss I 0.0000 | Loss O 5.6051 | \n",
      "Epoch 0101: Loss I 0.0000 | Loss O 5.6314 | \n",
      "Epoch 0102: Loss I 0.0000 | Loss O 5.6567 | \n",
      "Epoch 0103: Loss I 0.0000 | Loss O 5.6831 | \n",
      "Epoch 0104: Loss I 0.0000 | Loss O 5.7087 | \n",
      "Epoch 0105: Loss I 0.0000 | Loss O 5.7353 | \n",
      "Epoch 0106: Loss I 0.0000 | Loss O 5.7611 | \n",
      "Epoch 0107: Loss I 0.0000 | Loss O 5.7860 | \n",
      "Epoch 0108: Loss I 0.0000 | Loss O 5.8134 | \n",
      "Epoch 0109: Loss I 0.0000 | Loss O 5.8390 | \n",
      "Epoch 0110: Loss I 0.0000 | Loss O 5.8643 | \n",
      "Epoch 0111: Loss I 0.0000 | Loss O 5.8908 | \n",
      "Epoch 0112: Loss I 0.0000 | Loss O 5.9161 | \n",
      "Epoch 0113: Loss I 0.0000 | Loss O 5.9415 | \n",
      "Epoch 0114: Loss I 0.0000 | Loss O 5.9682 | \n",
      "Epoch 0115: Loss I 0.0000 | Loss O 5.9932 | \n",
      "Epoch 0116: Loss I 0.0000 | Loss O 6.0193 | \n",
      "Epoch 0117: Loss I 0.0000 | Loss O 6.0452 | \n",
      "Epoch 0118: Loss I 0.0000 | Loss O 6.0703 | \n",
      "Epoch 0119: Loss I 0.0000 | Loss O 6.0955 | \n",
      "Epoch 0120: Loss I 0.0000 | Loss O 6.1213 | \n",
      "Epoch 0121: Loss I 0.0000 | Loss O 6.1463 | \n",
      "Epoch 0122: Loss I 0.0000 | Loss O 6.1722 | \n",
      "Epoch 0123: Loss I 0.0000 | Loss O 6.1978 | \n",
      "Epoch 0124: Loss I 0.0000 | Loss O 6.2244 | \n",
      "Epoch 0125: Loss I 0.0000 | Loss O 6.2495 | \n",
      "Epoch 0126: Loss I 0.0000 | Loss O 6.2745 | \n",
      "Epoch 0127: Loss I 0.0000 | Loss O 6.3004 | \n",
      "Epoch 0128: Loss I 0.0000 | Loss O 6.3261 | \n",
      "Epoch 0129: Loss I 0.0000 | Loss O 6.3511 | \n",
      "Epoch 0130: Loss I 0.0000 | Loss O 6.3761 | \n",
      "Epoch 0131: Loss I 0.0000 | Loss O 6.4021 | \n",
      "Epoch 0132: Loss I 0.0000 | Loss O 6.4276 | \n",
      "Epoch 0133: Loss I 0.0000 | Loss O 6.4520 | \n",
      "Epoch 0134: Loss I 0.0000 | Loss O 6.4772 | \n",
      "Epoch 0135: Loss I 0.0000 | Loss O 6.5030 | \n",
      "Epoch 0136: Loss I 0.0000 | Loss O 6.5288 | \n",
      "Epoch 0137: Loss I 0.0000 | Loss O 6.5529 | \n",
      "Epoch 0138: Loss I 0.0000 | Loss O 6.5787 | \n",
      "Epoch 0139: Loss I 0.0000 | Loss O 6.6044 | \n",
      "Epoch 0140: Loss I 0.0000 | Loss O 6.6293 | \n",
      "Epoch 0141: Loss I 0.0000 | Loss O 6.6547 | \n",
      "Epoch 0142: Loss I 0.0000 | Loss O 6.6795 | \n",
      "Epoch 0143: Loss I 0.0000 | Loss O 6.7050 | \n",
      "Epoch 0144: Loss I 0.0000 | Loss O 6.7310 | \n",
      "Epoch 0145: Loss I 0.0000 | Loss O 6.7557 | \n",
      "Epoch 0146: Loss I 0.0000 | Loss O 6.7803 | \n",
      "Epoch 0147: Loss I 0.0000 | Loss O 6.8057 | \n",
      "Epoch 0148: Loss I 0.0000 | Loss O 6.8305 | \n",
      "Epoch 0149: Loss I 0.0000 | Loss O 6.8565 | \n",
      "Epoch 0150: Loss I 0.0000 | Loss O 6.8818 | \n",
      "Epoch 0151: Loss I 0.0000 | Loss O 6.9065 | \n",
      "Epoch 0152: Loss I 0.0000 | Loss O 6.9316 | \n",
      "Epoch 0153: Loss I 0.0000 | Loss O 6.9568 | \n",
      "Epoch 0154: Loss I 0.0000 | Loss O 6.9824 | \n",
      "Epoch 0155: Loss I 0.0000 | Loss O 7.0074 | \n",
      "Epoch 0156: Loss I 0.0000 | Loss O 7.0325 | \n",
      "Epoch 0157: Loss I 0.0000 | Loss O 7.0583 | \n",
      "Epoch 0158: Loss I 0.0000 | Loss O 7.0838 | \n",
      "Epoch 0159: Loss I 0.0000 | Loss O 7.1101 | \n",
      "Epoch 0160: Loss I 0.0000 | Loss O 7.1358 | \n",
      "Epoch 0161: Loss I 0.0000 | Loss O 7.1637 | \n",
      "Epoch 0162: Loss I 0.0000 | Loss O 7.1893 | \n",
      "Epoch 0163: Loss I 0.0000 | Loss O 7.2163 | \n",
      "Epoch 0164: Loss I 0.0000 | Loss O 7.2460 | \n",
      "Epoch 0165: Loss I 0.0000 | Loss O 7.2757 | \n",
      "Epoch 0166: Loss I 0.0000 | Loss O 7.3071 | \n",
      "Epoch 0167: Loss I 0.0000 | Loss O 7.3404 | \n",
      "Epoch 0168: Loss I 0.0000 | Loss O 7.3744 | \n",
      "Epoch 0169: Loss I 0.0000 | Loss O 7.4091 | \n",
      "Epoch 0170: Loss I 0.0000 | Loss O 7.4515 | \n",
      "Epoch 0171: Loss I 0.0000 | Loss O 7.4933 | \n",
      "Epoch 0172: Loss I 0.0000 | Loss O 7.5387 | \n",
      "Epoch 0173: Loss I 0.0000 | Loss O 7.5900 | \n",
      "Epoch 0174: Loss I 0.0000 | Loss O 7.6385 | \n",
      "Epoch 0175: Loss I 0.0000 | Loss O 7.6881 | \n",
      "Epoch 0176: Loss I 0.0000 | Loss O 7.7456 | \n",
      "Epoch 0177: Loss I 0.0000 | Loss O 7.8010 | \n",
      "Epoch 0178: Loss I 0.0000 | Loss O 7.8509 | \n",
      "Epoch 0179: Loss I 0.0000 | Loss O 7.9026 | \n",
      "Epoch 0180: Loss I 0.0000 | Loss O 7.9535 | \n",
      "Epoch 0181: Loss I 0.0000 | Loss O 8.0040 | \n",
      "Epoch 0182: Loss I 0.0000 | Loss O 8.0562 | \n",
      "Epoch 0183: Loss I 0.0000 | Loss O 8.1011 | \n",
      "Epoch 0184: Loss I 0.0000 | Loss O 8.1485 | \n",
      "Epoch 0185: Loss I 0.0000 | Loss O 8.1933 | \n",
      "Epoch 0186: Loss I 0.0000 | Loss O 8.2346 | \n",
      "Epoch 0187: Loss I 0.0000 | Loss O 8.2724 | \n",
      "Epoch 0188: Loss I 0.0000 | Loss O 8.3102 | \n",
      "Epoch 0189: Loss I 0.0000 | Loss O 8.3375 | \n",
      "Epoch 0190: Loss I 0.0000 | Loss O 8.3630 | \n",
      "Epoch 0191: Loss I 0.0000 | Loss O 8.3834 | \n",
      "Epoch 0192: Loss I 0.0000 | Loss O 8.3992 | \n",
      "Epoch 0193: Loss I 0.0000 | Loss O 8.4127 | \n",
      "Epoch 0194: Loss I 0.0000 | Loss O 8.4285 | \n",
      "Epoch 0195: Loss I 0.0000 | Loss O 8.4459 | \n",
      "Epoch 0196: Loss I 0.0000 | Loss O 8.4863 | \n",
      "Epoch 0197: Loss I 0.0000 | Loss O 8.5492 | \n",
      "Epoch 0198: Loss I 0.0000 | Loss O 8.6735 | \n",
      "Epoch 0199: Loss I 0.0000 | Loss O 8.9102 | \n",
      "Epoch 0000: Loss 23.6106 | \n",
      "Epoch 0001: Loss 23.0696 | \n",
      "Epoch 0002: Loss 22.8663 | \n",
      "Epoch 0003: Loss 22.6969 | \n",
      "Epoch 0004: Loss 22.4326 | \n",
      "Epoch 0005: Loss 22.0548 | \n",
      "Epoch 0006: Loss 21.5927 | \n",
      "Epoch 0007: Loss 21.0709 | \n",
      "Epoch 0008: Loss 20.4975 | \n",
      "Epoch 0009: Loss 19.8741 | \n",
      "Epoch 0010: Loss 19.2033 | \n",
      "Epoch 0011: Loss 18.4904 | \n",
      "Epoch 0012: Loss 17.7418 | \n",
      "Epoch 0013: Loss 16.9648 | \n",
      "Epoch 0014: Loss 16.1672 | \n",
      "Epoch 0015: Loss 15.3564 | \n",
      "Epoch 0016: Loss 14.5398 | \n",
      "Epoch 0017: Loss 13.7239 | \n",
      "Epoch 0018: Loss 12.9150 | \n",
      "Epoch 0019: Loss 12.1184 | \n",
      "Epoch 0020: Loss 11.3384 | \n",
      "Epoch 0021: Loss 10.5782 | \n",
      "Epoch 0022: Loss 9.8409 | \n",
      "Epoch 0023: Loss 9.1295 | \n",
      "Epoch 0024: Loss 8.4478 | \n",
      "Epoch 0025: Loss 7.7984 | \n",
      "Epoch 0026: Loss 7.1823 | \n",
      "Epoch 0027: Loss 6.5994 | \n",
      "Epoch 0028: Loss 6.0492 | \n",
      "Epoch 0029: Loss 5.5318 | \n",
      "Epoch 0030: Loss 5.0476 | \n",
      "Epoch 0031: Loss 4.5970 | \n",
      "Epoch 0032: Loss 4.1797 | \n",
      "Epoch 0033: Loss 3.7949 | \n",
      "Epoch 0034: Loss 3.4410 | \n",
      "Epoch 0035: Loss 3.1169 | \n",
      "Epoch 0036: Loss 2.8210 | \n",
      "Epoch 0037: Loss 2.5520 | \n",
      "Epoch 0038: Loss 2.3084 | \n",
      "Epoch 0039: Loss 2.0881 | \n",
      "Epoch 0040: Loss 1.8893 | \n",
      "Epoch 0041: Loss 1.7098 | \n",
      "Epoch 0042: Loss 1.5480 | \n",
      "Epoch 0043: Loss 1.4021 | \n",
      "Epoch 0044: Loss 1.2707 | \n",
      "Epoch 0045: Loss 1.1524 | \n",
      "Epoch 0046: Loss 1.0457 | \n",
      "Epoch 0047: Loss 0.9494 | \n",
      "Epoch 0048: Loss 0.8621 | \n",
      "Epoch 0049: Loss 0.7831 | \n",
      "Epoch 0050: Loss 0.7115 | \n",
      "Epoch 0051: Loss 0.6465 | \n",
      "Epoch 0052: Loss 0.5874 | \n",
      "Epoch 0053: Loss 0.5338 | \n",
      "Epoch 0054: Loss 0.4852 | \n",
      "Epoch 0055: Loss 0.4412 | \n",
      "Epoch 0056: Loss 0.4013 | \n",
      "Epoch 0057: Loss 0.3653 | \n",
      "Epoch 0058: Loss 0.3326 | \n",
      "Epoch 0059: Loss 0.3030 | \n",
      "Epoch 0060: Loss 0.2764 | \n",
      "Epoch 0061: Loss 0.2523 | \n",
      "Epoch 0062: Loss 0.2307 | \n",
      "Epoch 0063: Loss 0.2112 | \n",
      "Epoch 0064: Loss 0.1936 | \n",
      "Epoch 0065: Loss 0.1777 | \n",
      "Epoch 0066: Loss 0.1634 | \n",
      "Epoch 0067: Loss 0.1505 | \n",
      "Epoch 0068: Loss 0.1389 | \n",
      "Epoch 0069: Loss 0.1283 | \n",
      "Epoch 0070: Loss 0.1188 | \n",
      "Epoch 0071: Loss 0.1103 | \n",
      "Epoch 0072: Loss 0.1025 | \n",
      "Epoch 0073: Loss 0.0954 | \n",
      "Epoch 0074: Loss 0.0890 | \n",
      "Epoch 0075: Loss 0.0833 | \n",
      "Epoch 0076: Loss 0.0780 | \n",
      "Epoch 0077: Loss 0.0733 | \n",
      "Epoch 0078: Loss 0.0690 | \n",
      "Epoch 0079: Loss 0.0652 | \n",
      "Epoch 0080: Loss 0.0617 | \n",
      "Epoch 0081: Loss 0.0585 | \n",
      "Epoch 0082: Loss 0.0556 | \n",
      "Epoch 0083: Loss 0.0530 | \n",
      "Epoch 0084: Loss 0.0507 | \n",
      "Epoch 0085: Loss 0.0485 | \n",
      "Epoch 0086: Loss 0.0466 | \n",
      "Epoch 0087: Loss 0.0448 | \n",
      "Epoch 0088: Loss 0.0433 | \n",
      "Epoch 0089: Loss 0.0418 | \n",
      "Epoch 0090: Loss 0.0405 | \n",
      "Epoch 0091: Loss 0.0393 | \n",
      "Epoch 0092: Loss 0.0382 | \n",
      "Epoch 0093: Loss 0.0373 | \n",
      "Epoch 0094: Loss 0.0364 | \n",
      "Epoch 0095: Loss 0.0356 | \n",
      "Epoch 0096: Loss 0.0348 | \n",
      "Epoch 0097: Loss 0.0342 | \n",
      "Epoch 0098: Loss 0.0335 | \n",
      "Epoch 0099: Loss 0.0330 | \n",
      "Epoch 0100: Loss 0.0325 | \n",
      "Epoch 0101: Loss 0.0320 | \n",
      "Epoch 0102: Loss 0.0316 | \n",
      "Epoch 0103: Loss 0.0313 | \n",
      "Epoch 0104: Loss 0.0309 | \n",
      "Epoch 0105: Loss 0.0306 | \n",
      "Epoch 0106: Loss 0.0303 | \n",
      "Epoch 0107: Loss 0.0301 | \n",
      "Epoch 0108: Loss 0.0299 | \n",
      "Epoch 0109: Loss 0.0296 | \n",
      "Epoch 0110: Loss 0.0295 | \n",
      "Epoch 0111: Loss 0.0293 | \n",
      "Epoch 0112: Loss 0.0291 | \n",
      "Epoch 0113: Loss 0.0290 | \n",
      "Epoch 0114: Loss 0.0289 | \n",
      "Epoch 0115: Loss 0.0288 | \n",
      "Epoch 0116: Loss 0.0286 | \n",
      "Epoch 0117: Loss 0.0286 | \n",
      "Epoch 0118: Loss 0.0285 | \n",
      "Epoch 0119: Loss 0.0284 | \n",
      "Epoch 0120: Loss 0.0283 | \n",
      "Epoch 0121: Loss 0.0283 | \n",
      "Epoch 0122: Loss 0.0282 | \n",
      "Epoch 0123: Loss 0.0281 | \n",
      "Epoch 0124: Loss 0.0281 | \n",
      "Epoch 0125: Loss 0.0281 | \n",
      "Epoch 0126: Loss 0.0280 | \n",
      "Epoch 0127: Loss 0.0280 | \n",
      "Epoch 0128: Loss 0.0279 | \n",
      "Epoch 0129: Loss 0.0279 | \n",
      "Epoch 0130: Loss 0.0279 | \n",
      "Epoch 0131: Loss 0.0279 | \n",
      "Epoch 0132: Loss 0.0278 | \n",
      "Epoch 0133: Loss 0.0278 | \n",
      "Epoch 0134: Loss 0.0278 | \n",
      "Epoch 0135: Loss 0.0278 | \n",
      "Epoch 0136: Loss 0.0278 | \n",
      "Epoch 0137: Loss 0.0278 | \n",
      "Epoch 0138: Loss 0.0277 | \n",
      "Epoch 0139: Loss 0.0277 | \n",
      "Epoch 0140: Loss 0.0277 | \n",
      "Epoch 0141: Loss 0.0277 | \n",
      "Epoch 0142: Loss 0.0277 | \n",
      "Epoch 0143: Loss 0.0277 | \n",
      "Epoch 0144: Loss 0.0277 | \n",
      "Epoch 0145: Loss 0.0277 | \n",
      "Epoch 0146: Loss 0.0277 | \n",
      "Epoch 0147: Loss 0.0277 | \n",
      "Epoch 0148: Loss 0.0277 | \n",
      "Epoch 0149: Loss 0.0277 | \n",
      "Epoch 0150: Loss 0.0277 | \n",
      "Epoch 0151: Loss 0.0277 | \n",
      "Epoch 0152: Loss 0.0277 | \n",
      "Epoch 0153: Loss 0.0277 | \n",
      "Epoch 0154: Loss 0.0277 | \n",
      "Epoch 0155: Loss 0.0277 | \n",
      "Epoch 0156: Loss 0.0277 | \n",
      "Epoch 0157: Loss 0.0276 | \n",
      "Epoch 0158: Loss 0.0276 | \n",
      "Epoch 0159: Loss 0.0276 | \n",
      "Epoch 0160: Loss 0.0276 | \n",
      "Epoch 0161: Loss 0.0276 | \n",
      "Epoch 0162: Loss 0.0276 | \n",
      "Epoch 0163: Loss 0.0276 | \n",
      "Epoch 0164: Loss 0.0276 | \n",
      "Epoch 0165: Loss 0.0276 | \n",
      "Epoch 0166: Loss 0.0276 | \n",
      "Epoch 0167: Loss 0.0276 | \n",
      "Epoch 0168: Loss 0.0276 | \n",
      "Epoch 0169: Loss 0.0276 | \n",
      "Epoch 0170: Loss 0.0276 | \n",
      "Epoch 0171: Loss 0.0276 | \n",
      "Epoch 0172: Loss 0.0276 | \n",
      "Epoch 0173: Loss 0.0276 | \n",
      "Epoch 0174: Loss 0.0276 | \n",
      "Epoch 0175: Loss 0.0276 | \n",
      "Epoch 0176: Loss 0.0276 | \n",
      "Epoch 0177: Loss 0.0276 | \n",
      "Epoch 0178: Loss 0.0276 | \n",
      "Epoch 0179: Loss 0.0276 | \n",
      "Epoch 0180: Loss 0.0276 | \n",
      "Epoch 0181: Loss 0.0276 | \n",
      "Epoch 0182: Loss 0.0276 | \n",
      "Epoch 0183: Loss 0.0276 | \n",
      "Epoch 0184: Loss 0.0276 | \n",
      "Epoch 0185: Loss 0.0276 | \n",
      "Epoch 0186: Loss 0.0276 | \n",
      "Epoch 0187: Loss 0.0276 | \n",
      "Epoch 0188: Loss 0.0276 | \n",
      "Epoch 0189: Loss 0.0276 | \n",
      "Epoch 0190: Loss 0.0276 | \n",
      "Epoch 0191: Loss 0.0276 | \n",
      "Epoch 0192: Loss 0.0276 | \n",
      "Epoch 0193: Loss 0.0276 | \n",
      "Epoch 0194: Loss 0.0276 | \n",
      "Epoch 0195: Loss 0.0276 | \n",
      "Epoch 0196: Loss 0.0276 | \n",
      "Epoch 0197: Loss 0.0276 | \n",
      "Epoch 0198: Loss 0.0276 | \n",
      "Epoch 0199: Loss 0.0276 | \n",
      "Training GCN, epoch 0\n",
      "Training GCN, epoch 100\n",
      "Training GCN, epoch 200\n",
      "Training GCN, epoch 300\n",
      "Training GCN, epoch 400\n",
      "Training GCN, epoch 500\n",
      "Training GCN, epoch 600\n",
      "Training GCN, epoch 700\n",
      "Training GCN, epoch 800\n",
      "Training GCN, epoch 900\n",
      "Training GCN, epoch 1000\n",
      "Training GCN, epoch 1100\n",
      "Training GCN, epoch 1200\n",
      "Training GCN, epoch 1300\n",
      "Training GCN, epoch 1400\n",
      "Training GCN, epoch 1500\n",
      "Training GCN, epoch 1600\n",
      "Training GCN, epoch 1700\n",
      "Training GCN, epoch 1800\n",
      "Training GCN, epoch 1900\n",
      "Test: Loss I 0.0000 | Loss O 0.2400 | \n",
      "Test: Loss 0.0694 | \n",
      "Running node anomaly detection,  (105, 21225)\n",
      "Using precomputed node similarity\n",
      "noise_dim: 64, hid_dim: 128, num_layers: 2, dropout: 0.3, act: <class 'torch.nn.modules.activation.ReLU'>, backbone: None, contamination: 0.23809523809523808, lr: 5e-05, epoch: 200, gpu: 0, batch_size: 1, verbose: 1, isn: False, th: 0.93\n",
      "Create GAAN model\n",
      "Node Anomaly Detection task: expecting only one convergence/divergence graph\n",
      "GPU: 0\n",
      "Epoch 0000: Loss I 0.8253 | Loss O 0.7050 | \n",
      "Epoch 0001: Loss I 0.5425 | Loss O 0.6574 | \n",
      "Epoch 0002: Loss I 0.3532 | Loss O 0.7527 | \n",
      "Epoch 0003: Loss I 0.2199 | Loss O 0.9214 | \n",
      "Epoch 0004: Loss I 0.1375 | Loss O 1.1151 | \n",
      "Epoch 0005: Loss I 0.0888 | Loss O 1.3081 | \n",
      "Epoch 0006: Loss I 0.0609 | Loss O 1.4801 | \n",
      "Epoch 0007: Loss I 0.0432 | Loss O 1.6414 | \n",
      "Epoch 0008: Loss I 0.0325 | Loss O 1.7772 | \n",
      "Epoch 0009: Loss I 0.0250 | Loss O 1.9019 | \n",
      "Epoch 0010: Loss I 0.0197 | Loss O 2.0165 | \n",
      "Epoch 0011: Loss I 0.0160 | Loss O 2.1184 | \n",
      "Epoch 0012: Loss I 0.0132 | Loss O 2.2116 | \n",
      "Epoch 0013: Loss I 0.0110 | Loss O 2.2991 | \n",
      "Epoch 0014: Loss I 0.0094 | Loss O 2.3789 | \n",
      "Epoch 0015: Loss I 0.0080 | Loss O 2.4545 | \n",
      "Epoch 0016: Loss I 0.0069 | Loss O 2.5283 | \n",
      "Epoch 0017: Loss I 0.0061 | Loss O 2.5927 | \n",
      "Epoch 0018: Loss I 0.0053 | Loss O 2.6556 | \n",
      "Epoch 0019: Loss I 0.0047 | Loss O 2.7157 | \n",
      "Epoch 0020: Loss I 0.0042 | Loss O 2.7726 | \n",
      "Epoch 0021: Loss I 0.0038 | Loss O 2.8271 | \n",
      "Epoch 0022: Loss I 0.0034 | Loss O 2.8803 | \n",
      "Epoch 0023: Loss I 0.0031 | Loss O 2.9294 | \n",
      "Epoch 0024: Loss I 0.0028 | Loss O 2.9783 | \n",
      "Epoch 0025: Loss I 0.0025 | Loss O 3.0263 | \n",
      "Epoch 0026: Loss I 0.0023 | Loss O 3.0704 | \n",
      "Epoch 0027: Loss I 0.0021 | Loss O 3.1135 | \n",
      "Epoch 0028: Loss I 0.0019 | Loss O 3.1557 | \n",
      "Epoch 0029: Loss I 0.0018 | Loss O 3.1984 | \n",
      "Epoch 0030: Loss I 0.0016 | Loss O 3.2377 | \n",
      "Epoch 0031: Loss I 0.0015 | Loss O 3.2778 | \n",
      "Epoch 0032: Loss I 0.0014 | Loss O 3.3159 | \n",
      "Epoch 0033: Loss I 0.0013 | Loss O 3.3525 | \n",
      "Epoch 0034: Loss I 0.0012 | Loss O 3.3875 | \n",
      "Epoch 0035: Loss I 0.0011 | Loss O 3.4245 | \n",
      "Epoch 0036: Loss I 0.0011 | Loss O 3.4577 | \n",
      "Epoch 0037: Loss I 0.0010 | Loss O 3.4927 | \n",
      "Epoch 0038: Loss I 0.0009 | Loss O 3.5266 | \n",
      "Epoch 0039: Loss I 0.0009 | Loss O 3.5601 | \n",
      "Epoch 0040: Loss I 0.0008 | Loss O 3.5920 | \n",
      "Epoch 0041: Loss I 0.0008 | Loss O 3.6252 | \n",
      "Epoch 0042: Loss I 0.0007 | Loss O 3.6558 | \n",
      "Epoch 0043: Loss I 0.0007 | Loss O 3.6848 | \n",
      "Epoch 0044: Loss I 0.0006 | Loss O 3.7156 | \n",
      "Epoch 0045: Loss I 0.0006 | Loss O 3.7455 | \n",
      "Epoch 0046: Loss I 0.0006 | Loss O 3.7760 | \n",
      "Epoch 0047: Loss I 0.0005 | Loss O 3.8055 | \n",
      "Epoch 0048: Loss I 0.0005 | Loss O 3.8330 | \n",
      "Epoch 0049: Loss I 0.0005 | Loss O 3.8623 | \n",
      "Epoch 0050: Loss I 0.0004 | Loss O 3.8883 | \n",
      "Epoch 0051: Loss I 0.0004 | Loss O 3.9182 | \n",
      "Epoch 0052: Loss I 0.0004 | Loss O 3.9457 | \n",
      "Epoch 0053: Loss I 0.0004 | Loss O 3.9720 | \n",
      "Epoch 0054: Loss I 0.0004 | Loss O 3.9995 | \n",
      "Epoch 0055: Loss I 0.0003 | Loss O 4.0270 | \n",
      "Epoch 0056: Loss I 0.0003 | Loss O 4.0529 | \n",
      "Epoch 0057: Loss I 0.0003 | Loss O 4.0776 | \n",
      "Epoch 0058: Loss I 0.0003 | Loss O 4.1042 | \n",
      "Epoch 0059: Loss I 0.0003 | Loss O 4.1305 | \n",
      "Epoch 0060: Loss I 0.0003 | Loss O 4.1556 | \n",
      "Epoch 0061: Loss I 0.0002 | Loss O 4.1812 | \n",
      "Epoch 0062: Loss I 0.0002 | Loss O 4.2060 | \n",
      "Epoch 0063: Loss I 0.0002 | Loss O 4.2310 | \n",
      "Epoch 0064: Loss I 0.0002 | Loss O 4.2560 | \n",
      "Epoch 0065: Loss I 0.0002 | Loss O 4.2797 | \n",
      "Epoch 0066: Loss I 0.0002 | Loss O 4.3043 | \n",
      "Epoch 0067: Loss I 0.0002 | Loss O 4.3293 | \n",
      "Epoch 0068: Loss I 0.0002 | Loss O 4.3531 | \n",
      "Epoch 0069: Loss I 0.0002 | Loss O 4.3773 | \n",
      "Epoch 0070: Loss I 0.0002 | Loss O 4.4019 | \n",
      "Epoch 0071: Loss I 0.0002 | Loss O 4.4257 | \n",
      "Epoch 0072: Loss I 0.0001 | Loss O 4.4485 | \n",
      "Epoch 0073: Loss I 0.0001 | Loss O 4.4727 | \n",
      "Epoch 0074: Loss I 0.0001 | Loss O 4.4945 | \n",
      "Epoch 0075: Loss I 0.0001 | Loss O 4.5191 | \n",
      "Epoch 0076: Loss I 0.0001 | Loss O 4.5418 | \n",
      "Epoch 0077: Loss I 0.0001 | Loss O 4.5655 | \n",
      "Epoch 0078: Loss I 0.0001 | Loss O 4.5875 | \n",
      "Epoch 0079: Loss I 0.0001 | Loss O 4.6102 | \n",
      "Epoch 0080: Loss I 0.0001 | Loss O 4.6340 | \n",
      "Epoch 0081: Loss I 0.0001 | Loss O 4.6564 | \n",
      "Epoch 0082: Loss I 0.0001 | Loss O 4.6791 | \n",
      "Epoch 0083: Loss I 0.0001 | Loss O 4.7008 | \n",
      "Epoch 0084: Loss I 0.0001 | Loss O 4.7244 | \n",
      "Epoch 0085: Loss I 0.0001 | Loss O 4.7467 | \n",
      "Epoch 0086: Loss I 0.0001 | Loss O 4.7683 | \n",
      "Epoch 0087: Loss I 0.0001 | Loss O 4.7907 | \n",
      "Epoch 0088: Loss I 0.0001 | Loss O 4.8125 | \n",
      "Epoch 0089: Loss I 0.0001 | Loss O 4.8346 | \n",
      "Epoch 0090: Loss I 0.0001 | Loss O 4.8574 | \n",
      "Epoch 0091: Loss I 0.0001 | Loss O 4.8792 | \n",
      "Epoch 0092: Loss I 0.0001 | Loss O 4.9011 | \n",
      "Epoch 0093: Loss I 0.0001 | Loss O 4.9231 | \n",
      "Epoch 0094: Loss I 0.0001 | Loss O 4.9437 | \n",
      "Epoch 0095: Loss I 0.0001 | Loss O 4.9665 | \n",
      "Epoch 0096: Loss I 0.0000 | Loss O 4.9877 | \n",
      "Epoch 0097: Loss I 0.0000 | Loss O 5.0093 | \n",
      "Epoch 0098: Loss I 0.0000 | Loss O 5.0318 | \n",
      "Epoch 0099: Loss I 0.0000 | Loss O 5.0528 | \n",
      "Epoch 0100: Loss I 0.0000 | Loss O 5.0742 | \n",
      "Epoch 0101: Loss I 0.0000 | Loss O 5.0948 | \n",
      "Epoch 0102: Loss I 0.0000 | Loss O 5.1173 | \n",
      "Epoch 0103: Loss I 0.0000 | Loss O 5.1387 | \n",
      "Epoch 0104: Loss I 0.0000 | Loss O 5.1601 | \n",
      "Epoch 0105: Loss I 0.0000 | Loss O 5.1808 | \n",
      "Epoch 0106: Loss I 0.0000 | Loss O 5.2028 | \n",
      "Epoch 0107: Loss I 0.0000 | Loss O 5.2237 | \n",
      "Epoch 0108: Loss I 0.0000 | Loss O 5.2443 | \n",
      "Epoch 0109: Loss I 0.0000 | Loss O 5.2653 | \n",
      "Epoch 0110: Loss I 0.0000 | Loss O 5.2870 | \n",
      "Epoch 0111: Loss I 0.0000 | Loss O 5.3081 | \n",
      "Epoch 0112: Loss I 0.0000 | Loss O 5.3294 | \n",
      "Epoch 0113: Loss I 0.0000 | Loss O 5.3503 | \n",
      "Epoch 0114: Loss I 0.0000 | Loss O 5.3699 | \n",
      "Epoch 0115: Loss I 0.0000 | Loss O 5.3919 | \n",
      "Epoch 0116: Loss I 0.0000 | Loss O 5.4126 | \n",
      "Epoch 0117: Loss I 0.0000 | Loss O 5.4333 | \n",
      "Epoch 0118: Loss I 0.0000 | Loss O 5.4554 | \n",
      "Epoch 0119: Loss I 0.0000 | Loss O 5.4752 | \n",
      "Epoch 0120: Loss I 0.0000 | Loss O 5.4960 | \n",
      "Epoch 0121: Loss I 0.0000 | Loss O 5.5166 | \n",
      "Epoch 0122: Loss I 0.0000 | Loss O 5.5383 | \n",
      "Epoch 0123: Loss I 0.0000 | Loss O 5.5585 | \n",
      "Epoch 0124: Loss I 0.0000 | Loss O 5.5795 | \n",
      "Epoch 0125: Loss I 0.0000 | Loss O 5.6000 | \n",
      "Epoch 0126: Loss I 0.0000 | Loss O 5.6208 | \n",
      "Epoch 0127: Loss I 0.0000 | Loss O 5.6418 | \n",
      "Epoch 0128: Loss I 0.0000 | Loss O 5.6610 | \n",
      "Epoch 0129: Loss I 0.0000 | Loss O 5.6824 | \n",
      "Epoch 0130: Loss I 0.0000 | Loss O 5.7025 | \n",
      "Epoch 0131: Loss I 0.0000 | Loss O 5.7232 | \n",
      "Epoch 0132: Loss I 0.0000 | Loss O 5.7446 | \n",
      "Epoch 0133: Loss I 0.0000 | Loss O 5.7641 | \n",
      "Epoch 0134: Loss I 0.0000 | Loss O 5.7846 | \n",
      "Epoch 0135: Loss I 0.0000 | Loss O 5.8066 | \n",
      "Epoch 0136: Loss I 0.0000 | Loss O 5.8267 | \n",
      "Epoch 0137: Loss I 0.0000 | Loss O 5.8465 | \n",
      "Epoch 0138: Loss I 0.0000 | Loss O 5.8668 | \n",
      "Epoch 0139: Loss I 0.0000 | Loss O 5.8871 | \n",
      "Epoch 0140: Loss I 0.0000 | Loss O 5.9085 | \n",
      "Epoch 0141: Loss I 0.0000 | Loss O 5.9286 | \n",
      "Epoch 0142: Loss I 0.0000 | Loss O 5.9485 | \n",
      "Epoch 0143: Loss I 0.0000 | Loss O 5.9695 | \n",
      "Epoch 0144: Loss I 0.0000 | Loss O 5.9898 | \n",
      "Epoch 0145: Loss I 0.0000 | Loss O 6.0097 | \n",
      "Epoch 0146: Loss I 0.0000 | Loss O 6.0298 | \n",
      "Epoch 0147: Loss I 0.0000 | Loss O 6.0502 | \n",
      "Epoch 0148: Loss I 0.0000 | Loss O 6.0699 | \n",
      "Epoch 0149: Loss I 0.0000 | Loss O 6.0909 | \n",
      "Epoch 0150: Loss I 0.0000 | Loss O 6.1112 | \n",
      "Epoch 0151: Loss I 0.0000 | Loss O 6.1315 | \n",
      "Epoch 0152: Loss I 0.0000 | Loss O 6.1511 | \n",
      "Epoch 0153: Loss I 0.0000 | Loss O 6.1718 | \n",
      "Epoch 0154: Loss I 0.0000 | Loss O 6.1920 | \n",
      "Epoch 0155: Loss I 0.0000 | Loss O 6.2129 | \n",
      "Epoch 0156: Loss I 0.0000 | Loss O 6.2328 | \n",
      "Epoch 0157: Loss I 0.0000 | Loss O 6.2513 | \n",
      "Epoch 0158: Loss I 0.0000 | Loss O 6.2730 | \n",
      "Epoch 0159: Loss I 0.0000 | Loss O 6.2931 | \n",
      "Epoch 0160: Loss I 0.0000 | Loss O 6.3137 | \n",
      "Epoch 0161: Loss I 0.0000 | Loss O 6.3333 | \n",
      "Epoch 0162: Loss I 0.0000 | Loss O 6.3536 | \n",
      "Epoch 0163: Loss I 0.0000 | Loss O 6.3745 | \n",
      "Epoch 0164: Loss I 0.0000 | Loss O 6.3935 | \n",
      "Epoch 0165: Loss I 0.0000 | Loss O 6.4139 | \n",
      "Epoch 0166: Loss I 0.0000 | Loss O 6.4337 | \n",
      "Epoch 0167: Loss I 0.0000 | Loss O 6.4543 | \n",
      "Epoch 0168: Loss I 0.0000 | Loss O 6.4732 | \n",
      "Epoch 0169: Loss I 0.0000 | Loss O 6.4933 | \n",
      "Epoch 0170: Loss I 0.0000 | Loss O 6.5147 | \n",
      "Epoch 0171: Loss I 0.0000 | Loss O 6.5344 | \n",
      "Epoch 0172: Loss I 0.0000 | Loss O 6.5546 | \n",
      "Epoch 0173: Loss I 0.0000 | Loss O 6.5735 | \n",
      "Epoch 0174: Loss I 0.0000 | Loss O 6.5940 | \n",
      "Epoch 0175: Loss I 0.0000 | Loss O 6.6129 | \n",
      "Epoch 0176: Loss I 0.0000 | Loss O 6.6341 | \n",
      "Epoch 0177: Loss I 0.0000 | Loss O 6.6537 | \n",
      "Epoch 0178: Loss I 0.0000 | Loss O 6.6732 | \n",
      "Epoch 0179: Loss I 0.0000 | Loss O 6.6936 | \n",
      "Epoch 0180: Loss I 0.0000 | Loss O 6.7144 | \n",
      "Epoch 0181: Loss I 0.0000 | Loss O 6.7340 | \n",
      "Epoch 0182: Loss I 0.0000 | Loss O 6.7542 | \n",
      "Epoch 0183: Loss I 0.0000 | Loss O 6.7755 | \n",
      "Epoch 0184: Loss I 0.0000 | Loss O 6.7939 | \n",
      "Epoch 0185: Loss I 0.0000 | Loss O 6.8134 | \n",
      "Epoch 0186: Loss I 0.0000 | Loss O 6.8343 | \n",
      "Epoch 0187: Loss I 0.0000 | Loss O 6.8558 | \n",
      "Epoch 0188: Loss I 0.0000 | Loss O 6.8734 | \n",
      "Epoch 0189: Loss I 0.0000 | Loss O 6.8941 | \n",
      "Epoch 0190: Loss I 0.0000 | Loss O 6.9151 | \n",
      "Epoch 0191: Loss I 0.0000 | Loss O 6.9355 | \n",
      "Epoch 0192: Loss I 0.0000 | Loss O 6.9562 | \n",
      "Epoch 0193: Loss I 0.0000 | Loss O 6.9759 | \n",
      "Epoch 0194: Loss I 0.0000 | Loss O 6.9974 | \n",
      "Epoch 0195: Loss I 0.0000 | Loss O 7.0202 | \n",
      "Epoch 0196: Loss I 0.0000 | Loss O 7.0412 | \n",
      "Epoch 0197: Loss I 0.0000 | Loss O 7.0622 | \n",
      "Epoch 0198: Loss I 0.0000 | Loss O 7.0827 | \n",
      "Epoch 0199: Loss I 0.0000 | Loss O 7.1044 | \n",
      "Epoch 0000: Loss 1052221.3750 | \n",
      "Epoch 0001: Loss 1045006.8125 | \n",
      "Epoch 0002: Loss 1039425.5000 | \n",
      "Epoch 0003: Loss 1035090.1875 | \n",
      "Epoch 0004: Loss 1031673.4375 | \n",
      "Epoch 0005: Loss 1028903.6875 | \n",
      "Epoch 0006: Loss 1026560.1875 | \n",
      "Epoch 0007: Loss 1024470.3750 | \n",
      "Epoch 0008: Loss 1022496.0000 | \n",
      "Epoch 0009: Loss 1020521.9375 | \n",
      "Epoch 0010: Loss 1018450.8125 | \n",
      "Epoch 0011: Loss 1016201.4375 | \n",
      "Epoch 0012: Loss 1013708.2500 | \n",
      "Epoch 0013: Loss 1010919.7500 | \n",
      "Epoch 0014: Loss 1007796.6250 | \n",
      "Epoch 0015: Loss 1004309.5000 | \n",
      "Epoch 0016: Loss 1000437.5000 | \n",
      "Epoch 0017: Loss 996165.6875 | \n",
      "Epoch 0018: Loss 991482.3125 | \n",
      "Epoch 0019: Loss 986377.8750 | \n",
      "Epoch 0020: Loss 980844.6875 | \n",
      "Epoch 0021: Loss 974876.5000 | \n",
      "Epoch 0022: Loss 968467.5000 | \n",
      "Epoch 0023: Loss 961614.4375 | \n",
      "Epoch 0024: Loss 954316.9375 | \n",
      "Epoch 0025: Loss 946577.4375 | \n",
      "Epoch 0026: Loss 938402.6250 | \n",
      "Epoch 0027: Loss 929804.2500 | \n",
      "Epoch 0028: Loss 920798.3750 | \n",
      "Epoch 0029: Loss 911404.5000 | \n",
      "Epoch 0030: Loss 901644.6250 | \n",
      "Epoch 0031: Loss 891543.4375 | \n",
      "Epoch 0032: Loss 881127.6875 | \n",
      "Epoch 0033: Loss 870424.8750 | \n",
      "Epoch 0034: Loss 859462.8125 | \n",
      "Epoch 0035: Loss 848269.0000 | \n",
      "Epoch 0036: Loss 836870.3750 | \n",
      "Epoch 0037: Loss 825292.8125 | \n",
      "Epoch 0038: Loss 813560.3125 | \n",
      "Epoch 0039: Loss 801694.5625 | \n",
      "Epoch 0040: Loss 789716.7500 | \n",
      "Epoch 0041: Loss 777648.8750 | \n",
      "Epoch 0042: Loss 765512.6250 | \n",
      "Epoch 0043: Loss 753327.2500 | \n",
      "Epoch 0044: Loss 741110.3750 | \n",
      "Epoch 0045: Loss 728882.6250 | \n",
      "Epoch 0046: Loss 716669.0625 | \n",
      "Epoch 0047: Loss 704496.8750 | \n",
      "Epoch 0048: Loss 692390.5625 | \n",
      "Epoch 0049: Loss 680370.3750 | \n",
      "Epoch 0050: Loss 668452.7500 | \n",
      "Epoch 0051: Loss 656649.1250 | \n",
      "Epoch 0052: Loss 644969.1250 | \n",
      "Epoch 0053: Loss 633422.8125 | \n",
      "Epoch 0054: Loss 622019.5000 | \n",
      "Epoch 0055: Loss 610765.1250 | \n",
      "Epoch 0056: Loss 599661.0625 | \n",
      "Epoch 0057: Loss 588708.5000 | \n",
      "Epoch 0058: Loss 577912.1875 | \n",
      "Epoch 0059: Loss 567281.9375 | \n",
      "Epoch 0060: Loss 556827.6875 | \n",
      "Epoch 0061: Loss 546553.3125 | \n",
      "Epoch 0062: Loss 536455.8750 | \n",
      "Epoch 0063: Loss 526532.1250 | \n",
      "Epoch 0064: Loss 516783.4375 | \n",
      "Epoch 0065: Loss 507212.0938 | \n",
      "Epoch 0066: Loss 497816.7188 | \n",
      "Epoch 0067: Loss 488596.3750 | \n",
      "Epoch 0068: Loss 479552.5938 | \n",
      "Epoch 0069: Loss 470684.1562 | \n",
      "Epoch 0070: Loss 461990.6250 | \n",
      "Epoch 0071: Loss 453475.0625 | \n",
      "Epoch 0072: Loss 445142.9375 | \n",
      "Epoch 0073: Loss 436998.5625 | \n",
      "Epoch 0074: Loss 429042.4062 | \n",
      "Epoch 0075: Loss 421271.4375 | \n",
      "Epoch 0076: Loss 413681.7500 | \n",
      "Epoch 0077: Loss 406269.0312 | \n",
      "Epoch 0078: Loss 399030.3125 | \n",
      "Epoch 0079: Loss 391962.7500 | \n",
      "Epoch 0080: Loss 385062.5312 | \n",
      "Epoch 0081: Loss 378327.4375 | \n",
      "Epoch 0082: Loss 371756.3750 | \n",
      "Epoch 0083: Loss 365347.4062 | \n",
      "Epoch 0084: Loss 359093.9688 | \n",
      "Epoch 0085: Loss 352988.3125 | \n",
      "Epoch 0086: Loss 347024.3125 | \n",
      "Epoch 0087: Loss 341197.7188 | \n",
      "Epoch 0088: Loss 335504.7500 | \n",
      "Epoch 0089: Loss 329940.1562 | \n",
      "Epoch 0090: Loss 324500.4688 | \n",
      "Epoch 0091: Loss 319181.6562 | \n",
      "Epoch 0092: Loss 313980.7500 | \n",
      "Epoch 0093: Loss 308896.7500 | \n",
      "Epoch 0094: Loss 303929.1562 | \n",
      "Epoch 0095: Loss 299077.5625 | \n",
      "Epoch 0096: Loss 294339.8750 | \n",
      "Epoch 0097: Loss 289711.4062 | \n",
      "Epoch 0098: Loss 285188.0625 | \n",
      "Epoch 0099: Loss 280767.2500 | \n",
      "Epoch 0100: Loss 276446.0312 | \n",
      "Epoch 0101: Loss 272223.4062 | \n",
      "Epoch 0102: Loss 268098.1875 | \n",
      "Epoch 0103: Loss 264069.8125 | \n",
      "Epoch 0104: Loss 260137.0156 | \n",
      "Epoch 0105: Loss 256297.3438 | \n",
      "Epoch 0106: Loss 252550.0781 | \n",
      "Epoch 0107: Loss 248894.6250 | \n",
      "Epoch 0108: Loss 245330.0469 | \n",
      "Epoch 0109: Loss 241854.2969 | \n",
      "Epoch 0110: Loss 238465.9375 | \n",
      "Epoch 0111: Loss 235164.1406 | \n",
      "Epoch 0112: Loss 231946.9062 | \n",
      "Epoch 0113: Loss 228812.7344 | \n",
      "Epoch 0114: Loss 225759.9219 | \n",
      "Epoch 0115: Loss 222786.4531 | \n",
      "Epoch 0116: Loss 219890.8906 | \n",
      "Epoch 0117: Loss 217073.1719 | \n",
      "Epoch 0118: Loss 214332.1250 | \n",
      "Epoch 0119: Loss 211666.9375 | \n",
      "Epoch 0120: Loss 209076.4531 | \n",
      "Epoch 0121: Loss 206559.7031 | \n",
      "Epoch 0122: Loss 204115.5938 | \n",
      "Epoch 0123: Loss 201743.6719 | \n",
      "Epoch 0124: Loss 199443.7500 | \n",
      "Epoch 0125: Loss 197215.1719 | \n",
      "Epoch 0126: Loss 195057.4531 | \n",
      "Epoch 0127: Loss 192969.7344 | \n",
      "Epoch 0128: Loss 190951.4844 | \n",
      "Epoch 0129: Loss 189002.5781 | \n",
      "Epoch 0130: Loss 187122.0625 | \n",
      "Epoch 0131: Loss 185309.3125 | \n",
      "Epoch 0132: Loss 183564.1250 | \n",
      "Epoch 0133: Loss 181884.4844 | \n",
      "Epoch 0134: Loss 180270.2500 | \n",
      "Epoch 0135: Loss 178720.5938 | \n",
      "Epoch 0136: Loss 177233.7656 | \n",
      "Epoch 0137: Loss 175808.7500 | \n",
      "Epoch 0138: Loss 174444.3750 | \n",
      "Epoch 0139: Loss 173139.5938 | \n",
      "Epoch 0140: Loss 171892.8438 | \n",
      "Epoch 0141: Loss 170702.9688 | \n",
      "Epoch 0142: Loss 169568.7656 | \n",
      "Epoch 0143: Loss 168488.1094 | \n",
      "Epoch 0144: Loss 167459.2031 | \n",
      "Epoch 0145: Loss 166481.4531 | \n",
      "Epoch 0146: Loss 165553.2656 | \n",
      "Epoch 0147: Loss 164672.6562 | \n",
      "Epoch 0148: Loss 163837.6719 | \n",
      "Epoch 0149: Loss 163046.4688 | \n",
      "Epoch 0150: Loss 162298.4844 | \n",
      "Epoch 0151: Loss 161591.3750 | \n",
      "Epoch 0152: Loss 160922.9375 | \n",
      "Epoch 0153: Loss 160292.0938 | \n",
      "Epoch 0154: Loss 159698.3125 | \n",
      "Epoch 0155: Loss 159139.0625 | \n",
      "Epoch 0156: Loss 158612.2031 | \n",
      "Epoch 0157: Loss 158117.5938 | \n",
      "Epoch 0158: Loss 157653.4219 | \n",
      "Epoch 0159: Loss 157217.5938 | \n",
      "Epoch 0160: Loss 156808.8750 | \n",
      "Epoch 0161: Loss 156426.6562 | \n",
      "Epoch 0162: Loss 156069.2656 | \n",
      "Epoch 0163: Loss 155734.7969 | \n",
      "Epoch 0164: Loss 155422.4844 | \n",
      "Epoch 0165: Loss 155131.1250 | \n",
      "Epoch 0166: Loss 154859.6406 | \n",
      "Epoch 0167: Loss 154606.6719 | \n",
      "Epoch 0168: Loss 154370.5938 | \n",
      "Epoch 0169: Loss 154151.1719 | \n",
      "Epoch 0170: Loss 153947.3125 | \n",
      "Epoch 0171: Loss 153757.3125 | \n",
      "Epoch 0172: Loss 153580.7344 | \n",
      "Epoch 0173: Loss 153416.8281 | \n",
      "Epoch 0174: Loss 153264.2500 | \n",
      "Epoch 0175: Loss 153122.3281 | \n",
      "Epoch 0176: Loss 152990.8125 | \n",
      "Epoch 0177: Loss 152868.9062 | \n",
      "Epoch 0178: Loss 152755.2344 | \n",
      "Epoch 0179: Loss 152649.6406 | \n",
      "Epoch 0180: Loss 152551.9688 | \n",
      "Epoch 0181: Loss 152460.8281 | \n",
      "Epoch 0182: Loss 152375.7969 | \n",
      "Epoch 0183: Loss 152296.5781 | \n",
      "Epoch 0184: Loss 152222.7969 | \n",
      "Epoch 0185: Loss 152154.0781 | \n",
      "Epoch 0186: Loss 152089.5938 | \n",
      "Epoch 0187: Loss 152029.4531 | \n",
      "Epoch 0188: Loss 151973.3906 | \n",
      "Epoch 0189: Loss 151920.5469 | \n",
      "Epoch 0190: Loss 151871.1562 | \n",
      "Epoch 0191: Loss 151824.7969 | \n",
      "Epoch 0192: Loss 151780.9531 | \n",
      "Epoch 0193: Loss 151739.9375 | \n",
      "Epoch 0194: Loss 151701.2500 | \n",
      "Epoch 0195: Loss 151664.4844 | \n",
      "Epoch 0196: Loss 151629.7969 | \n",
      "Epoch 0197: Loss 151597.0312 | \n",
      "Epoch 0198: Loss 151566.0469 | \n",
      "Epoch 0199: Loss 151536.6094 | \n",
      "Training GCN, epoch 0\n",
      "Training GCN, epoch 100\n",
      "Training GCN, epoch 200\n",
      "Training GCN, epoch 300\n",
      "Training GCN, epoch 400\n",
      "Training GCN, epoch 500\n",
      "Training GCN, epoch 600\n",
      "Training GCN, epoch 700\n",
      "Training GCN, epoch 800\n",
      "Training GCN, epoch 900\n",
      "Training GCN, epoch 1000\n",
      "Training GCN, epoch 1100\n",
      "Training GCN, epoch 1200\n",
      "Training GCN, epoch 1300\n",
      "Training GCN, epoch 1400\n",
      "Training GCN, epoch 1500\n",
      "Training GCN, epoch 1600\n",
      "Training GCN, epoch 1700\n",
      "Training GCN, epoch 1800\n",
      "Training GCN, epoch 1900\n",
      "Test: Loss I 0.0000 | Loss O 0.2278 | \n",
      "Test: Loss 643967.2188 | \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "ths = {\n",
    "    \"celiac\": 0.98,\n",
    "    \"bladder_cancer\": 0.93,\n",
    "    \"colorectal_cancer\": 0.98,\n",
    "    \"parkinson\": 0.98,\n",
    "}\n",
    "\n",
    "df_result_dl = None\n",
    "\n",
    "for i in range(2):\n",
    "    print(f\"Iteration {i+1} of 2\")    \n",
    "    for dataset_name in datasets_name:\n",
    "        \n",
    "        df = load_dataset(datasets_path + sep + dataset_name)\n",
    "        if df is None:\n",
    "            print(f\"Dataset {dataset_name} not found or empty.\")\n",
    "            continue\n",
    "\n",
    "        if \"colorectal_cancer\" != dataset_name:\n",
    "            \n",
    "            targets = df['Target']\n",
    "            expr = df.drop(columns=['Target'])\n",
    "            print(\"Running node anomaly detection, \", expr.shape)\n",
    "            df_result, models_dl, dataloader_test = node_anomaly_detection(expr, targets, dataset_name, gaan_params=None, th = ths[dataset_name])\n",
    "            df_result['Dataset'] = dataset_name\n",
    "        \n",
    "        df_result_dl = pd.concat([df_result_dl, df_result]) if df_result_dl is not None else df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fa7f710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 2\n",
      "Loading dataset from c:\\Users\\Utente\\Desktop\\E-ABIN\\use_case\\data\\bladder_cancer\n",
      "Loading train/test split from disk...\n",
      "Running ML models\n",
      "Number of splits for cross-validation: 5\n",
      "Unique classes in target: {0: 2, 1: 5}\n",
      "Not enough data for 5 splits, reducing to 2 splits.\n",
      "Cross-validation splits:  2\n",
      "Performing grid search for hyperparameter tuning...\n",
      "Grid search for LR...\n",
      "Performing grid search for LR with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__solver': ['liblinear', 'saga'], 'model__penalty': ['l2', 'l1', None], 'model__max_iter': [100, 200, 300]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning:\n",
      "\n",
      "\n",
      "18 fits failed out of a total of 108.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [1.         0.83333333 1.         1.         0.83333333 1.\n",
      " 0.5        0.5        0.58333333 0.5        0.5        1.\n",
      "        nan        nan        nan 1.         0.83333333 1.\n",
      " 1.         0.83333333 1.         1.         0.83333333 1.\n",
      " 0.5        0.5        0.83333333 0.5        0.5        1.\n",
      "        nan        nan        nan 1.         0.83333333 1.\n",
      " 1.         0.83333333 1.         1.         0.83333333 1.\n",
      " 0.5        0.5        1.         0.5        0.5        1.\n",
      "        nan        nan        nan 1.         0.83333333 1.        ]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__max_iter': 100, 'model__penalty': 'l2', 'model__solver': 'liblinear', 'scaler': None}\n",
      "Best score: 1.0000\n",
      "Best model for LR: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 LogisticRegression(class_weight='balanced',\n",
      "                                    solver='liblinear'))])\n",
      "Grid search for KNN...\n",
      "Performing grid search for KNN with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__n_neighbors': [3, 5, 7], 'model__metric': ['euclidean', 'manhattan']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [0.5   0.5   0.5     nan   nan   nan   nan   nan   nan 0.5   0.625 0.625\n",
      " 0.5   0.5   0.5   0.5   0.5   0.5  ]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__metric': 'manhattan', 'model__n_neighbors': 3, 'scaler': MinMaxScaler()}\n",
      "Best score: 0.6250\n",
      "Best model for KNN: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 KNeighborsClassifier(metric='manhattan', n_neighbors=3))])\n",
      "Grid search for DT...\n",
      "Performing grid search for DT with parameters: {'scaler': [None], 'model__max_depth': [None, 5, 10], 'model__min_samples_split': [2, 5, 10]}\n",
      "Best parameters: {'model__max_depth': 10, 'model__min_samples_split': 2, 'scaler': None}\n",
      "Best score: 0.8333\n",
      "Best model for DT: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 DecisionTreeClassifier(class_weight='balanced',\n",
      "                                        max_depth=10))])\n",
      "Grid search for SVM...\n",
      "Performing grid search for SVM with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__kernel': ['linear', 'rbf'], 'model__C': [0.1, 1, 10]}\n",
      "Best parameters: {'model__C': 0.1, 'model__kernel': 'linear', 'scaler': None}\n",
      "Best score: 1.0000\n",
      "Best model for SVM: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 SVC(C=0.1, class_weight='balanced', kernel='linear',\n",
      "                     probability=True))])\n",
      "Grid search for RF...\n",
      "Performing grid search for RF with parameters: {'scaler': [None], 'model__n_estimators': [50, 100, 200], 'model__max_depth': [None, 5, 10]}\n",
      "Best parameters: {'model__max_depth': None, 'model__n_estimators': 50, 'scaler': None}\n",
      "Best score: 1.0000\n",
      "Best model for RF: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 RandomForestClassifier(class_weight='balanced',\n",
      "                                        n_estimators=50))])\n",
      "LR: 1.0000 (0.0000)\n",
      "KNN: 0.7500 (0.2500)\n",
      "DT: 0.9167 (0.0833)\n",
      "SVM: 0.0000 (0.0000)\n",
      "RF: 1.0000 (0.0000)\n",
      "LR [1.0, 1.0]\n",
      "KNN [0.5, 1.0]\n",
      "DT [0.8333333333333333, 1.0]\n",
      "SVM [0.0, 0.0]\n",
      "RF [1.0, 1.0]\n",
      "Best model: LR with roc_auc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from c:\\Users\\Utente\\Desktop\\E-ABIN\\use_case\\data\\celiac\n",
      "Loading train/test split from disk...\n",
      "Running ML models\n",
      "Number of splits for cross-validation: 5\n",
      "Unique classes in target: {0: 6, 1: 33}\n",
      "Cross-validation splits:  5\n",
      "Performing grid search for hyperparameter tuning...\n",
      "Grid search for LR...\n",
      "Performing grid search for LR with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__solver': ['liblinear', 'saga'], 'model__penalty': ['l2', 'l1', None], 'model__max_iter': [100, 200, 300]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning:\n",
      "\n",
      "\n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [0.82857143 0.77142857 0.8        0.82857143 0.77142857 0.8\n",
      " 0.72857143 0.60714286 0.77142857 0.82857143 0.85714286 0.82857143\n",
      "        nan        nan        nan 0.82857143 0.77142857 0.77142857\n",
      " 0.82857143 0.77142857 0.8        0.82857143 0.77142857 0.77142857\n",
      " 0.72857143 0.60714286 0.77142857 0.82857143 0.82857143 0.82857143\n",
      "        nan        nan        nan 0.82857143 0.77142857 0.77142857\n",
      " 0.82857143 0.77142857 0.8        0.82857143 0.77142857 0.77142857\n",
      " 0.72857143 0.60714286 0.8        0.82857143 0.82857143 0.82857143\n",
      "        nan        nan        nan 0.82857143 0.77142857 0.77142857]\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__max_iter': 100, 'model__penalty': 'l1', 'model__solver': 'saga', 'scaler': MinMaxScaler()}\n",
      "Best score: 0.8571\n",
      "Best model for LR: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 LogisticRegression(class_weight='balanced', penalty='l1',\n",
      "                                    solver='saga'))])\n",
      "Grid search for KNN...\n",
      "Performing grid search for KNN with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__n_neighbors': [3, 5, 7], 'model__metric': ['euclidean', 'manhattan']}\n",
      "Best parameters: {'model__metric': 'euclidean', 'model__n_neighbors': 7, 'scaler': None}\n",
      "Best score: 0.8131\n",
      "Best model for KNN: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=7))])\n",
      "Grid search for DT...\n",
      "Performing grid search for DT with parameters: {'scaler': [None], 'model__max_depth': [None, 5, 10], 'model__min_samples_split': [2, 5, 10]}\n",
      "Best parameters: {'model__max_depth': None, 'model__min_samples_split': 2, 'scaler': None}\n",
      "Best score: 0.5571\n",
      "Best model for DT: Pipeline(steps=[('scaler', None),\n",
      "                ('model', DecisionTreeClassifier(class_weight='balanced'))])\n",
      "Grid search for SVM...\n",
      "Performing grid search for SVM with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__kernel': ['linear', 'rbf'], 'model__C': [0.1, 1, 10]}\n",
      "Best parameters: {'model__C': 0.1, 'model__kernel': 'rbf', 'scaler': None}\n",
      "Best score: 0.8286\n",
      "Best model for SVM: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 SVC(C=0.1, class_weight='balanced', probability=True))])\n",
      "Grid search for RF...\n",
      "Performing grid search for RF with parameters: {'scaler': [None], 'model__n_estimators': [50, 100, 200], 'model__max_depth': [None, 5, 10]}\n",
      "Best parameters: {'model__max_depth': 5, 'model__n_estimators': 100, 'scaler': None}\n",
      "Best score: 0.8429\n",
      "Best model for RF: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 RandomForestClassifier(class_weight='balanced', max_depth=5))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.8833 (0.1633)\n",
      "KNN: 0.7833 (0.2963)\n",
      "DT: 0.6095 (0.1661)\n",
      "SVM: 0.1167 (0.1633)\n",
      "RF: 0.9083 (0.1190)\n",
      "LR [0.5833333333333334, 1.0, 1.0, 1.0, 0.8333333333333334]\n",
      "KNN [0.6666666666666667, 1.0, 1.0, 1.0, 0.25]\n",
      "DT [0.5, 0.7857142857142857, 0.5, 0.42857142857142855, 0.8333333333333333]\n",
      "SVM [0.4166666666666667, 0.0, 0.0, 0.0, 0.16666666666666666]\n",
      "RF [0.7083333333333333, 1.0, 1.0, 1.0, 0.8333333333333334]\n",
      "Best model: RF with roc_auc: 0.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from c:\\Users\\Utente\\Desktop\\E-ABIN\\use_case\\data\\colorectal_cancer\n",
      "Loading train/test split from disk...\n",
      "Running ML models\n",
      "Number of splits for cross-validation: 5\n",
      "Unique classes in target: {0: 7, 1: 37}\n",
      "Cross-validation splits:  5\n",
      "Performing grid search for hyperparameter tuning...\n",
      "Grid search for LR...\n",
      "Performing grid search for LR with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__solver': ['liblinear', 'saga'], 'model__penalty': ['l2', 'l1', None], 'model__max_iter': [100, 200, 300]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning:\n",
      "\n",
      "\n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [0.975 1.    1.    0.975 1.    1.    1.    1.    1.    0.975 1.    0.975\n",
      "   nan   nan   nan 0.975 1.    1.    0.975 1.    1.    0.975 1.    1.\n",
      " 1.    1.    1.    0.975 1.    0.975   nan   nan   nan 0.975 1.    1.\n",
      " 0.975 1.    1.    0.975 1.    1.    1.    1.    1.    0.975 1.    0.975\n",
      "   nan   nan   nan 0.975 1.    1.   ]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__max_iter': 100, 'model__penalty': 'l2', 'model__solver': 'liblinear', 'scaler': MinMaxScaler()}\n",
      "Best score: 1.0000\n",
      "Best model for LR: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 LogisticRegression(class_weight='balanced',\n",
      "                                    solver='liblinear'))])\n",
      "Grid search for KNN...\n",
      "Performing grid search for KNN with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__n_neighbors': [3, 5, 7], 'model__metric': ['euclidean', 'manhattan']}\n",
      "Best parameters: {'model__metric': 'euclidean', 'model__n_neighbors': 3, 'scaler': None}\n",
      "Best score: 1.0000\n",
      "Best model for KNN: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])\n",
      "Grid search for DT...\n",
      "Performing grid search for DT with parameters: {'scaler': [None], 'model__max_depth': [None, 5, 10], 'model__min_samples_split': [2, 5, 10]}\n",
      "Best parameters: {'model__max_depth': None, 'model__min_samples_split': 2, 'scaler': None}\n",
      "Best score: 0.8500\n",
      "Best model for DT: Pipeline(steps=[('scaler', None),\n",
      "                ('model', DecisionTreeClassifier(class_weight='balanced'))])\n",
      "Grid search for SVM...\n",
      "Performing grid search for SVM with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__kernel': ['linear', 'rbf'], 'model__C': [0.1, 1, 10]}\n",
      "Best parameters: {'model__C': 0.1, 'model__kernel': 'linear', 'scaler': MinMaxScaler()}\n",
      "Best score: 1.0000\n",
      "Best model for SVM: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 SVC(C=0.1, class_weight='balanced', kernel='linear',\n",
      "                     probability=True))])\n",
      "Grid search for RF...\n",
      "Performing grid search for RF with parameters: {'scaler': [None], 'model__n_estimators': [50, 100, 200], 'model__max_depth': [None, 5, 10]}\n",
      "Best parameters: {'model__max_depth': None, 'model__n_estimators': 100, 'scaler': None}\n",
      "Best score: 1.0000\n",
      "Best model for RF: Pipeline(steps=[('scaler', None),\n",
      "                ('model', RandomForestClassifier(class_weight='balanced'))])\n",
      "LR: 1.0000 (0.0000)\n",
      "KNN: 1.0000 (0.0000)\n",
      "DT: 0.9089 (0.1190)\n",
      "SVM: 1.0000 (0.0000)\n",
      "RF: 1.0000 (0.0000)\n",
      "LR [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "KNN [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "DT [1.0, 0.6785714285714286, 1.0, 0.9375, 0.9285714285714286]\n",
      "SVM [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "RF [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Best model: LR with roc_auc: 1.0000\n",
      "Loading dataset from c:\\Users\\Utente\\Desktop\\E-ABIN\\use_case\\data\\parkinson\n",
      "Loading train/test split from disk...\n",
      "Running ML models\n",
      "Number of splits for cross-validation: 5\n",
      "Unique classes in target: {0: 16, 1: 15}\n",
      "Cross-validation splits:  5\n",
      "Performing grid search for hyperparameter tuning...\n",
      "Grid search for LR...\n",
      "Performing grid search for LR with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__solver': ['liblinear', 'saga'], 'model__penalty': ['l2', 'l1', None], 'model__max_iter': [100, 200, 300]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning:\n",
      "\n",
      "\n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [0.51111111 0.43333333 0.43333333 0.35       0.43333333 0.43333333\n",
      " 0.48333333 0.40555556 0.6        0.35       0.56666667 0.41111111\n",
      "        nan        nan        nan 0.35       0.43333333 0.43333333\n",
      " 0.51111111 0.43333333 0.43333333 0.38888889 0.43333333 0.43333333\n",
      " 0.37777778 0.40555556 0.57777778 0.38888889 0.61666667 0.42777778\n",
      "        nan        nan        nan 0.38888889 0.43333333 0.43333333\n",
      " 0.51111111 0.43333333 0.43333333 0.41111111 0.43333333 0.43333333\n",
      " 0.4        0.40555556 0.59444444 0.41111111 0.57777778 0.53333333\n",
      "        nan        nan        nan 0.41111111 0.43333333 0.43333333]\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__max_iter': 200, 'model__penalty': 'l1', 'model__solver': 'saga', 'scaler': MinMaxScaler()}\n",
      "Best score: 0.6167\n",
      "Best model for LR: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=200,\n",
      "                                    penalty='l1', solver='saga'))])\n",
      "Grid search for KNN...\n",
      "Performing grid search for KNN with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__n_neighbors': [3, 5, 7], 'model__metric': ['euclidean', 'manhattan']}\n",
      "Best parameters: {'model__metric': 'euclidean', 'model__n_neighbors': 3, 'scaler': MinMaxScaler()}\n",
      "Best score: 0.7222\n",
      "Best model for KNN: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])\n",
      "Grid search for DT...\n",
      "Performing grid search for DT with parameters: {'scaler': [None], 'model__max_depth': [None, 5, 10], 'model__min_samples_split': [2, 5, 10]}\n",
      "Best parameters: {'model__max_depth': 5, 'model__min_samples_split': 10, 'scaler': None}\n",
      "Best score: 0.4917\n",
      "Best model for DT: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 DecisionTreeClassifier(class_weight='balanced', max_depth=5,\n",
      "                                        min_samples_split=10))])\n",
      "Grid search for SVM...\n",
      "Performing grid search for SVM with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__kernel': ['linear', 'rbf'], 'model__C': [0.1, 1, 10]}\n",
      "Best parameters: {'model__C': 10, 'model__kernel': 'rbf', 'scaler': None}\n",
      "Best score: 0.5333\n",
      "Best model for SVM: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 SVC(C=10, class_weight='balanced', probability=True))])\n",
      "Grid search for RF...\n",
      "Performing grid search for RF with parameters: {'scaler': [None], 'model__n_estimators': [50, 100, 200], 'model__max_depth': [None, 5, 10]}\n",
      "Best parameters: {'model__max_depth': None, 'model__n_estimators': 100, 'scaler': None}\n",
      "Best score: 0.5667\n",
      "Best model for RF: Pipeline(steps=[('scaler', None),\n",
      "                ('model', RandomForestClassifier(class_weight='balanced'))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.4000 (0.1133)\n",
      "KNN: 0.5917 (0.2794)\n",
      "DT: 0.3833 (0.0667)\n",
      "SVM: 0.5778 (0.1879)\n",
      "RF: 0.3056 (0.2018)\n",
      "LR [0.3333333333333333, 0.22222222222222224, 0.5555555555555556, 0.4444444444444444, 0.4444444444444445]\n",
      "KNN [0.29166666666666663, 0.5, 0.33333333333333337, 1.0, 0.8333333333333333]\n",
      "DT [0.41666666666666663, 0.5, 0.33333333333333337, 0.33333333333333337, 0.33333333333333337]\n",
      "SVM [0.5, 0.33333333333333337, 0.888888888888889, 0.6666666666666666, 0.5]\n",
      "RF [0.25, 0.0, 0.22222222222222224, 0.5555555555555556, 0.5]\n",
      "Best model: KNN with roc_auc: 0.5917\n",
      "Iteration 2 of 2\n",
      "Loading dataset from c:\\Users\\Utente\\Desktop\\E-ABIN\\use_case\\data\\bladder_cancer\n",
      "Loading train/test split from disk...\n",
      "Running ML models\n",
      "Number of splits for cross-validation: 5\n",
      "Unique classes in target: {0: 2, 1: 5}\n",
      "Not enough data for 5 splits, reducing to 2 splits.\n",
      "Cross-validation splits:  2\n",
      "Performing grid search for hyperparameter tuning...\n",
      "Grid search for LR...\n",
      "Performing grid search for LR with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__solver': ['liblinear', 'saga'], 'model__penalty': ['l2', 'l1', None], 'model__max_iter': [100, 200, 300]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning:\n",
      "\n",
      "\n",
      "18 fits failed out of a total of 108.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [1.         0.83333333 1.         1.         0.83333333 1.\n",
      " 0.5        0.5        0.66666667 0.5        0.5        1.\n",
      "        nan        nan        nan 1.         0.83333333 1.\n",
      " 1.         0.83333333 1.         1.         0.83333333 1.\n",
      " 0.5        0.5        1.         0.5        0.5        1.\n",
      "        nan        nan        nan 1.         0.83333333 1.\n",
      " 1.         0.83333333 1.         1.         0.83333333 1.\n",
      " 0.5        0.5        0.83333333 0.5        0.5        1.\n",
      "        nan        nan        nan 1.         0.83333333 1.        ]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__max_iter': 100, 'model__penalty': 'l2', 'model__solver': 'liblinear', 'scaler': None}\n",
      "Best score: 1.0000\n",
      "Best model for LR: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 LogisticRegression(class_weight='balanced',\n",
      "                                    solver='liblinear'))])\n",
      "Grid search for KNN...\n",
      "Performing grid search for KNN with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__n_neighbors': [3, 5, 7], 'model__metric': ['euclidean', 'manhattan']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [0.5   0.5   0.5     nan   nan   nan   nan   nan   nan 0.5   0.625 0.625\n",
      " 0.5   0.5   0.5   0.5   0.5   0.5  ]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__metric': 'manhattan', 'model__n_neighbors': 3, 'scaler': MinMaxScaler()}\n",
      "Best score: 0.6250\n",
      "Best model for KNN: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 KNeighborsClassifier(metric='manhattan', n_neighbors=3))])\n",
      "Grid search for DT...\n",
      "Performing grid search for DT with parameters: {'scaler': [None], 'model__max_depth': [None, 5, 10], 'model__min_samples_split': [2, 5, 10]}\n",
      "Best parameters: {'model__max_depth': 5, 'model__min_samples_split': 2, 'scaler': None}\n",
      "Best score: 0.5833\n",
      "Best model for DT: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 DecisionTreeClassifier(class_weight='balanced', max_depth=5))])\n",
      "Grid search for SVM...\n",
      "Performing grid search for SVM with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__kernel': ['linear', 'rbf'], 'model__C': [0.1, 1, 10]}\n",
      "Best parameters: {'model__C': 0.1, 'model__kernel': 'linear', 'scaler': None}\n",
      "Best score: 1.0000\n",
      "Best model for SVM: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 SVC(C=0.1, class_weight='balanced', kernel='linear',\n",
      "                     probability=True))])\n",
      "Grid search for RF...\n",
      "Performing grid search for RF with parameters: {'scaler': [None], 'model__n_estimators': [50, 100, 200], 'model__max_depth': [None, 5, 10]}\n",
      "Best parameters: {'model__max_depth': None, 'model__n_estimators': 50, 'scaler': None}\n",
      "Best score: 1.0000\n",
      "Best model for RF: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 RandomForestClassifier(class_weight='balanced',\n",
      "                                        n_estimators=50))])\n",
      "LR: 1.0000 (0.0000)\n",
      "KNN: 0.6250 (0.1250)\n",
      "DT: 0.7500 (0.2500)\n",
      "SVM: 0.0000 (0.0000)\n",
      "RF: 1.0000 (0.0000)\n",
      "LR [1.0, 1.0]\n",
      "KNN [0.5, 0.75]\n",
      "DT [0.5, 1.0]\n",
      "SVM [0.0, 0.0]\n",
      "RF [1.0, 1.0]\n",
      "Best model: LR with roc_auc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from c:\\Users\\Utente\\Desktop\\E-ABIN\\use_case\\data\\celiac\n",
      "Loading train/test split from disk...\n",
      "Running ML models\n",
      "Number of splits for cross-validation: 5\n",
      "Unique classes in target: {0: 6, 1: 33}\n",
      "Cross-validation splits:  5\n",
      "Performing grid search for hyperparameter tuning...\n",
      "Grid search for LR...\n",
      "Performing grid search for LR with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__solver': ['liblinear', 'saga'], 'model__penalty': ['l2', 'l1', None], 'model__max_iter': [100, 200, 300]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning:\n",
      "\n",
      "\n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [0.82857143 0.77142857 0.8        0.82857143 0.77142857 0.77142857\n",
      " 0.72857143 0.60714286 0.77142857 0.82857143 0.85714286 0.82857143\n",
      "        nan        nan        nan 0.82857143 0.77142857 0.77142857\n",
      " 0.82857143 0.77142857 0.8        0.82857143 0.77142857 0.8\n",
      " 0.72857143 0.60714286 0.8        0.82857143 0.82857143 0.82857143\n",
      "        nan        nan        nan 0.82857143 0.77142857 0.8\n",
      " 0.82857143 0.77142857 0.8        0.82857143 0.77142857 0.77142857\n",
      " 0.72857143 0.60714286 0.82857143 0.82857143 0.82857143 0.82857143\n",
      "        nan        nan        nan 0.82857143 0.77142857 0.8       ]\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__max_iter': 100, 'model__penalty': 'l1', 'model__solver': 'saga', 'scaler': MinMaxScaler()}\n",
      "Best score: 0.8571\n",
      "Best model for LR: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 LogisticRegression(class_weight='balanced', penalty='l1',\n",
      "                                    solver='saga'))])\n",
      "Grid search for KNN...\n",
      "Performing grid search for KNN with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__n_neighbors': [3, 5, 7], 'model__metric': ['euclidean', 'manhattan']}\n",
      "Best parameters: {'model__metric': 'euclidean', 'model__n_neighbors': 7, 'scaler': None}\n",
      "Best score: 0.8131\n",
      "Best model for KNN: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=7))])\n",
      "Grid search for DT...\n",
      "Performing grid search for DT with parameters: {'scaler': [None], 'model__max_depth': [None, 5, 10], 'model__min_samples_split': [2, 5, 10]}\n",
      "Best parameters: {'model__max_depth': 5, 'model__min_samples_split': 2, 'scaler': None}\n",
      "Best score: 0.5714\n",
      "Best model for DT: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 DecisionTreeClassifier(class_weight='balanced', max_depth=5))])\n",
      "Grid search for SVM...\n",
      "Performing grid search for SVM with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__kernel': ['linear', 'rbf'], 'model__C': [0.1, 1, 10]}\n",
      "Best parameters: {'model__C': 0.1, 'model__kernel': 'rbf', 'scaler': None}\n",
      "Best score: 0.8286\n",
      "Best model for SVM: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 SVC(C=0.1, class_weight='balanced', probability=True))])\n",
      "Grid search for RF...\n",
      "Performing grid search for RF with parameters: {'scaler': [None], 'model__n_estimators': [50, 100, 200], 'model__max_depth': [None, 5, 10]}\n",
      "Best parameters: {'model__max_depth': 5, 'model__n_estimators': 50, 'scaler': None}\n",
      "Best score: 0.8714\n",
      "Best model for RF: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 RandomForestClassifier(class_weight='balanced', max_depth=5,\n",
      "                                        n_estimators=50))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.8310 (0.1630)\n",
      "KNN: 0.7929 (0.2695)\n",
      "DT: 0.4952 (0.1254)\n",
      "SVM: 0.1405 (0.1770)\n",
      "RF: 0.8429 (0.1829)\n",
      "LR [0.5833333333333334, 0.7142857142857143, 0.8571428571428571, 1.0, 1.0]\n",
      "KNN [0.75, 0.2857142857142857, 1.0, 0.9285714285714286, 1.0]\n",
      "DT [0.5, 0.5, 0.42857142857142855, 0.7142857142857143, 0.3333333333333333]\n",
      "SVM [0.4166666666666667, 0.2857142857142857, 0.0, 0.0, 0.0]\n",
      "RF [0.5, 0.8571428571428571, 0.8571428571428571, 1.0, 1.0]\n",
      "Best model: RF with roc_auc: 0.8429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from c:\\Users\\Utente\\Desktop\\E-ABIN\\use_case\\data\\colorectal_cancer\n",
      "Loading train/test split from disk...\n",
      "Running ML models\n",
      "Number of splits for cross-validation: 5\n",
      "Unique classes in target: {0: 7, 1: 37}\n",
      "Cross-validation splits:  5\n",
      "Performing grid search for hyperparameter tuning...\n",
      "Grid search for LR...\n",
      "Performing grid search for LR with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__solver': ['liblinear', 'saga'], 'model__penalty': ['l2', 'l1', None], 'model__max_iter': [100, 200, 300]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning:\n",
      "\n",
      "\n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [0.975 1.    1.    0.975 1.    1.    1.    1.    1.    0.975 1.    0.975\n",
      "   nan   nan   nan 0.975 1.    1.    0.975 1.    1.    0.975 1.    1.\n",
      " 1.    1.    1.    0.975 1.    0.975   nan   nan   nan 0.975 1.    1.\n",
      " 0.975 1.    1.    0.975 1.    1.    1.    1.    1.    0.975 1.    0.975\n",
      "   nan   nan   nan 0.975 1.    1.   ]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__max_iter': 100, 'model__penalty': 'l2', 'model__solver': 'liblinear', 'scaler': MinMaxScaler()}\n",
      "Best score: 1.0000\n",
      "Best model for LR: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 LogisticRegression(class_weight='balanced',\n",
      "                                    solver='liblinear'))])\n",
      "Grid search for KNN...\n",
      "Performing grid search for KNN with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__n_neighbors': [3, 5, 7], 'model__metric': ['euclidean', 'manhattan']}\n",
      "Best parameters: {'model__metric': 'euclidean', 'model__n_neighbors': 3, 'scaler': None}\n",
      "Best score: 1.0000\n",
      "Best model for KNN: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])\n",
      "Grid search for DT...\n",
      "Performing grid search for DT with parameters: {'scaler': [None], 'model__max_depth': [None, 5, 10], 'model__min_samples_split': [2, 5, 10]}\n",
      "Best parameters: {'model__max_depth': 5, 'model__min_samples_split': 2, 'scaler': None}\n",
      "Best score: 0.9000\n",
      "Best model for DT: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 DecisionTreeClassifier(class_weight='balanced', max_depth=5))])\n",
      "Grid search for SVM...\n",
      "Performing grid search for SVM with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__kernel': ['linear', 'rbf'], 'model__C': [0.1, 1, 10]}\n",
      "Best parameters: {'model__C': 0.1, 'model__kernel': 'linear', 'scaler': MinMaxScaler()}\n",
      "Best score: 1.0000\n",
      "Best model for SVM: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 SVC(C=0.1, class_weight='balanced', kernel='linear',\n",
      "                     probability=True))])\n",
      "Grid search for RF...\n",
      "Performing grid search for RF with parameters: {'scaler': [None], 'model__n_estimators': [50, 100, 200], 'model__max_depth': [None, 5, 10]}\n",
      "Best parameters: {'model__max_depth': None, 'model__n_estimators': 50, 'scaler': None}\n",
      "Best score: 1.0000\n",
      "Best model for RF: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 RandomForestClassifier(class_weight='balanced',\n",
      "                                        n_estimators=50))])\n",
      "LR: 1.0000 (0.0000)\n",
      "KNN: 1.0000 (0.0000)\n",
      "DT: 0.8607 (0.2137)\n",
      "SVM: 1.0000 (0.0000)\n",
      "RF: 1.0000 (0.0000)\n",
      "LR [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "KNN [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "DT [1.0, 1.0, 0.9375, 0.4375, 0.9285714285714286]\n",
      "SVM [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "RF [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Best model: LR with roc_auc: 1.0000\n",
      "Loading dataset from c:\\Users\\Utente\\Desktop\\E-ABIN\\use_case\\data\\parkinson\n",
      "Loading train/test split from disk...\n",
      "Running ML models\n",
      "Number of splits for cross-validation: 5\n",
      "Unique classes in target: {0: 16, 1: 15}\n",
      "Cross-validation splits:  5\n",
      "Performing grid search for hyperparameter tuning...\n",
      "Grid search for LR...\n",
      "Performing grid search for LR with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__solver': ['liblinear', 'saga'], 'model__penalty': ['l2', 'l1', None], 'model__max_iter': [100, 200, 300]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning:\n",
      "\n",
      "\n",
      "45 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning:\n",
      "\n",
      "One or more of the test scores are non-finite: [0.51111111 0.43333333 0.43333333 0.33333333 0.43333333 0.43333333\n",
      " 0.46666667 0.40555556 0.55555556 0.35       0.56666667 0.43333333\n",
      "        nan        nan        nan 0.35       0.43333333 0.43333333\n",
      " 0.51111111 0.43333333 0.43333333 0.38888889 0.43333333 0.43333333\n",
      " 0.37777778 0.40555556 0.51111111 0.38888889 0.61666667 0.42777778\n",
      "        nan        nan        nan 0.38888889 0.43333333 0.43333333\n",
      " 0.51111111 0.43333333 0.43333333 0.41111111 0.43333333 0.43333333\n",
      " 0.37777778 0.40555556 0.48333333 0.41111111 0.57777778 0.53333333\n",
      "        nan        nan        nan 0.41111111 0.43333333 0.43333333]\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__max_iter': 200, 'model__penalty': 'l1', 'model__solver': 'saga', 'scaler': MinMaxScaler()}\n",
      "Best score: 0.6167\n",
      "Best model for LR: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=200,\n",
      "                                    penalty='l1', solver='saga'))])\n",
      "Grid search for KNN...\n",
      "Performing grid search for KNN with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__n_neighbors': [3, 5, 7], 'model__metric': ['euclidean', 'manhattan']}\n",
      "Best parameters: {'model__metric': 'euclidean', 'model__n_neighbors': 3, 'scaler': MinMaxScaler()}\n",
      "Best score: 0.7222\n",
      "Best model for KNN: Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('model',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])\n",
      "Grid search for DT...\n",
      "Performing grid search for DT with parameters: {'scaler': [None], 'model__max_depth': [None, 5, 10], 'model__min_samples_split': [2, 5, 10]}\n",
      "Best parameters: {'model__max_depth': 5, 'model__min_samples_split': 10, 'scaler': None}\n",
      "Best score: 0.5250\n",
      "Best model for DT: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 DecisionTreeClassifier(class_weight='balanced', max_depth=5,\n",
      "                                        min_samples_split=10))])\n",
      "Grid search for SVM...\n",
      "Performing grid search for SVM with parameters: {'scaler': [None, MinMaxScaler(), StandardScaler()], 'model__kernel': ['linear', 'rbf'], 'model__C': [0.1, 1, 10]}\n",
      "Best parameters: {'model__C': 10, 'model__kernel': 'rbf', 'scaler': None}\n",
      "Best score: 0.5333\n",
      "Best model for SVM: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 SVC(C=10, class_weight='balanced', probability=True))])\n",
      "Grid search for RF...\n",
      "Performing grid search for RF with parameters: {'scaler': [None], 'model__n_estimators': [50, 100, 200], 'model__max_depth': [None, 5, 10]}\n",
      "Best parameters: {'model__max_depth': 10, 'model__n_estimators': 50, 'scaler': None}\n",
      "Best score: 0.7333\n",
      "Best model for RF: Pipeline(steps=[('scaler', None),\n",
      "                ('model',\n",
      "                 RandomForestClassifier(class_weight='balanced', max_depth=10,\n",
      "                                        n_estimators=50))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n",
      "d:\\anaconda3\\envs\\eabin13\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning:\n",
      "\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.5833 (0.2992)\n",
      "KNN: 0.4833 (0.1856)\n",
      "DT: 0.5417 (0.1624)\n",
      "SVM: 0.3944 (0.2183)\n",
      "RF: 0.1444 (0.1556)\n",
      "LR [0.9166666666666666, 0.4444444444444445, 0.11111111111111112, 0.888888888888889, 0.5555555555555556]\n",
      "KNN [0.75, 0.33333333333333337, 0.33333333333333337, 0.33333333333333337, 0.6666666666666667]\n",
      "DT [0.5416666666666666, 0.33333333333333337, 0.8333333333333333, 0.5, 0.5]\n",
      "SVM [0.25, 0.5555555555555556, 0.3333333333333333, 0.11111111111111112, 0.7222222222222223]\n",
      "RF [0.3333333333333333, 0.33333333333333337, 0.0, 0.0, 0.05555555555555556]\n",
      "Best model: LR with roc_auc: 0.5833\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_result_ml = None \n",
    "for i in range(2):\n",
    "    print(f\"Iteration {i+1} of 2\")\n",
    "    for dataset_name in datasets_name:\n",
    "    \n",
    "        \n",
    "        dataset_path = datasets_path + sep + dataset_name\n",
    "        print(f\"Loading dataset from {dataset_path}\")\n",
    "        df = load_dataset(dataset_path)\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(df, dataset_path, test_size = 0.7, target_name = 'Target')\n",
    "        print(\"Running ML models\")\n",
    "        models_tuple, fig_roc, fig_box = baselineComparison(X_train, y_train, params = ml_params, grid_search=True)\n",
    "            \n",
    "        models = {}\n",
    "        for model_name, model in models_tuple:\n",
    "            models[model_name] = model\n",
    "\n",
    "\n",
    "        df_result = create_results_df(models, X_test, y_test)\n",
    "        df_result['Dataset'] = dataset_name  \n",
    "\n",
    "        df_result_ml = pd.concat([df_result_ml, df_result]) if df_result_ml is not None else df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42d8bea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>AUC score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>bladder_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>88.240</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>86.670000</td>\n",
       "      <td>bladder_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT</td>\n",
       "      <td>82.350</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>84.620000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>91.670000</td>\n",
       "      <td>bladder_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>76.470</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>76.470000</td>\n",
       "      <td>bladder_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>bladder_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>GAE</td>\n",
       "      <td>35.000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>70.588235</td>\n",
       "      <td>celiac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>GCN</td>\n",
       "      <td>82.500</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>82.500000</td>\n",
       "      <td>celiac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>GAAN</td>\n",
       "      <td>46.875</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>76.470588</td>\n",
       "      <td>0.449020</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>parkinson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>GAE</td>\n",
       "      <td>40.625</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>58.823529</td>\n",
       "      <td>0.394118</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>parkinson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>GCN</td>\n",
       "      <td>46.875</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>46.875000</td>\n",
       "      <td>parkinson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Name  Accuracy        F1  Sensitivity  Specificity  AUC score  \\\n",
       "0          LR   100.000  1.000000   100.000000   100.000000   1.000000   \n",
       "1          RF    88.240  0.930000   100.000000    50.000000   1.000000   \n",
       "2          DT    82.350  0.880000    84.620000    75.000000   0.800000   \n",
       "3         KNN    76.470  0.870000   100.000000     0.000000   0.650000   \n",
       "4         SVM   100.000  1.000000   100.000000   100.000000   0.000000   \n",
       "..        ...       ...       ...          ...          ...        ...   \n",
       "59        GAE    35.000  0.480000    36.363636    28.571429   0.324675   \n",
       "60        GCN    82.500  0.904110   100.000000     0.000000   0.500000   \n",
       "61       GAAN    46.875  0.190476    13.333333    76.470588   0.449020   \n",
       "62        GAE    40.625  0.240000    20.000000    58.823529   0.394118   \n",
       "63        GCN    46.875  0.638298   100.000000     0.000000   0.500000   \n",
       "\n",
       "     Precision         Dataset  \n",
       "0   100.000000  bladder_cancer  \n",
       "1    86.670000  bladder_cancer  \n",
       "2    91.670000  bladder_cancer  \n",
       "3    76.470000  bladder_cancer  \n",
       "4   100.000000  bladder_cancer  \n",
       "..         ...             ...  \n",
       "59   70.588235          celiac  \n",
       "60   82.500000          celiac  \n",
       "61   33.333333       parkinson  \n",
       "62   30.000000       parkinson  \n",
       "63   46.875000       parkinson  \n",
       "\n",
       "[64 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_all = pd.concat([df_result_ml, df_result_dl], ignore_index=True)\n",
    "df_result_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5be491e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>AUC score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>bladder_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>88.240</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>86.670000</td>\n",
       "      <td>bladder_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT</td>\n",
       "      <td>82.350</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>84.620000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>91.670000</td>\n",
       "      <td>bladder_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>76.470</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>76.470000</td>\n",
       "      <td>bladder_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>bladder_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>GAE</td>\n",
       "      <td>35.000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>70.588235</td>\n",
       "      <td>celiac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>GCN</td>\n",
       "      <td>82.500</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>82.500000</td>\n",
       "      <td>celiac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>GAAN</td>\n",
       "      <td>46.875</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>76.470588</td>\n",
       "      <td>0.449020</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>parkinson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>GAE</td>\n",
       "      <td>40.625</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>58.823529</td>\n",
       "      <td>0.394118</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>parkinson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>GCN</td>\n",
       "      <td>46.875</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>46.875000</td>\n",
       "      <td>parkinson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Name  Accuracy        F1  Sensitivity  Specificity  AUC score  \\\n",
       "0          LR   100.000  1.000000   100.000000   100.000000   1.000000   \n",
       "1          RF    88.240  0.930000   100.000000    50.000000   1.000000   \n",
       "2          DT    82.350  0.880000    84.620000    75.000000   0.800000   \n",
       "3         KNN    76.470  0.870000   100.000000     0.000000   0.650000   \n",
       "4         SVM   100.000  1.000000   100.000000   100.000000   0.000000   \n",
       "..        ...       ...       ...          ...          ...        ...   \n",
       "59        GAE    35.000  0.480000    36.363636    28.571429   0.324675   \n",
       "60        GCN    82.500  0.904110   100.000000     0.000000   0.500000   \n",
       "61       GAAN    46.875  0.190476    13.333333    76.470588   0.449020   \n",
       "62        GAE    40.625  0.240000    20.000000    58.823529   0.394118   \n",
       "63        GCN    46.875  0.638298   100.000000     0.000000   0.500000   \n",
       "\n",
       "     Precision         Dataset  \n",
       "0   100.000000  bladder_cancer  \n",
       "1    86.670000  bladder_cancer  \n",
       "2    91.670000  bladder_cancer  \n",
       "3    76.470000  bladder_cancer  \n",
       "4   100.000000  bladder_cancer  \n",
       "..         ...             ...  \n",
       "59   70.588235          celiac  \n",
       "60   82.500000          celiac  \n",
       "61   33.333333       parkinson  \n",
       "62   30.000000       parkinson  \n",
       "63   46.875000       parkinson  \n",
       "\n",
       "[64 rows x 8 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_all.to_csv(cwd + sep + \"eabin_internal_comparison.csv\", index=False)\n",
    "df_result_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d1c1aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_compare = compare_plot(df_result_all, column_name = \"AUC score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "675d0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_compare.savefig(cwd + sep + \"eabin_internal_comparison_auc.png\", dpi=600, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eabin13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
